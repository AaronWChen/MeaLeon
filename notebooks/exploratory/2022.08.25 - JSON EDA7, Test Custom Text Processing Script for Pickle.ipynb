{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9afa101e-a9e5-4d1d-9e3e-416f1c767233",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis of Epicurious Scrape in a JSON file\n",
    "\n",
    "This is an idealized workflow for Aaron Chen in looking at data science problems. It likely isn't the best path, nor has he rigidly applied or stuck to this ideal, but he wishes that he worked this way more frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59c575-9aca-4d08-9b03-3425fa33cb03",
   "metadata": {},
   "source": [
    "## Purpose: Work through some exploratory data analysis of the Epicurious scrape on stream. Try to write some functions to help process the data.\n",
    "\n",
    "### Author: Aaron Chen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21b7d4-5980-4adf-a62b-38b8e89031ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d4a89-cf20-435c-bcc0-6f1ebcd127c7",
   "metadata": {},
   "source": [
    "### If needed, run shell commands here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed4e2d-6f77-4196-b548-d5896e13193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -c \"import tkinter\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cede1d1-3a00-4471-91ae-50be260464a8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b66d7b-46ad-4f10-9cb9-72c03e319fea",
   "metadata": {},
   "source": [
    "## External Resources\n",
    "\n",
    "List out references or documentation that has helped you with this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b23fd3-6cf1-42b3-aee3-82306eaade6a",
   "metadata": {},
   "source": [
    "### Code\n",
    "Regex Checker: https://regex101.com/\n",
    "\n",
    "#### Scikit-learn\n",
    "1. https://scikit-learn.org/stable/modules/decomposition.html#latent-dirichlet-allocation-lda\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf83f44-334c-448a-afa7-187f63dd7285",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "For this notebook, the data is stored in the repo base folder/data/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0dcb8e-5a84-4e96-b07d-24909352935d",
   "metadata": {},
   "source": [
    "### Process\n",
    "\n",
    "Are there steps or tutorials you are following? Those are things I try to list in Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a44e4-081e-4f8f-88d2-3eccd0f2d767",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418bdb7-3ecf-4eb5-af6f-2c0e699d2394",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f480787a-b5ed-4bf2-8553-504d6cfffe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# from examples import utils\n",
    "from joblib import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "from openTSNE import TSNE\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "# from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "import spacy\n",
    "from tkinter import N\n",
    "import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60dd0a-089e-41b7-9828-8f2b13648993",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde95259-0cd4-4ae1-8485-d51ada957529",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a3ea1e-a26e-4545-b950-ec823cbd6a26",
   "metadata": {},
   "source": [
    "My workflow is to try things with code cells, then when the code cells get messy and repetitive, to convert into helper functions that can be called.\n",
    "\n",
    "When the helper functions are getting used a lot, it is usually better to convert them to scripts or classes that can be called/instantiated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12bdc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d61885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_matrices_to_df(df, vectorized_ingred_matrix, cv):\n",
    "    \"\"\"This function takes in a dataframe and concats the matrix generated by either CountVectorizer or TFIDF-Transformer onto the records so that the recipes can be used for classification purposes.\n",
    "\n",
    "    Args: \n",
    "        df: preprocessed dataframe from preprocess_dataframe\n",
    "        vectorized_ingred_matrix: sparse csr matrix created from doing fit_transform on the recipe_megalist\n",
    "     \n",
    "    Returns:\n",
    "        A pandas dataframe with the vectorized_ingred_matrix appended as columns to df\n",
    "    \"\"\"\n",
    "    repo_tfidf_df = pd.DataFrame(vectorized_ingred_matrix.toarray(), columns=cv.get_feature_names_out(), index=df.index)\n",
    "    return pd.concat([df, repo_tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724190c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d(points, points_color, title):\n",
    "    x, y, z = points.T\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(6, 6),\n",
    "        facecolor=\"white\",\n",
    "        tight_layout=True,\n",
    "        subplot_kw={\"projection\": \"3d\"},\n",
    "    )\n",
    "    fig.suptitle(title, size=16)\n",
    "    col = ax.scatter(x, y, z, c=points_color, s=50, alpha=0.8)\n",
    "    ax.view_init(azim=-60, elev=9)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.zaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    fig.colorbar(col, ax=ax, orientation=\"horizontal\", shrink=0.6, aspect=60, pad=0.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7306b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_2d_scatter(ax, points, points_color, title=None):\n",
    "    x, y = points.T\n",
    "    ax.scatter(x, y, c=points_color, s=50, alpha=0.8)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_major_formatter(ticker.NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(ticker.NullFormatter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4264e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d(points, points_color, title):\n",
    "    fig, ax = plt.subplots(figsize=(3, 3), facecolor=\"white\", constrained_layout=True)\n",
    "    fig.suptitle(title, size=16)\n",
    "    add_2d_scatter(ax, points, points_color)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fdd4b0-9acb-445e-acad-2b3eb6ab9e4c",
   "metadata": {},
   "source": [
    "### Import local script\n",
    "\n",
    "I started grouping this in with importing libraries, but putting them at the bottom of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ae4f42-17a3-4564-95b7-7993e5989d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import project_path\n",
    "\n",
    "import src.dataframe_preprocessor as dfpp\n",
    "import src.nlp_processor as nlp_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1b7742-3b22-4f48-877f-9f32cd7251fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7445bf0-0b00-45b9-a2db-84c568bced29",
   "metadata": {},
   "source": [
    "## Define global variables \n",
    "### Remember to refactor these out, not ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d00c380-63ee-4141-a073-fbe725cb826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../data/recipes-en-201706/epicurious-recipes_m2.json\"\n",
    "food_stopwords_path = \"../../food_stopwords.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab98b748-74ad-4dff-bebe-aa794f7a4591",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b325f7ef-07c7-46b1-8595-9749259da76b",
   "metadata": {},
   "source": [
    "## Running Commentary\n",
    "\n",
    "1. I used numbered lists to keep track of things I noticed\n",
    "\n",
    "### To Do\n",
    "\n",
    "1. Try to determine consistency of nested data structures\n",
    "   1. Is the photoData or number of things inside photoData the same from record to record\n",
    "   2. What about for tag?\n",
    "\n",
    "Data wasn't fully consistent but logic in helper function helped handle nulls\n",
    "\n",
    "2. How to handle nulls?\n",
    "   1. Author      Filled in with \"Missing Author\"\n",
    "   2. Tag         Filled in with \"Missing Cuisine\"\n",
    "3. ~~Convert pubDate to actual timestamp~~  \n",
    "4. ~~Convert ScrapeDate to actual timestamp~~\n",
    "   1. This was ignored as the datestamp was not useful (generally within minutes of the origin of UNIX time)\n",
    "   \n",
    "**5. Append new columns for relevant nested structures and unfold them**\n",
    "\n",
    "6. Determine actual types of `ingredients` and `prepSteps`\n",
    "7. Continue working through test example of single recipe to feed into spaCy and then sklearn.feature_extraction.text stack\n",
    "8. Will need to remove numbers, punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2173e5-db8a-4fff-8072-98d6b16f7a4d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf870da-c2ed-4cde-9dd8-1591cbd2f24c",
   "metadata": {},
   "source": [
    "## Importing and viewing the data as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d4ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheeeeeesh = pd.read_json(path_or_buf=data_path) # type:ignore\n",
    "# pd.read_json(data_path, typ='frame') # type:ignore\n",
    "\n",
    "letsgoooo = dfpp.preprocess_dataframe(df=sheeeeeesh) # type:ignore\n",
    "\n",
    "recipe_megalist = [ingred for recipe in letsgoooo['ingredients'].tolist() for ingred in recipe]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# this is a redeem for variable naming mixed with a free pun-ish me daddy, flushtrated will be the list of all stopword to exclude so named because we're throwing these words down the drain\n",
    "\n",
    "flushtrated = {x for x in pd.read_csv(food_stopwords_path)}\n",
    "additional_to_exclude = {'red', 'green', 'black', 'yellow', 'white', 'inch', 'mince', 'chop', 'fry', 'trim', 'flat', 'beat', 'brown', 'golden', 'balsamic', 'halve', 'blue', 'divide', 'trim', 'unbleache', 'granulate', 'Frank', 'alternative', 'american', 'annie', 'asian', 'balance', 'band', 'barrel', 'bay', 'bayou', 'beam', 'beard', 'bell', 'betty', 'bird', 'blast', 'bob', 'bone', 'breyers', 'calore', 'carb', 'card', 'chachere', 'change', 'circle', 'coffee', 'coil', 'country', 'cow', 'crack', 'cracker', 'crocker', 'crystal', 'dean', 'degree', 'deluxe', 'direction', 'duncan', 'earth', 'eggland', 'ener', 'envelope', 'eye', 'fantastic', 'far', 'fat', 'feather', 'flake', 'foot', 'fourth', 'frank', 'french', 'fusion', 'genoa', 'genovese', 'germain', 'giada', 'gold', 'granule', 'greek', 'hamburger', 'helper', 'herbe', 'hines', 'hodgson', 'hunt', 'instruction', 'interval', 'italianstyle', 'jim', 'jimmy', 'kellogg', 'lagrille', 'lake', 'land', 'laurentiis', 'lawry', 'lipton', 'litre', 'll', 'maid', 'malt', 'mate', 'mayer', 'meal', 'medal', 'medallion', 'member', 'mexicanstyle', 'monte', 'mori', 'nest', 'nu', 'oounce', 'oscar', 'ox', 'paso', 'pasta', 'patty', 'petal', 'pinche', 'preserve', 'quartere', 'ranch', 'ranchstyle', 'rasher', 'redhot', 'resemble', 'rice', 'ro', 'roni', 'scissor', 'scrap', 'secret', 'semicircle', 'shard', 'shear', 'sixth', 'sliver', 'smucker', 'snicker', 'source', 'spot', 'state', 'strand', 'sun', 'supreme', 'tablepoon', 'tail', 'target', 'tm', 'tong', 'toothpick', 'triangle', 'trimming', 'tweezer', 'valley', 'vay', 'wise', 'wishbone', 'wrapper', 'yoplait', 'ziploc'}\n",
    "\n",
    "flushtrated = flushtrated.union(STOP_WORDS)\n",
    "flushtrated = flushtrated.union(additional_to_exclude)\n",
    "flushtrated_list = list(flushtrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0715280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_nlp_proc = nlp_proc.NLP_Processor(\"en_core_web_sm\")\n",
    "\n",
    "cv = CountVectorizer(strip_accents='unicode', \n",
    "                        lowercase=True, \n",
    "                        preprocessor=custom_nlp_proc.custom_preprocessor, \n",
    "                        tokenizer=custom_nlp_proc.custom_lemmatizer, \n",
    "                        stop_words=flushtrated_list, \n",
    "                        token_pattern=r\"(?u)\\b[a-zA-Z]{2,}\\b\", \n",
    "                        ngram_range=(1,4), \n",
    "                        min_df=10\n",
    "                        )\n",
    "\n",
    "cv.fit(tqdm(recipe_megalist))\n",
    "\n",
    "temp = letsgoooo[\"ingredients\"].apply(\" \".join).str.lower()\n",
    "\n",
    "repo_transformed = cv.transform(tqdm(temp))\n",
    "\n",
    "cv.get_feature_names_out().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286cd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "\n",
    "repo_tfidf = tfidf.fit_transform(repo_transformed)\n",
    "\n",
    "repo_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2110f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_with_cv = concat_matrices_to_df(letsgoooo, repo_tfidf, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbae5a4",
   "metadata": {},
   "source": [
    "We can try to filter out the adjectives in the lemmatization step, because spaCy allows filtering based on Parts of Speech. But this might exclude them from the ngrams. Let's try augmenting stopwords and excluding colors that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce0d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = recipes_with_cv.drop(['dek', 'hed', 'aggregateRating', 'ingredients', 'prepSteps',\n",
    "       'reviewsCount', 'willMakeAgainPct', 'photo_filename',\n",
    "       'photo_credit', 'author_name', 'date_published', 'recipe_url'], axis=1)\n",
    "\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e029690",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = filtered_df[filtered_df['cuisine_name'] != 'Missing Cuisine']\n",
    "y = reduced_df['cuisine_name']\n",
    "X = reduced_df.drop(['id', 'cuisine_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d206d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=240, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8edb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_clf = RandomForestClassifier(max_depth=50, random_state=572, class_weight=\"balanced\", n_jobs=-1)\n",
    "\n",
    "rfc_clf.fit(X_train, y_train)\n",
    "print(rfc_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72b94b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib_basepath = '../../joblib/2022.08.23/'\n",
    "\n",
    "cv_path = joblib_basepath + 'countvec.joblib'\n",
    "tfidf_path = joblib_basepath + 'tfidf.joblib'\n",
    "full_df_path = joblib_basepath + 'recipes_with_cv.joblib'\n",
    "reduced_df_path = joblib_basepath + 'reduced_df.joblib'\n",
    "rfc_path = joblib_basepath + 'rfc_clf.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(cv, cv_path)\n",
    "dump(tfidf, tfidf_path)\n",
    "dump(recipes_with_cv, full_df_path)\n",
    "dump(reduced_df, reduced_df_path)\n",
    "dump(rfc_clf, rfc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "756b5caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = load(cv_path)\n",
    "tfidf = load(tfidf_path)\n",
    "recipes_with_cv = load(full_df_path)\n",
    "reduced_df = load(reduced_df_path)\n",
    "rfc_clf = load(rfc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff7e8ef",
   "metadata": {},
   "source": [
    "Sklearn works with DOT formatted trees. ETE does not support this yet. It is a feature being added for ETE milestone 4 but that is as of this stream 50% complete https://github.com/etetoolkit/ete/issues/361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e029690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_df = filtered_df[filtered_df['cuisine_name'] != 'Missing Cuisine']\n",
    "y = reduced_df['cuisine_name']\n",
    "X = reduced_df.drop(['id', 'cuisine_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d206d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=240, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40f6959d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11124, 3351)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a34635c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "we_were_talking_about_variable_name = TruncatedSVD(n_components=100, n_iter=15, random_state=268)\n",
    "we_were_talking_about_variable_name_svd = we_were_talking_about_variable_name.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fdd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sne = TSNE(n_components=2, verbose=20, random_state=144, n_jobs=-1)\n",
    "\n",
    "vis_t_sne = t_sne.fit_transform(we_were_talking_about_variable_name_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "plt.scatter(x=vis_t_sne[:,0], y=vis_t_sne[:,1], c=colors, s=sizes, alpha=0.3, cmap='viridis'); \n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db76862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheeeeeesh['cuisine_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3d0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheeeeeesh['cuisine_name'][49]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7142e37045ca6e2f7ebd069a40dd15c94e2caae7a512958c86ea35c1f050d31a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
