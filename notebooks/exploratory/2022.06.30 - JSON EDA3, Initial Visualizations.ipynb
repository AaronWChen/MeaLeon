{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9afa101e-a9e5-4d1d-9e3e-416f1c767233",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis of Epicurious Scrape in a JSON file\n",
    "\n",
    "This is an idealized workflow for Aaron Chen in looking at data science problems. It likely isn't the best path, nor has he rigidly applied or stuck to this ideal, but he wishes that he worked this way more frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59c575-9aca-4d08-9b03-3425fa33cb03",
   "metadata": {},
   "source": [
    "## Purpose: Work through some exploratory data analysis of the Epicurious scrape on stream. Try to write some functions to help process the data.\n",
    "\n",
    "### Author: Aaron Chen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21b7d4-5980-4adf-a62b-38b8e89031ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d4a89-cf20-435c-bcc0-6f1ebcd127c7",
   "metadata": {},
   "source": [
    "### If needed, run shell commands here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ed4e2d-6f77-4196-b548-d5896e13193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cede1d1-3a00-4471-91ae-50be260464a8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b66d7b-46ad-4f10-9cb9-72c03e319fea",
   "metadata": {},
   "source": [
    "## External Resources\n",
    "\n",
    "List out references or documentation that has helped you with this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b23fd3-6cf1-42b3-aee3-82306eaade6a",
   "metadata": {},
   "source": [
    "### Code\n",
    "Regex Checker: https://regex101.com/\n",
    "\n",
    "#### Scikit-learn\n",
    "1. https://scikit-learn.org/stable/modules/decomposition.html#latent-dirichlet-allocation-lda\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf83f44-334c-448a-afa7-187f63dd7285",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "For this notebook, the data is stored in the repo base folder/data/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0dcb8e-5a84-4e96-b07d-24909352935d",
   "metadata": {},
   "source": [
    "### Process\n",
    "\n",
    "Are there steps or tutorials you are following? Those are things I try to list in Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a44e4-081e-4f8f-88d2-3eccd0f2d767",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418bdb7-3ecf-4eb5-af6f-2c0e699d2394",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f480787a-b5ed-4bf2-8553-504d6cfffe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# from spacy.lemmatizer import Lemmatizer\n",
    "from tqdm import tqdm\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60dd0a-089e-41b7-9828-8f2b13648993",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde95259-0cd4-4ae1-8485-d51ada957529",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a3ea1e-a26e-4545-b950-ec823cbd6a26",
   "metadata": {},
   "source": [
    "My workflow is to try things with code cells, then when the code cells get messy and repetitive, to convert into helper functions that can be called.\n",
    "\n",
    "When the helper functions are getting used a lot, it is usually better to convert them to scripts or classes that can be called/instantiated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2dbd771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lemmatizer(ingredients: list) -> Any: # spacy nlp.Doc\n",
    "    \"\"\"This takes in a string representing the recipe and an NLP model and lemmatize with the NER. \n",
    "    \n",
    "    Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    Remove punctuation\n",
    "\n",
    "    Args:\n",
    "        ingredients: string\n",
    "        nlp_mod: spacy model (try built in first, by default called nlp)\n",
    "    \n",
    "    Returns:\n",
    "        nlp.Doc\n",
    "    \"\"\"\n",
    "    lemmas = [token.lemma_ for token in ingredients if (token.is_alpha and token.pos_ not in [\"PRON\", \"VERB\"] and len(token.lemma_) > 1)]\n",
    "    return lemmas\n",
    "    # return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27b6d33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_preprocessor(recipe_ingreds: str) -> list:\n",
    "    \"\"\"This function replaces the default sklearn CountVectorizer preprocessor to use spaCy. sklearn CountVectorizer's preprocessor only performs accent removal and lowercasing.\n",
    "\n",
    "    Args:\n",
    "        A string to tokenize from a recipe representing the ingredients used in the recipe\n",
    "\n",
    "    Returns:\n",
    "        A list of strings that have been de-accented and lowercased to be used in tokenization\n",
    "    \"\"\"\n",
    "    preprocessed = [token for token in nlp(recipe_ingreds)]\n",
    "\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a12bdc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fdd4b0-9acb-445e-acad-2b3eb6ab9e4c",
   "metadata": {},
   "source": [
    "### Import local script\n",
    "\n",
    "I started grouping this in with importing libraries, but putting them at the bottom of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27ae4f42-17a3-4564-95b7-7993e5989d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import project_path\n",
    "\n",
    "import src.dataframe_preprocessor as dfpp\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1b7742-3b22-4f48-877f-9f32cd7251fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7445bf0-0b00-45b9-a2db-84c568bced29",
   "metadata": {},
   "source": [
    "## Define global variables \n",
    "### Remember to refactor these out, not ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d00c380-63ee-4141-a073-fbe725cb826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../data/recipes-en-201706/epicurious-recipes_m2.json\"\n",
    "food_stopwords_path = \"../../food_stopwords.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab98b748-74ad-4dff-bebe-aa794f7a4591",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b325f7ef-07c7-46b1-8595-9749259da76b",
   "metadata": {},
   "source": [
    "## Running Commentary\n",
    "\n",
    "1. I used numbered lists to keep track of things I noticed\n",
    "\n",
    "### To Do\n",
    "\n",
    "1. Try to determine consistency of nested data structures\n",
    "   1. Is the photoData or number of things inside photoData the same from record to record\n",
    "   2. What about for tag?\n",
    "\n",
    "Data wasn't fully consistent but logic in helper function helped handle nulls\n",
    "\n",
    "2. How to handle nulls?\n",
    "   1. Author      Filled in with \"Missing Author\"\n",
    "   2. Tag         Filled in with \"Missing Cuisine\"\n",
    "3. ~~Convert pubDate to actual timestamp~~  \n",
    "4. ~~Convert ScrapeDate to actual timestamp~~\n",
    "   1. This was ignored as the datestamp was not useful (generally within minutes of the origin of UNIX time)\n",
    "   \n",
    "**5. Append new columns for relevant nested structures and unfold them**\n",
    "\n",
    "6. Determine actual types of `ingredients` and `prepSteps`\n",
    "7. Continue working through test example of single recipe to feed into spaCy and then sklearn.feature_extraction.text stack\n",
    "8. Will need to remove numbers, punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2173e5-db8a-4fff-8072-98d6b16f7a4d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf870da-c2ed-4cde-9dd8-1591cbd2f24c",
   "metadata": {},
   "source": [
    "## Importing and viewing the data as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f96d4ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34656, 14)\n"
     ]
    }
   ],
   "source": [
    "repo = pd.read_json(path_or_buf=data_path) # type:ignore\n",
    "pd.read_json(data_path, typ='frame') # type:ignore\n",
    "\n",
    "dfpp.preprocess_dataframe(df=repo) # type:ignore\n",
    "print(repo.shape)\n",
    "repo.head(10) # type:ignore\n",
    "\n",
    "recipe_megalist = [ingred for recipe in repo['ingredients'].tolist() for ingred in recipe]\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# this is a redeem for variable naming mixed with a free pun-ish me daddy, flushtrated will be the list of all stopword to exclude so named because we're throwing these words down the drain\n",
    "\n",
    "flushtrated = {x for x in pd.read_csv(food_stopwords_path)}\n",
    "flushtrated = flushtrated.union(STOP_WORDS)\n",
    "flushtrated_list = list(flushtrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0715280d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/341271 [00:00<?, ?it/s]/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['Frank', 'alternative', 'american', 'annie', 'asian', 'balance', 'band', 'barrel', 'bay', 'bayou', 'beam', 'beard', 'bell', 'betty', 'bird', 'blast', 'bob', 'bone', 'breyers', 'calore', 'carb', 'card', 'chachere', 'change', 'circle', 'coffee', 'coil', 'country', 'cow', 'crack', 'cracker', 'crocker', 'crystal', 'dean', 'degree', 'deluxe', 'direction', 'duncan', 'earth', 'eggland', 'ener', 'envelope', 'eye', 'fantastic', 'far', 'fat', 'feather', 'flake', 'foot', 'fourth', 'frank', 'french', 'fry', 'fusion', 'genoa', 'genovese', 'germain', 'giada', 'gold', 'golden', 'granule', 'greek', 'hamburger', 'helper', 'herbe', 'hines', 'hodgson', 'hunt', 'instruction', 'interval', 'italianstyle', 'jim', 'jimmy', 'kellogg', 'lagrille', 'lake', 'land', 'laurentiis', 'lawry', 'lipton', 'litre', 'll', 'maid', 'malt', 'mate', 'mayer', 'meal', 'medal', 'medallion', 'member', 'mexicanstyle', 'mince', 'monte', 'mori', 'nest', 'nu', 'oounce', 'oscar', 'ox', 'paso', 'pasta', 'patty', 'petal', 'pinche', 'preserve', 'quartere', 'ranch', 'ranchstyle', 'rasher', 'redhot', 'resemble', 'rice', 'ro', 'roni', 'scissor', 'scrap', 'secret', 'semicircle', 'shard', 'shear', 'sixth', 'sliver', 'smucker', 'snicker', 'source', 'spot', 'state', 'strand', 'sun', 'supreme', 'tablepoon', 'tail', 'target', 'tm', 'tong', 'toothpick', 'triangle', 'trimming', 'tweezer', 'valley', 'vay', 'wise', 'wishbone', 'wrapper', 'yoplait', 'ziploc'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████| 341271/341271 [14:03<00:00, 404.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(79401,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(strip_accents='unicode', lowercase=True, preprocessor=custom_preprocessor, tokenizer=custom_lemmatizer, stop_words=flushtrated_list, ngram_range=(1,4))\n",
    "repo_transformed = cv.fit_transform(tqdm(recipe_megalist))\n",
    "cv.get_feature_names_out().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a68b5cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 202. GiB for an array with shape (341271, 79401) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.06.30 - JSON EDA3, Initial Visualizations.ipynb Cell 31'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.06.30%20-%20JSON%20EDA3%2C%20Initial%20Visualizations.ipynb#ch0000042vscode-remote?line=0'>1</a>\u001b[0m repo_transformed\u001b[39m.\u001b[39;49mtodense()\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/scipy/sparse/_base.py:936\u001b[0m, in \u001b[0;36mspmatrix.todense\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtodense\u001b[39m(\u001b[39mself\u001b[39m, order\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    907\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[39m    Return a dense matrix representation of this matrix.\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39m        `numpy.matrix` object that shares the same memory.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 936\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ascontainer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoarray(order\u001b[39m=\u001b[39;49morder, out\u001b[39m=\u001b[39;49mout))\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/scipy/sparse/_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m order \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1050\u001b[0m     order \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap(\u001b[39m'\u001b[39m\u001b[39mcf\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m-> 1051\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_toarray_args(order, out)\n\u001b[1;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mc_contiguous \u001b[39mor\u001b[39;00m out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mf_contiguous):\n\u001b[1;32m   1053\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOutput array must be C or F contiguous\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/scipy/sparse/_base.py:1288\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m   1287\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1288\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 202. GiB for an array with shape (341271, 79401) and data type int64"
     ]
    }
   ],
   "source": [
    "repo_transformed.todense().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbae5a4",
   "metadata": {},
   "source": [
    "We can try to filter out the adjectives in the lemmatization step, because spaCy allows filtering based on Parts of Speech. But this might exclude them from the ngrams. Let's try augmenting stopwords and excluding colors that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeef7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_20 = LatentDirichletAllocation(n_components=20, n_jobs=-1, verbose=100, random_state=200)\n",
    "lda_20_repo_transformed = lda_20.fit_transform(repo_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4fa39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_20, repo_transformed, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f044642",
   "metadata": {},
   "source": [
    "## Manual Topic Labeling Based on LDA\n",
    "1. aliums, alium prep, chocolate and rosemary\n",
    "2. oil, cheese, game meats\n",
    "3. peppers and parsley\n",
    "\n",
    "Based on these three topics, I think it is better to train a new model, since these models don't seem to carry much information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f1533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_auto_stopwords_085 = CountVectorizer(strip_accents='unicode', lowercase=True, preprocessor=custom_preprocessor, tokenizer=custom_lemmatizer, stop_words=None, ngram_range=(1,4), max_df=0.85)\n",
    "\n",
    "repo_transformed_auto_stopwords_085 = cv_auto_stopwords_085.fit_transform(tqdm(recipe_megalist))\n",
    "cv_auto_stopwords_085.get_feature_names_out().shape\n",
    "lda_20_auto_stopwords_085 = LatentDirichletAllocation(n_components=20, n_jobs=-1, verbose=100, random_state=200)\n",
    "lda_20_repo_transformed_auto_stopwords_085 = lda_20_auto_stopwords_085.fit_transform(repo_transformed_auto_stopwords_085)\n",
    "pyLDAvis.sklearn.prepare(lda_20_auto_stopwords_085, repo_transformed_auto_stopwords_085, cv_auto_stopwords_085)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05ca65",
   "metadata": {},
   "source": [
    "This LDA is probably less useful\n",
    "\n",
    "1. Has a lot of prep and units of measurement\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_to_exclude = {'red', 'green', 'black', 'yellow', 'white', 'inch', 'mince', 'chop', 'fry', 'trim', 'flat', 'beat', 'brown', 'golden', 'balsamic', 'halve', 'blue', 'divide', 'trim', 'unbleache', 'granulate'}\n",
    "flushtrated_augment = flushtrated.union(additional_to_exclude)\n",
    "flushtrated_augment = list(flushtrated_augment)\n",
    "\n",
    "cv_stopwords_aug = CountVectorizer(strip_accents='unicode', lowercase=True, preprocessor=custom_preprocessor, tokenizer=custom_lemmatizer, stop_words=flushtrated_augment, ngram_range=(1,4))\n",
    "\n",
    "repo_transformed_stopwords_aug = cv_stopwords_aug.fit_transform(tqdm(recipe_megalist))\n",
    "cv_stopwords_aug.get_feature_names_out().shape\n",
    "lda_20_stopwords_aug = LatentDirichletAllocation(n_components=20, n_jobs=-1, verbose=100, random_state=200)\n",
    "lda_20_repo_transformed_aug = lda_20_stopwords_aug.fit_transform(repo_transformed_stopwords_aug)\n",
    "pyLDAvis.sklearn.prepare(lda_20_stopwords_aug, repo_transformed_stopwords_aug, cv_stopwords_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfef373",
   "metadata": {},
   "source": [
    "These topic models/word groupings also don't seem to make much sense, so let's throw this into a TF-IDF and see what happens, even though the authors of LDA don't like doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "repo_tfidf_stopwords_aug = tfidf.fit_transform(repo_transformed_stopwords_aug)\n",
    "tfidf_lda_20_stopwords_aug = LatentDirichletAllocation(n_components=20, n_jobs=-1, verbose=100, random_state=200)\n",
    "tfidf_lda_20_repo_transformed_aug = tfidf_lda_20_stopwords_aug.fit_transform(repo_tfidf_stopwords_aug)\n",
    "pyLDAvis.sklearn.prepare(tfidf_lda_20_stopwords_aug, repo_tfidf_stopwords_aug, cv_stopwords_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62467b5",
   "metadata": {},
   "source": [
    "pyLDAvis calls to a deprecated function inside CountVectorizer, which is incompatible with TFIDF. Can we can find an alternate version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652331a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7142e37045ca6e2f7ebd069a40dd15c94e2caae7a512958c86ea35c1f050d31a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
