{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9afa101e-a9e5-4d1d-9e3e-416f1c767233",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis of Epicurious Scrape in a JSON file\n",
    "\n",
    "This is an idealized workflow for Aaron Chen in looking at data science problems. It likely isn't the best path, nor has he rigidly applied or stuck to this ideal, but he wishes that he worked this way more frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59c575-9aca-4d08-9b03-3425fa33cb03",
   "metadata": {},
   "source": [
    "## Purpose: Work through some exploratory data analysis of the Epicurious scrape on stream. Try to write some functions to help process the data.\n",
    "\n",
    "### Author: Aaron Chen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21b7d4-5980-4adf-a62b-38b8e89031ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d4a89-cf20-435c-bcc0-6f1ebcd127c7",
   "metadata": {},
   "source": [
    "### If needed, run shell commands here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed4e2d-6f77-4196-b548-d5896e13193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cede1d1-3a00-4471-91ae-50be260464a8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b66d7b-46ad-4f10-9cb9-72c03e319fea",
   "metadata": {},
   "source": [
    "## External Resources\n",
    "\n",
    "List out references or documentation that has helped you with this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b23fd3-6cf1-42b3-aee3-82306eaade6a",
   "metadata": {},
   "source": [
    "### Code\n",
    "Regex Checker: https://regex101.com/\n",
    "\n",
    "#### Scikit-learn\n",
    "1. https://scikit-learn.org/stable/modules/decomposition.html#latent-dirichlet-allocation-lda\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf83f44-334c-448a-afa7-187f63dd7285",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "For this notebook, the data is stored in the repo base folder/data/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0dcb8e-5a84-4e96-b07d-24909352935d",
   "metadata": {},
   "source": [
    "### Process\n",
    "\n",
    "Are there steps or tutorials you are following? Those are things I try to list in Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a44e4-081e-4f8f-88d2-3eccd0f2d767",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418bdb7-3ecf-4eb5-af6f-2c0e699d2394",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f480787a-b5ed-4bf2-8553-504d6cfffe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import project_path\n",
    "# from collections import defaultdict \n",
    "from datetime import datetime\n",
    "# import dvc.api\n",
    "# import gensim\n",
    "# import gensim.corpora as corpora\n",
    "# from gensim.models import CoherenceModel, Phrases\n",
    "# from gensim.utils import simple_preprocess\n",
    "# import logging\n",
    "# import mlflow\n",
    "# import mlflow.sklearn\n",
    "# import mlflow.spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# from spacy.lemmatizer import Lemmatizer\n",
    "from tqdm import tqdm\n",
    "# from typing import List\n",
    "import unicodedata\n",
    "\n",
    "import src.dataframe_preprocessor as dfpp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60dd0a-089e-41b7-9828-8f2b13648993",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde95259-0cd4-4ae1-8485-d51ada957529",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a3ea1e-a26e-4545-b950-ec823cbd6a26",
   "metadata": {},
   "source": [
    "My workflow is to try things with code cells, then when the code cells get messy and repetitive, to convert into helper functions that can be called.\n",
    "\n",
    "When the helper functions are getting used a lot, it is usually better to convert them to scripts or classes that can be called/instantiated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b922c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" This function takes in a pandas DataFrame from pd.read_json and performs some preprocessing by unpacking the nested dictionaries and creating new columns with the simplified structures. It will then drop the original columns that would no longer be needed.\n",
    "\n",
    "    Args:\n",
    "        pd.DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    def null_filler(to_check: dict['str','str'], key_target: str) -> str:\n",
    "        \"\"\" This function takes in a dictionary that is currently fed in with a lambda function and then performs column specific preprocessing.\n",
    "        \n",
    "        Args:\n",
    "            to_check: dict\n",
    "            key_target: str\n",
    "            \n",
    "        Returns:\n",
    "            str\n",
    "        \"\"\"\n",
    "\n",
    "        # Only look in the following keys, if the input isn't one of these, it should be recognized as an improper key\n",
    "        valid_keys = ['name', 'filename', 'credit']\n",
    "\n",
    "        # This dictionary converts the input keys into substrings that can be used in f-strings to fill in missing values in the record\n",
    "        translation_keys = {\n",
    "                            'name': \"Cuisine\"\n",
    "                            , 'filename': \"Photo\"\n",
    "                            , 'credit': \"Photo Credit\"\n",
    "                            }\n",
    "\n",
    "        if key_target not in valid_keys:\n",
    "            # this logic makes sure we are only looking at valid keys\n",
    "            return \"Improper key target: can only pick from 'name', 'filename', 'credit'.\"\n",
    "\n",
    "        else:\n",
    "            if pd.isna(to_check):\n",
    "                # this logic checks to see if the dictionary exists at all. if so, return Missing\n",
    "                return f'Missing {translation_keys[key_target]}'\n",
    "            else:\n",
    "                if key_target == 'name' and (to_check['category'] != 'cuisine'):\n",
    "                    # This logic checks for the cuisine, if the cuisine is not there (and instead has 'ingredient', 'type', 'item', 'equipment', 'meal'), mark as missing\n",
    "                    return f'Missing {translation_keys[key_target]}'\n",
    "                else:\n",
    "                    # Otherwise, there should be no issue with returning \n",
    "                    return to_check[key_target]\n",
    "                \n",
    "\n",
    "    # Dive into the tag column and extract the cuisine label. Put into new column or fills with \"missing data\"\n",
    "    df[\"cuisine_name\"] = df[\"tag\"].apply(lambda x: null_filler(to_check=x, key_target='name'))\n",
    "    # df[\"cuisine_name\"] = df[\"tag\"].apply(lambda x: x['name'] if not pd.isna(x) and x['category'] == 'cuisine' else 'Cuisine Missing')\n",
    "\n",
    "    # this lambda function goes into the photo data column and extracts just the filename from the dictionary\n",
    "    df[\"photo_filename\"] = df[\"photoData\"].apply(lambda x: null_filler(to_check=x, key_target='filename'))\n",
    "    # df[\"photo_filename\"] = df['photoData'].apply(lambda x: x['filename'] if not pd.isna(x) else 'Missing photo')\n",
    "\n",
    "    # This lambda function goes into the photo data column and extracts just the photo credit from the dictionary   \n",
    "    df[\"photo_credit\"] = df[\"photoData\"].apply(lambda x: null_filler(to_check=x, key_target='credit'))\n",
    "    # df[\"photo_credit\"] = df['photoData'].apply(lambda x: x['credit'] if not pd.isna(x) else 'Missing credit')\n",
    "\n",
    "    # for the above, maybe they can be refactored to one function where the arguments are a column name, dictionary key name, the substring return \n",
    "\n",
    "    # this lambda funciton goes into the author column and extract the author name or fills iwth \"missing data\"\n",
    "    df[\"author_name\"] = df[\"author\"].apply(lambda x: x[0]['name'] if x else 'Missing Author Name')\n",
    "\n",
    "    # This function takes in the given pubDate column and creates a new column with the pubDate values converted to datetime objects\n",
    "    df['date_published'] = pd.to_datetime(df['pubDate'], infer_datetime_format=True)\n",
    "    \n",
    "    # drop some original columns to clean up the dataframe\n",
    "    df.drop(labels=[\"tag\", 'photoData', \"author\", \"type\", 'dateCrawled', 'pubDate'], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd1d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee176cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import antigravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32db6018-d8b8-492f-927e-f1e7d7398588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_empties(deficiency_text: List) -> List:\n",
    "#     \"\"\"This function takes in a list of strings and removes empty strings from the list. The function is needed \n",
    "#     because if the list does not contain empty strings, the default remove() function returns None and an Error.\"\"\"\n",
    "    \n",
    "#     filtered = list(filter(lambda x: x != '', deficiency_text))\n",
    "\n",
    "#     return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6fb08d-a266-4785-8d88-e2a3ca89cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatizer(doc):\n",
    "#     # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "#     # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "#     doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "#     doc = u' '.join(doc)\n",
    "#     return nlp.make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b37e7b8-fa9c-45b5-96c1-fde99ed40f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_stopwords(doc):\n",
    "#     # This will remove stopwords and punctuation.\n",
    "#     # Use token.text to return strings, which we'll need for Gensim.\n",
    "#     doc = [token for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "#     return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10554bde-9215-47dc-a90d-9bc53fc7d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.add_pipe(lemmatizer,name='lemmatizer',after='ner')\n",
    "# nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fdd4b0-9acb-445e-acad-2b3eb6ab9e4c",
   "metadata": {},
   "source": [
    "### Import local script\n",
    "\n",
    "I started grouping this in with importing libraries, but putting them at the bottom of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae4f42-17a3-4564-95b7-7993e5989d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import project_path\n",
    "# import src.nhsn_vac_df_builder as nhsn_vac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1b7742-3b22-4f48-877f-9f32cd7251fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7445bf0-0b00-45b9-a2db-84c568bced29",
   "metadata": {},
   "source": [
    "## Define global variables \n",
    "### Remember to refactor these out, not ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d00c380-63ee-4141-a073-fbe725cb826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../data/recipes-en-201706/epicurious-recipes_m2.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab98b748-74ad-4dff-bebe-aa794f7a4591",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b325f7ef-07c7-46b1-8595-9749259da76b",
   "metadata": {},
   "source": [
    "## Running Commentary\n",
    "\n",
    "1. I used numbered lists to keep track of things I noticed\n",
    "\n",
    "### To Do\n",
    "\n",
    "1. Try to determine consistency of nested data structures\n",
    "   1. Is the photoData or number of things inside photoData the same from record to record\n",
    "   2. What about for tag?\n",
    "\n",
    "Data wasn't fully consistent but logic in helper function helped handle nulls\n",
    "\n",
    "2. How to handle nulls?\n",
    "   1. Author      Filled in with \"Missing Author\"\n",
    "   2. Tag         Filled in with \"Missing Cuisine\"\n",
    "3. ~~Convert pubDate to actual timestamp~~  \n",
    "4. ~~Convert ScrapeDate to actual timestamp~~\n",
    "   1. This was ignored as the datestamp was not useful (generally within minutes of the origin of UNIX time)\n",
    "   \n",
    "**5. Append new columns for relevant nested structures and unfold them**\n",
    "\n",
    "6. Determine actual types of `ingredients` and `prepSteps`\n",
    "7. Continue working through test example of single recipe to feed into spaCy and then sklearn.feature_extraction.text stack\n",
    "8. Will need to remove numbers, punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2173e5-db8a-4fff-8072-98d6b16f7a4d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf870da-c2ed-4cde-9dd8-1591cbd2f24c",
   "metadata": {},
   "source": [
    "## Importing and viewing the data as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d4ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe = pd.read_json(path_or_buf=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db92c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e284211",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ced96",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5349e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7bd715",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe['aggregateRating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c401fb53",
   "metadata": {},
   "source": [
    "Columns:\n",
    "\n",
    "    Index\n",
    "\n",
    "    ID: string\n",
    "\n",
    "    dek: appears to be description of the recipe, string\n",
    "\n",
    "    hed: Appears to be title, string\n",
    "\n",
    "    pubDate: appears to be publication date, may need to reformat to datetime objects\n",
    "\n",
    "    author: appears that each record contains an array (list), inside each list is a dictionary with 'name' as the key and author name as the value. Notably, not a unique identifier for the value. Because the data is technically nested, may need to extract and transform and add columns to dataframe\n",
    "\n",
    "    type: string, but all the values are exactly the same and they are all in the category of \"recipe\". Drop column\n",
    "\n",
    "    url: Appears to be a long string leading to where the recipe can be found on Epicurious's website\n",
    "\n",
    "    photoData: nested structure, inside each record is a dictionary\n",
    "\n",
    "    tag: each record contains a dictionary. may need to extract and transform and add columns to dataframe\n",
    "\n",
    "    aggregateRating: float, let's say it's out of 4.0\n",
    "\n",
    "    ingredients: appears to be a list, does look like a list of strings\n",
    "\n",
    "    prepSteps: appears to be a list, does look like a list of strings\n",
    "\n",
    "    reviewsCount: int\n",
    "\n",
    "    willMakeAgainPct: integer\t\n",
    "    \n",
    "    dateCrawled: appears to be a unix timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14af2568",
   "metadata": {},
   "source": [
    "Let's take a look at the possibly problematic columns and see if the data structures make sense or how we can approach transforming them into new columns for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a334155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c941bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe.loc[0][\"photoData\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed7c58c",
   "metadata": {},
   "source": [
    "It looks like photoData contains:\n",
    "    1. photo ID, string\n",
    "    2. photo filename, string\n",
    "    3. photo caption, string\n",
    "    4. photo credit, string\n",
    "    5. promoTitle, string\n",
    "    6. title, string\n",
    "       1. caption, promoTitle, and title could be all the same\n",
    "    7. orientation, string\n",
    "    8. restrictCropping: boolean\n",
    "\n",
    "Of these, maybe we should keep\n",
    "id => photoID\n",
    "filename => photoFilename\n",
    "caption => photoCaption\n",
    "credit => photoCredit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e8d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe.loc[0][\"tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a17062",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe.loc[100][\"tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cec3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe.loc[10][\"tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f15f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe.loc[1][\"tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19eba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe.loc[1][\"ingredients\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52485966",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(epic_dataframe.loc[1][\"ingredients\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe.loc[1][\"dek\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99416f09",
   "metadata": {},
   "source": [
    "### Let's skip ahead and throw this into CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d01275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recipes_list = epic_dataframe['ingredients'].str.join(\" \")\n",
    "# .apply(\" \".join).str.lower()\n",
    "#print(type(all_recipes_list))\n",
    "all_recipes_list\n",
    "# for i in range(0,5):\n",
    "#     print((i, all_recipes_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e9fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b73ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_rec = all_recipes_list[0]\n",
    "print(first_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_first_rec = nlp(first_rec)\n",
    "for token in doc_first_rec:\n",
    "    if token.like_num == False:\n",
    "        print(token.text, token.pos_, token.ent_type_, token.lemma_, token.is_digit)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae9863",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe['tag'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe['cuisine_name'] = epic_dataframe['tag'].apply(lambda x: x['name'] if not pd.isna(x) and x['category'] == 'cuisine' else 'Cuisine Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe['cuisine_name'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe['cuisine_name'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016802d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epic_dataframe.shape)\n",
    "print(epic_dataframe[epic_dataframe['cuisine_name'] != 'Missing'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0138c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe[epic_dataframe['cuisine_name'] == 'Cuisine Missing'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad05b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this lambda function goes into the photo data column and extracts just the filename from the dictionary\n",
    "epic_dataframe[\"photo_filename\"] = epic_dataframe['photoData'].apply(lambda x: x['filename'] if not pd.isna(x) \n",
    " else 'Missing photo')\n",
    "\n",
    "# This lambda function goes into the photo data column and extracts just the photo credit from the dictionary \n",
    "epic_dataframe[\"photo_credit\"] = epic_dataframe['photoData'].apply(lambda x: x['credit'] if not pd.isna(x) \n",
    " else 'Missing credit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a35aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac81ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This lambda function cleans up the column and adds a new column dataframe\n",
    "epic_dataframe[\"author_name\"] = epic_dataframe['author'].apply(lambda x: x[0]['name'] if not pd.isna(x) \n",
    " else 'Missing author name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d5a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_five_epic = epic_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffdc72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_five_epic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6083ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_five_epic.iloc[0][\"author\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a2822",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_five_epic.iloc[1][\"author\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7697a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_five_epic[\"author\"].apply(lambda x: x[0]['name'] if x else 'Missing author name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2a753",
   "metadata": {},
   "source": [
    "This lambda function works enough! It goes into author column and extracts the author as long as the record isn't an empty list. This can be refactored into a helper function. But we need to apply to the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe[\"author_name\"] = epic_dataframe[\"author\"].apply(lambda x: x[0]['name'] if x else 'Missing author name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da2cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191d3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe2 = preprocess_dataframe(pd.read_json(path_or_buf=data_path))\n",
    "\n",
    "epic_dataframe2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56dfa9c",
   "metadata": {},
   "source": [
    "## Let's add a feature to fix the datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2189ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pubdate_array = epic_dataframe2['pubDate'][0:5]\n",
    "test_pubdate_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6a057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(test_pubdate_array[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a78755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pubdate_array[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ef9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe2['publication_date'] = epic_dataframe2['pubDate'].apply(lambda x: datetime.strptime(x[:10], \"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe2['publication_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe2['publication_date_todt'] = pd.to_datetime(epic_dataframe2['pubDate'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6abd69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe2['publication_date_todt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662959d6",
   "metadata": {},
   "source": [
    "Don't need the apply with lambda function anymore because to_datetime succesfully resolved the odd string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a3d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe2['date_scraped'] = pd.to_datetime(epic_dataframe2['dateCrawled'], infer_datetime_format=True)\n",
    "epic_dataframe2['date_scraped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b89d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe2['date_scraped'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be44977e",
   "metadata": {},
   "source": [
    "Based on the timestamps, it seems like we can drop the crawled/scraped column because the values don't really make sense and would not help\n",
    "\n",
    "To Do after break:\n",
    "- Refactor datetime processing into functions\n",
    "- Consider deploying as a pd.pipe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb6b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe = pd.read_json(path_or_buf=data_path)\n",
    "preprocess_dataframe(epic_dataframe)\n",
    "epic_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a797e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc34727",
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf26c212",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Series' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.05.19 - JSON source data EDA.ipynb Cell 91'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.05.19%20-%20JSON%20source%20data%20EDA.ipynb#ch0000095vscode-remote?line=0'>1</a>\u001b[0m data_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../../data/recipes-en-201706/epicurious-recipes_m2.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.05.19%20-%20JSON%20source%20data%20EDA.ipynb#ch0000095vscode-remote?line=2'>3</a>\u001b[0m epic_dataframe \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_json(data_path, typ\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.05.19%20-%20JSON%20source%20data%20EDA.ipynb#ch0000095vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(epic_dataframe, Series[Unknown] ):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.05.19%20-%20JSON%20source%20data%20EDA.ipynb#ch0000095vscode-remote?line=5'>6</a>\u001b[0m     epic_dataframe\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.05.19%20-%20JSON%20source%20data%20EDA.ipynb#ch0000095vscode-remote?line=6'>7</a>\u001b[0m el\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Series' is not defined"
     ]
    }
   ],
   "source": [
    "data_path = \"../../data/recipes-en-201706/epicurious-recipes_m2.json\"\n",
    "\n",
    "epic_dataframe = pd.read_json(data_path, typ='frame')\n",
    "\n",
    "dfpp.preprocess_dataframe(df=epic_dataframe)\n",
    "epic_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c25daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is looking for accented characters inside text, which may not be necessary\n",
    "for word in test_rec_list:     \n",
    "    print(unicodedata.normalize(\"NFKD\", word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb61f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(input=aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ba907d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7142e37045ca6e2f7ebd069a40dd15c94e2caae7a512958c86ea35c1f050d31a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
