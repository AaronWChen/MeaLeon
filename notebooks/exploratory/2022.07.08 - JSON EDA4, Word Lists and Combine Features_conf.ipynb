{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9afa101e-a9e5-4d1d-9e3e-416f1c767233",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis of Epicurious Scrape in a JSON file\n",
    "\n",
    "This is an idealized workflow for Aaron Chen in looking at data science problems. It likely isn't the best path, nor has he rigidly applied or stuck to this ideal, but he wishes that he worked this way more frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59c575-9aca-4d08-9b03-3425fa33cb03",
   "metadata": {},
   "source": [
    "## Purpose: Work through some exploratory data analysis of the Epicurious scrape on stream. Try to write some functions to help process the data.\n",
    "\n",
    "### Author: Aaron Chen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21b7d4-5980-4adf-a62b-38b8e89031ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d4a89-cf20-435c-bcc0-6f1ebcd127c7",
   "metadata": {},
   "source": [
    "### If needed, run shell commands here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ed4e2d-6f77-4196-b548-d5896e13193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cede1d1-3a00-4471-91ae-50be260464a8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b66d7b-46ad-4f10-9cb9-72c03e319fea",
   "metadata": {},
   "source": [
    "## External Resources\n",
    "\n",
    "List out references or documentation that has helped you with this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b23fd3-6cf1-42b3-aee3-82306eaade6a",
   "metadata": {},
   "source": [
    "### Code\n",
    "Regex Checker: https://regex101.com/\n",
    "\n",
    "#### Scikit-learn\n",
    "1. https://scikit-learn.org/stable/modules/decomposition.html#latent-dirichlet-allocation-lda\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf83f44-334c-448a-afa7-187f63dd7285",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "For this notebook, the data is stored in the repo base folder/data/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0dcb8e-5a84-4e96-b07d-24909352935d",
   "metadata": {},
   "source": [
    "### Process\n",
    "\n",
    "Are there steps or tutorials you are following? Those are things I try to list in Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a44e4-081e-4f8f-88d2-3eccd0f2d767",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418bdb7-3ecf-4eb5-af6f-2c0e699d2394",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f480787a-b5ed-4bf2-8553-504d6cfffe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# from spacy.lemmatizer import Lemmatizer\n",
    "from tqdm import tqdm\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60dd0a-089e-41b7-9828-8f2b13648993",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde95259-0cd4-4ae1-8485-d51ada957529",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a3ea1e-a26e-4545-b950-ec823cbd6a26",
   "metadata": {},
   "source": [
    "My workflow is to try things with code cells, then when the code cells get messy and repetitive, to convert into helper functions that can be called.\n",
    "\n",
    "When the helper functions are getting used a lot, it is usually better to convert them to scripts or classes that can be called/instantiated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2dbd771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lemmatizer(ingredients: list) -> Any: # spacy nlp.Doc\n",
    "    \"\"\"This takes in a string representing the recipe and an NLP model and lemmatize with the NER. \n",
    "    \n",
    "    Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    Remove punctuation\n",
    "\n",
    "    Args:\n",
    "        ingredients: string\n",
    "        nlp_mod: spacy model (try built in first, by default called nlp)\n",
    "    \n",
    "    Returns:\n",
    "        nlp.Doc\n",
    "    \"\"\"\n",
    "    lemmas = [token.lemma_ for token in ingredients if (token.is_alpha and token.pos_ not in [\"PRON\", \"VERB\"] and len(token.lemma_) > 1)]\n",
    "    return lemmas\n",
    "    # return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27b6d33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_preprocessor(recipe_ingreds: str) -> list:\n",
    "    \"\"\"This function replaces the default sklearn CountVectorizer preprocessor to use spaCy. sklearn CountVectorizer's preprocessor only performs accent removal and lowercasing.\n",
    "\n",
    "    Args:\n",
    "        A string to tokenize from a recipe representing the ingredients used in the recipe\n",
    "\n",
    "    Returns:\n",
    "        A list of strings that have been de-accented and lowercased to be used in tokenization\n",
    "    \"\"\"\n",
    "    preprocessed = [token for token in nlp(recipe_ingreds)]\n",
    "\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a12bdc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d61885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_matrices_to_df(df, vectorized_ingred_matrix, cv):\n",
    "    \"\"\"This function takes in a dataframe and concats the matrix generated by either CountVectorizer or TFIDF-Transformer onto the records so that the recipes can be used for classification purposes.\n",
    "\n",
    "    Args: \n",
    "        df: preprocessed dataframe from preprocess_dataframe\n",
    "        vectorized_ingred_matrix: sparse csr matrix created from doing fit_transform on the recipe_megalist\n",
    "     \n",
    "    Returns:\n",
    "        A pandas dataframe with the vectorized_ingred_matrix appended as columns to df\n",
    "    \"\"\"\n",
    "    repo_tfidf_df = pd.DataFrame(vectorized_ingred_matrix.toarray(), columns=cv.get_feature_names_out(), index=df.index)\n",
    "    return pd.concat([df, repo_tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f735db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuisine_namer(text):\n",
    "    \"\"\"This function converts redundant and/or rare categories into more common\n",
    "    ones/umbrella ones.\n",
    "\n",
    "    In the future, there's a hope that this renaming mechanism will not have\n",
    "    under sampled cuisine tags.\n",
    "    \"\"\"\n",
    "    if text == \"Central American/Caribbean\":\n",
    "        return \"Caribbean\"\n",
    "    elif text == \"Jewish\":\n",
    "        return \"Kosher\"\n",
    "    elif text == \"Eastern European/Russian\":\n",
    "        return \"Eastern European\"\n",
    "    elif text in [\"Spanish/Portuguese\", \"Greek\"]:\n",
    "        return \"Mediterranean\"\n",
    "    elif text == \"Central/South American\":\n",
    "        return \"Latin American\"\n",
    "    elif text == \"Sushi\":\n",
    "        return \"Japanese\"\n",
    "    elif text == \"Southern Italian\":\n",
    "        return \"Italian\"\n",
    "    elif text in [\"Southern\", \"Tex-Mex\"]:\n",
    "        return \"American\"\n",
    "    elif text in [\"Southeast Asian\", \"Korean\"]:\n",
    "        return \"Asian\"\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fdd4b0-9acb-445e-acad-2b3eb6ab9e4c",
   "metadata": {},
   "source": [
    "### Import local script\n",
    "\n",
    "I started grouping this in with importing libraries, but putting them at the bottom of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27ae4f42-17a3-4564-95b7-7993e5989d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import project_path\n",
    "\n",
    "import src.dataframe_preprocessor as dfpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1b7742-3b22-4f48-877f-9f32cd7251fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7445bf0-0b00-45b9-a2db-84c568bced29",
   "metadata": {},
   "source": [
    "## Define global variables \n",
    "### Remember to refactor these out, not ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d00c380-63ee-4141-a073-fbe725cb826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../data/recipes-en-201706/epicurious-recipes_m2.json\"\n",
    "food_stopwords_path = \"../../food_stopwords.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab98b748-74ad-4dff-bebe-aa794f7a4591",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b325f7ef-07c7-46b1-8595-9749259da76b",
   "metadata": {},
   "source": [
    "## Running Commentary\n",
    "\n",
    "1. I used numbered lists to keep track of things I noticed\n",
    "\n",
    "### To Do\n",
    "\n",
    "1. Try to determine consistency of nested data structures\n",
    "   1. Is the photoData or number of things inside photoData the same from record to record\n",
    "   2. What about for tag?\n",
    "\n",
    "Data wasn't fully consistent but logic in helper function helped handle nulls\n",
    "\n",
    "2. How to handle nulls?\n",
    "   1. Author      Filled in with \"Missing Author\"\n",
    "   2. Tag         Filled in with \"Missing Cuisine\"\n",
    "3. ~~Convert pubDate to actual timestamp~~  \n",
    "4. ~~Convert ScrapeDate to actual timestamp~~\n",
    "   1. This was ignored as the datestamp was not useful (generally within minutes of the origin of UNIX time)\n",
    "   \n",
    "**5. Append new columns for relevant nested structures and unfold them**\n",
    "\n",
    "6. Determine actual types of `ingredients` and `prepSteps`\n",
    "7. Continue working through test example of single recipe to feed into spaCy and then sklearn.feature_extraction.text stack\n",
    "8. Will need to remove numbers, punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2173e5-db8a-4fff-8072-98d6b16f7a4d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf870da-c2ed-4cde-9dd8-1591cbd2f24c",
   "metadata": {},
   "source": [
    "## Importing and viewing the data as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f96d4ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34656, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "repo = pd.read_json(path_or_buf=data_path) # type:ignore\n",
    "pd.read_json(data_path, typ='frame') # type:ignore\n",
    "\n",
    "dfpp.preprocess_dataframe(df=repo) # type:ignore\n",
    "print(repo.shape)\n",
    "repo.head(10) # type:ignore\n",
    "\n",
    "recipe_megalist = [ingred for recipe in repo['ingredients'].tolist() for ingred in recipe]\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# this is a redeem for variable naming mixed with a free pun-ish me daddy, flushtrated will be the list of all stopword to exclude so named because we're throwing these words down the drain\n",
    "\n",
    "flushtrated = {x for x in pd.read_csv(food_stopwords_path)}\n",
    "additional_to_exclude = {'red', 'green', 'black', 'yellow', 'white', 'inch', 'mince', 'chop', 'fry', 'trim', 'flat', 'beat', 'brown', 'golden', 'balsamic', 'halve', 'blue', 'divide', 'trim', 'unbleache', 'granulate', 'Frank', 'alternative', 'american', 'annie', 'asian', 'balance', 'band', 'barrel', 'bay', 'bayou', 'beam', 'beard', 'bell', 'betty', 'bird', 'blast', 'bob', 'bone', 'breyers', 'calore', 'carb', 'card', 'chachere', 'change', 'circle', 'coffee', 'coil', 'country', 'cow', 'crack', 'cracker', 'crocker', 'crystal', 'dean', 'degree', 'deluxe', 'direction', 'duncan', 'earth', 'eggland', 'ener', 'envelope', 'eye', 'fantastic', 'far', 'fat', 'feather', 'flake', 'foot', 'fourth', 'frank', 'french', 'fusion', 'genoa', 'genovese', 'germain', 'giada', 'gold', 'granule', 'greek', 'hamburger', 'helper', 'herbe', 'hines', 'hodgson', 'hunt', 'instruction', 'interval', 'italianstyle', 'jim', 'jimmy', 'kellogg', 'lagrille', 'lake', 'land', 'laurentiis', 'lawry', 'lipton', 'litre', 'll', 'maid', 'malt', 'mate', 'mayer', 'meal', 'medal', 'medallion', 'member', 'mexicanstyle', 'monte', 'mori', 'nest', 'nu', 'oounce', 'oscar', 'ox', 'paso', 'pasta', 'patty', 'petal', 'pinche', 'preserve', 'quartere', 'ranch', 'ranchstyle', 'rasher', 'redhot', 'resemble', 'rice', 'ro', 'roni', 'scissor', 'scrap', 'secret', 'semicircle', 'shard', 'shear', 'sixth', 'sliver', 'smucker', 'snicker', 'source', 'spot', 'state', 'strand', 'sun', 'supreme', 'tablepoon', 'tail', 'target', 'tm', 'tong', 'toothpick', 'triangle', 'trimming', 'tweezer', 'valley', 'vay', 'wise', 'wishbone', 'wrapper', 'yoplait', 'ziploc'}\n",
    "\n",
    "flushtrated = flushtrated.union(STOP_WORDS)\n",
    "flushtrated = flushtrated.union(additional_to_exclude)\n",
    "flushtrated_list = list(flushtrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4884b9c2-9f20-491e-97a5-5cd4975261cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34656, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dek</th>\n",
       "      <th>hed</th>\n",
       "      <th>aggregateRating</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>prepSteps</th>\n",
       "      <th>reviewsCount</th>\n",
       "      <th>willMakeAgainPct</th>\n",
       "      <th>cuisine_name</th>\n",
       "      <th>photo_filename</th>\n",
       "      <th>photo_credit</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date_published</th>\n",
       "      <th>recipe_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54a2b6b019925f464b373351</td>\n",
       "      <td>How does fried chicken achieve No. 1 status? B...</td>\n",
       "      <td>Pickle-Brined Fried Chicken</td>\n",
       "      <td>3.11</td>\n",
       "      <td>[1 tablespoons yellow mustard seeds, 1 tablesp...</td>\n",
       "      <td>[Toast mustard and coriander seeds in a dry me...</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>51247610_fried-chicken_1x1.jpg</td>\n",
       "      <td>Michael Graydon and Nikole Herriott</td>\n",
       "      <td>Missing Author Name</td>\n",
       "      <td>2014-08-19 04:00:00+00:00</td>\n",
       "      <td>https://www.epicurious.com/recipes/food/views/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54a408a019925f464b3733bc</td>\n",
       "      <td>Spinaci all'Ebraica</td>\n",
       "      <td>Spinach Jewish Style</td>\n",
       "      <td>3.22</td>\n",
       "      <td>[3 pounds small-leaved bulk spinach, Salt, 1/2...</td>\n",
       "      <td>[Remove the stems and roots from the spinach. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>Italian</td>\n",
       "      <td>EP_12162015_placeholders_rustic.jpg</td>\n",
       "      <td>Photo by Chelsea Kyle, Prop Styling by Anna St...</td>\n",
       "      <td>Edda Servi Machlin</td>\n",
       "      <td>2008-09-09 04:00:00+00:00</td>\n",
       "      <td>https://www.epicurious.com/recipes/food/views/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54a408a26529d92b2c003631</td>\n",
       "      <td>This majestic, moist, and richly spiced honey ...</td>\n",
       "      <td>New Year’s Honey Cake</td>\n",
       "      <td>3.62</td>\n",
       "      <td>[3 1/2 cups all-purpose flour, 1 tablespoon ba...</td>\n",
       "      <td>[I like this cake best baked in a 9-inch angel...</td>\n",
       "      <td>105</td>\n",
       "      <td>88</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>EP_09022015_honeycake-2.jpg</td>\n",
       "      <td>Photo by Chelsea Kyle, Food Styling by Anna St...</td>\n",
       "      <td>Marcy Goldman</td>\n",
       "      <td>2008-09-10 04:00:00+00:00</td>\n",
       "      <td>https://www.epicurious.com/recipes/food/views/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54a408a66529d92b2c003638</td>\n",
       "      <td>The idea for this sandwich came to me when my ...</td>\n",
       "      <td>The B.L.A.Bagel with Lox and Avocado</td>\n",
       "      <td>4.00</td>\n",
       "      <td>[1 small ripe avocado, preferably Hass (see No...</td>\n",
       "      <td>[A short time before serving, mash avocado and...</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>EP_12162015_placeholders_casual.jpg</td>\n",
       "      <td>Photo by Chelsea Kyle, Prop Styling by Rhoda B...</td>\n",
       "      <td>Faye Levy</td>\n",
       "      <td>2008-09-08 04:00:00+00:00</td>\n",
       "      <td>https://www.epicurious.com/recipes/food/views/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54a408a719925f464b3733cc</td>\n",
       "      <td>In 1930, Simon Agranat, the chief justice of t...</td>\n",
       "      <td>Shakshuka a la Doktor Shakshuka</td>\n",
       "      <td>2.71</td>\n",
       "      <td>[2 pounds fresh tomatoes, unpeeled and cut in ...</td>\n",
       "      <td>[1. Place the tomatoes, garlic, salt, paprika,...</td>\n",
       "      <td>7</td>\n",
       "      <td>83</td>\n",
       "      <td>Jewish</td>\n",
       "      <td>EP_12162015_placeholders_formal.jpg</td>\n",
       "      <td>Photo by Chelsea Kyle, Prop Styling by Rhoda B...</td>\n",
       "      <td>Joan Nathan</td>\n",
       "      <td>2008-09-09 04:00:00+00:00</td>\n",
       "      <td>https://www.epicurious.com/recipes/food/views/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  54a2b6b019925f464b373351   \n",
       "1  54a408a019925f464b3733bc   \n",
       "2  54a408a26529d92b2c003631   \n",
       "3  54a408a66529d92b2c003638   \n",
       "4  54a408a719925f464b3733cc   \n",
       "\n",
       "                                                 dek  \\\n",
       "0  How does fried chicken achieve No. 1 status? B...   \n",
       "1                                Spinaci all'Ebraica   \n",
       "2  This majestic, moist, and richly spiced honey ...   \n",
       "3  The idea for this sandwich came to me when my ...   \n",
       "4  In 1930, Simon Agranat, the chief justice of t...   \n",
       "\n",
       "                                     hed  aggregateRating  \\\n",
       "0            Pickle-Brined Fried Chicken             3.11   \n",
       "1                   Spinach Jewish Style             3.22   \n",
       "2                  New Year’s Honey Cake             3.62   \n",
       "3  The B.L.A.Bagel with Lox and Avocado             4.00   \n",
       "4        Shakshuka a la Doktor Shakshuka             2.71   \n",
       "\n",
       "                                         ingredients  \\\n",
       "0  [1 tablespoons yellow mustard seeds, 1 tablesp...   \n",
       "1  [3 pounds small-leaved bulk spinach, Salt, 1/2...   \n",
       "2  [3 1/2 cups all-purpose flour, 1 tablespoon ba...   \n",
       "3  [1 small ripe avocado, preferably Hass (see No...   \n",
       "4  [2 pounds fresh tomatoes, unpeeled and cut in ...   \n",
       "\n",
       "                                           prepSteps  reviewsCount  \\\n",
       "0  [Toast mustard and coriander seeds in a dry me...             7   \n",
       "1  [Remove the stems and roots from the spinach. ...             5   \n",
       "2  [I like this cake best baked in a 9-inch angel...           105   \n",
       "3  [A short time before serving, mash avocado and...             7   \n",
       "4  [1. Place the tomatoes, garlic, salt, paprika,...             7   \n",
       "\n",
       "   willMakeAgainPct     cuisine_name                       photo_filename  \\\n",
       "0               100  Missing Cuisine       51247610_fried-chicken_1x1.jpg   \n",
       "1                80          Italian  EP_12162015_placeholders_rustic.jpg   \n",
       "2                88           Jewish          EP_09022015_honeycake-2.jpg   \n",
       "3               100           Jewish  EP_12162015_placeholders_casual.jpg   \n",
       "4                83           Jewish  EP_12162015_placeholders_formal.jpg   \n",
       "\n",
       "                                        photo_credit          author_name  \\\n",
       "0                Michael Graydon and Nikole Herriott  Missing Author Name   \n",
       "1  Photo by Chelsea Kyle, Prop Styling by Anna St...   Edda Servi Machlin   \n",
       "2  Photo by Chelsea Kyle, Food Styling by Anna St...        Marcy Goldman   \n",
       "3  Photo by Chelsea Kyle, Prop Styling by Rhoda B...            Faye Levy   \n",
       "4  Photo by Chelsea Kyle, Prop Styling by Rhoda B...          Joan Nathan   \n",
       "\n",
       "             date_published                                         recipe_url  \n",
       "0 2014-08-19 04:00:00+00:00  https://www.epicurious.com/recipes/food/views/...  \n",
       "1 2008-09-09 04:00:00+00:00  https://www.epicurious.com/recipes/food/views/...  \n",
       "2 2008-09-10 04:00:00+00:00  https://www.epicurious.com/recipes/food/views/...  \n",
       "3 2008-09-08 04:00:00+00:00  https://www.epicurious.com/recipes/food/views/...  \n",
       "4 2008-09-09 04:00:00+00:00  https://www.epicurious.com/recipes/food/views/...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(repo.shape)\n",
    "repo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0715280d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/341271 [00:00<?, ?it/s]/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['breyer', 'hine'] not in stop_words.\n",
      "  warnings.warn(\n",
      " 20%|█▉        | 68010/341271 [03:09<12:41, 358.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08 - JSON EDA4, Word Lists and Combine Features_conf.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08%20-%20JSON%20EDA4%2C%20Word%20Lists%20and%20Combine%20Features_conf.ipynb#ch0000032vscode-remote?line=0'>1</a>\u001b[0m cv \u001b[39m=\u001b[39m CountVectorizer(strip_accents\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39municode\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08%20-%20JSON%20EDA4%2C%20Word%20Lists%20and%20Combine%20Features_conf.ipynb#ch0000032vscode-remote?line=1'>2</a>\u001b[0m                         lowercase\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08%20-%20JSON%20EDA4%2C%20Word%20Lists%20and%20Combine%20Features_conf.ipynb#ch0000032vscode-remote?line=2'>3</a>\u001b[0m                         preprocessor\u001b[39m=\u001b[39mcustom_preprocessor, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08%20-%20JSON%20EDA4%2C%20Word%20Lists%20and%20Combine%20Features_conf.ipynb#ch0000032vscode-remote?line=7'>8</a>\u001b[0m                         min_df\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08%20-%20JSON%20EDA4%2C%20Word%20Lists%20and%20Combine%20Features_conf.ipynb#ch0000032vscode-remote?line=8'>9</a>\u001b[0m                         )\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08%20-%20JSON%20EDA4%2C%20Word%20Lists%20and%20Combine%20Features_conf.ipynb#ch0000032vscode-remote?line=10'>11</a>\u001b[0m cv\u001b[39m.\u001b[39;49mfit(tqdm(recipe_megalist))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08%20-%20JSON%20EDA4%2C%20Word%20Lists%20and%20Combine%20Features_conf.ipynb#ch0000032vscode-remote?line=12'>13</a>\u001b[0m temp \u001b[39m=\u001b[39m repo[\u001b[39m\"\u001b[39m\u001b[39mingredients\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin)\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mlower()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08%20-%20JSON%20EDA4%2C%20Word%20Lists%20and%20Combine%20Features_conf.ipynb#ch0000032vscode-remote?line=14'>15</a>\u001b[0m repo_transformed \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39mtransform(tqdm(temp))\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1291\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[39m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[1;32m   1276\u001b[0m \n\u001b[1;32m   1277\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1288\u001b[0m \u001b[39m    Fitted vectorizer.\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m-> 1291\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[1;32m   1292\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m             )\n\u001b[1;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1208\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1209\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1210\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "\u001b[1;32m/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08 - JSON EDA4, Word Lists and Combine Features_conf.ipynb Cell 33\u001b[0m in \u001b[0;36mcustom_preprocessor\u001b[0;34m(recipe_ingreds)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08%20-%20JSON%20EDA4%2C%20Word%20Lists%20and%20Combine%20Features_conf.ipynb#ch0000032vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcustom_preprocessor\u001b[39m(recipe_ingreds: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08%20-%20JSON%20EDA4%2C%20Word%20Lists%20and%20Combine%20Features_conf.ipynb#ch0000032vscode-remote?line=1'>2</a>\u001b[0m     \u001b[39m\"\"\"This function replaces the default sklearn CountVectorizer preprocessor to use spaCy. sklearn CountVectorizer's preprocessor only performs accent removal and lowercasing.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08%20-%20JSON%20EDA4%2C%20Word%20Lists%20and%20Combine%20Features_conf.ipynb#ch0000032vscode-remote?line=2'>3</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08%20-%20JSON%20EDA4%2C%20Word%20Lists%20and%20Combine%20Features_conf.ipynb#ch0000032vscode-remote?line=3'>4</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08%20-%20JSON%20EDA4%2C%20Word%20Lists%20and%20Combine%20Features_conf.ipynb#ch0000032vscode-remote?line=7'>8</a>\u001b[0m \u001b[39m        A list of strings that have been de-accented and lowercased to be used in tokenization\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08%20-%20JSON%20EDA4%2C%20Word%20Lists%20and%20Combine%20Features_conf.ipynb#ch0000032vscode-remote?line=8'>9</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08%20-%20JSON%20EDA4%2C%20Word%20Lists%20and%20Combine%20Features_conf.ipynb#ch0000032vscode-remote?line=9'>10</a>\u001b[0m     preprocessed \u001b[39m=\u001b[39m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(recipe_ingreds)]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/notebooks/exploratory/2022.07.08%20-%20JSON%20EDA4%2C%20Word%20Lists%20and%20Combine%20Features_conf.ipynb#ch0000032vscode-remote?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m preprocessed\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/spacy/language.py:1020\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1019\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1020\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1022\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/spacy/pipeline/transition_parser.pyx:250\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/spacy/pipeline/transition_parser.pyx:265\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/spacy/ml/tb_framework.py:33\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model, X, is_train):\n\u001b[0;32m---> 33\u001b[0m     step_model \u001b[39m=\u001b[39m ParserStepModel(\n\u001b[1;32m     34\u001b[0m         X,\n\u001b[1;32m     35\u001b[0m         model\u001b[39m.\u001b[39;49mlayers,\n\u001b[1;32m     36\u001b[0m         unseen_classes\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mattrs[\u001b[39m\"\u001b[39;49m\u001b[39munseen_classes\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     37\u001b[0m         train\u001b[39m=\u001b[39;49mis_train,\n\u001b[1;32m     38\u001b[0m         has_upper\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mattrs[\u001b[39m\"\u001b[39;49m\u001b[39mhas_upper\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m step_model, step_model\u001b[39m.\u001b[39mfinish_steps\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/spacy/ml/parser_model.pyx:218\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/thinc/model.py:170\u001b[0m, in \u001b[0;36mModel.get_dim\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dim\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    169\u001b[0m     \u001b[39m\"\"\"Retrieve the value of a dimension of the given name.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dims:\n\u001b[1;32m    171\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot get dimension \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for model \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dims[name]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(strip_accents='unicode', \n",
    "                        lowercase=True, \n",
    "                        preprocessor=custom_preprocessor, \n",
    "                        tokenizer=custom_lemmatizer, \n",
    "                        stop_words=flushtrated_list, \n",
    "                        token_pattern=r\"(?u)\\b[a-zA-Z]{2,}\\b\", \n",
    "                        ngram_range=(1,4), \n",
    "                        min_df=10\n",
    "                        )\n",
    "\n",
    "cv.fit(tqdm(recipe_megalist))\n",
    "\n",
    "temp = repo[\"ingredients\"].apply(\" \".join).str.lower()\n",
    "\n",
    "repo_transformed = cv.transform(tqdm(temp))\n",
    "\n",
    "cv.get_feature_names_out().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a68b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286cd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "\n",
    "repo_tfidf = tfidf.fit_transform(repo_transformed)\n",
    "\n",
    "repo_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2110f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_with_cv = concat_matrices_to_df(repo, repo_tfidf, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_with_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbae5a4",
   "metadata": {},
   "source": [
    "We can try to filter out the adjectives in the lemmatization step, because spaCy allows filtering based on Parts of Speech. But this might exclude them from the ngrams. Let's try augmenting stopwords and excluding colors that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d660819",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce0d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = recipes_with_cv.drop(['dek', 'hed', 'aggregateRating', 'ingredients', 'prepSteps',\n",
    "       'reviewsCount', 'willMakeAgainPct', 'photo_filename',\n",
    "       'photo_credit', 'author_name', 'date_published', 'recipe_url'], axis=1)\n",
    "\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c4a40e",
   "metadata": {},
   "source": [
    "Ok so this all works! Let's try making a Logistic Regressor to classify things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd87916",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['cuisine_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc060878",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo['cuisine_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49eaf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo[repo['cuisine_name'] == 'Missing Cuisine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['cuisine_name'] = filtered_df['cuisine_name'].apply(cuisine_namer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50224e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['cuisine_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e6b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = filtered_df['cuisine_name']\n",
    "X = filtered_df.drop(['id', 'cuisine_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1add48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1ce1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fff4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=240, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cc03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(class_weight='balanced', verbose=20, solver='saga', multi_class='ovr', n_jobs=-1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49383b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_train, y_train, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn import metrics\n",
    "\n",
    "train_pred = clf.predict(X_train)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_train, train_pred, labels=clf.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "plt.figure(figsize=(40,20))\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(zip(train_pred.tolist()[0:30], y_train.tolist()[0:30])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa48c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.tolist()[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = filtered_df[filtered_df['cuisine_name'] != 'Missing Cuisine']\n",
    "\n",
    "y = reduced_df['cuisine_name']\n",
    "X = reduced_df.drop(['id', 'cuisine_name'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=240, stratify=y)\n",
    "\n",
    "clf = LogisticRegression(class_weight='balanced', verbose=20, solver='saga', multi_class='multinomial', n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "clf.score(X_train, y_train, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = clf.predict(X_train)\n",
    "\n",
    "joined = list(zip(train_pred.tolist()[0:30], y_train.tolist()[0:30]))\n",
    "for pairing in joined:\n",
    "    print(pairing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d205f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df['cuisine_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d13af",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66e1254",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(cv.stop_words_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd72b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cv.stop_words_)[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803589e0-6c28-40c9-9b0b-81a5ba8ce6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_megalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ee598-a363-4fdf-86da-a7707eb8d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recipe_megalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0937b-7b60-4fcd-b32e-ddea5d9afd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo['ingredients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06fdde8-1014-402b-be30-ff72bd78e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo['ingredients'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc8f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeef7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_20 = LatentDirichletAllocation(n_components=20, n_jobs=-1, verbose=100, random_state=200)\n",
    "lda_20_repo_transformed = lda_20.fit_transform(repo_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_lda_20 = LatentDirichletAllocation(n_components=20, n_jobs=-1, verbose=100, random_state=200)\n",
    "tfidf_lda_20_repo_transformed = tfidf_lda_20.fit_transform(repo_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f044642",
   "metadata": {},
   "source": [
    "## Manual Topic Labeling Based on LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62467b5",
   "metadata": {},
   "source": [
    "pyLDAvis calls to a deprecated function inside CountVectorizer, which is incompatible with TFIDF. Can we can find an alternate version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652331a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7142e37045ca6e2f7ebd069a40dd15c94e2caae7a512958c86ea35c1f050d31a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
