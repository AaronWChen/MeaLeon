{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: test\n",
    "output-file: template.html\n",
    "title: Template\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import dagshub\n",
    "import mlflow\n",
    "import nbdev\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | export\n",
    "def foo():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# this function allows us to get the experiment ID from an experiment name\n",
    "def get_experiment_id(name):\n",
    "    exp = mlflow.get_experiment_by_name(name)\n",
    "    if exp is None:\n",
    "      exp_id = mlflow.create_experiment(name)\n",
    "      return exp_id\n",
    "    return exp.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stages of Pipeline Deployment\n",
    "\n",
    "For LLMs, this is a data augmentation pipeline. Raw data will be augmented to compute one or more new columns. This needs to go through the familiar stages of Development, Staging, and Produciton.\n",
    "\n",
    "### Development\n",
    "\n",
    "LLMOps goals for Development/Evaluation are\n",
    "\n",
    "1. track what is being done carefully for later auditing and reproducibility\n",
    "2. package models or pipelines in a format which will make future deployment easier. \n",
    "\n",
    "We will:\n",
    "* Load data\n",
    "* Build an LLM pipeline\n",
    "* Test applying the pipeline to data and log queries and results to MLflow Tracking\n",
    "* Log the pipeline to the MLflow tracking server as an MLflow model\n",
    "\n",
    "The EDA/desired transformations are not really done in this step. The example video mentions that the processing is done during the **course** and not in the LLMOps video. The video starts the workflow focusing on tracking.\n",
    "\n",
    "### Staging\n",
    "\n",
    "LLMOps goals for staging/testing/QA are\n",
    "1. track the LLM's progress through testing and towards production\n",
    "2. work programmatically to demonstrated the APIs needed for future CI/CD automation\n",
    "\n",
    "We will:\n",
    "* Register the pipeline to the MLflow Model Registry\n",
    "* Test the pipeline on sample data\n",
    "* Promote the registered model (pipeline) to production\n",
    "\n",
    "### Production\n",
    "\n",
    "LLMOps goals for production are \n",
    "1. write scale-out code that can meet scaling demands in the future\n",
    "2. simplify deployment by using MLflow to write model-agnostic deployment code\n",
    "\n",
    "We will:\n",
    "1. Load the latest production LLM pipeline from the Model Registry\n",
    "2. Apply the pipeline to an Apache Spark Dataframe\n",
    "3. Append the results to a Delta Lake Table\n",
    "\n",
    "\n",
    "## Notes about this workflow\n",
    "\n",
    "### Notebook vs modular scripts\n",
    "For a demo, everything in the workflow is divided into notebook sections, but this should really be split into separate notebooks or scripts\n",
    "\n",
    "### Models vs code\n",
    "Since the path here is tracked via MLflow Model Registry, this workflow promotes models over code. See \"The Big Book of MLOps\" for more discussion over the distinction (one difference is Model Registry vs Git)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | Below this are blocks to use DagsHub with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Enter the username of your DAGsHub account:\n",
    "DAGSHUB_USER_NAME = \"AaronWChen\"                        #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the email for your DAGsHub account:\n",
    "DAGSHUB_EMAIL = \"awc33@cornell.edu\"                     #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the repo name \n",
    "DAGSHUB_REPO_NAME= \"\"                                   #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the name of the branch you are working on \n",
    "BRANCH= \"\"                                              #@param {type:\"string\"}\n",
    "dagshub.init(repo_name=DAGSHUB_REPO_NAME\n",
    "             , repo_owner=DAGSHUB_USER_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(f'https://dagshub.com/{DAGSHUB_USER_NAME}/{DAGSHUB_REPO_NAME}.mlflow')\n",
    "\n",
    "# starter idea for making an experiment name can be the git branch, but need more specificity\n",
    "DAGSHUB_TEST_NAME = \"stanza_quadgrams_small_set_v1\"     #@param {type:\"string\"}\n",
    "experiment_name = f\"{DAGSHUB_EMAIL}/{DAGSHUB_TEST_NAME}\"\n",
    "mlflow_exp_id = get_experiment_id(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries to handle raw data\n",
    "import dill as pickle\n",
    "import dvc.api\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer\n",
    "    , TfidfTransformer\n",
    "    , TfidfVectorizer\n",
    "    ,\n",
    ")\n",
    "from src.custom_stanza_mlflow import StanzaWrapper\n",
    "import src.dataframe_preprocessor as dfpp\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data and preprocess/clean\n",
    "data = dvc.api.read(\n",
    "        path='../data/raw/recipes-en-201706/epicurious-recipes_m2.json',\n",
    "        mode='r')\n",
    "raw_df = pd.read_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset for dev\n",
    "dev_df = raw_df[0:50]\n",
    "\n",
    "# pre_proc_df is cleaned dataframe\n",
    "pre_proc_df = dfpp.preprocess_dataframe(dev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to Delta format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "# save and log preprocessed dataframe(s)\n",
    "prod_data_path = \"../../data/processed/prod_data\"\n",
    "test_spark_dataset = ps.from_pandas(pre_proc_df)\n",
    "test_spark_dataset.to_delta(path=prod_data_path,\n",
    "                            mode='overwrite',\n",
    "                            index='id')\n",
    "mlflow.log_artifacts(\"../../data/processed/prod_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop the pipeline\n",
    "\n",
    "MLflow Tracking is organized as follows:\n",
    "* An **experiment** generlaly corresponds to the creation of 1 primary model or pipeline. It contains some number of /*runs*/\n",
    "  * A **run** generally corresponds to the creation of 1 sub-model, such as 1 trial during hyperparameter tuing in traditional ML. In this example, running the notebook once only creates 1 run, but a second execution of the notebook would create a second run. This version tracking can be useful during iterative development. Each run contains some number of logged parameters, metrics, tags, models, artifacts, and other metadata\n",
    "    * A **parameter** is an input to the model or pipeline, such as a regularization parameter in traditional ML \n",
    "    * A **metric** is an output of evaluation, such as accuracy or loss\n",
    "    * An **artifact** is an arbitrary file stored alongside a run's metadata, such as the serialized model itself\n",
    "    * A **flavor** is an MLflow format for serializing models. This format uses the underlying ML library's (ie, PyTorch, TensorFlow, Hugging Face) format plus metadata\n",
    "    * As of MLflow 2.3.1, there is an experimental API for logging Predictions\n",
    "  * Wrap model development with a call to ```with mlflow.start_run():``` This context manager syntax starts and ends the MLflow run explicitly, which is a best practice for code which may be moved to production. See API doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipelines relevant to library used\n",
    "# MLflow example uses HuggingFace\n",
    "# below is example for MeaLeon with Stanza and sklearn NLP pipeline\n",
    "\n",
    "# mlflow.set_experiment(\"\") this can be in the start_run though\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'language': 'english',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(cv_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=mlflow_exp_id):\n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    # useful for complex learning, say with few-shot pipeline\n",
    "\n",
    "    # mlflow.llm.log_predictions(inputs=, outputs=, prompts=[])\n",
    "    \n",
    "    # LOG MODEL\n",
    "    # useful to log a \"signature\" with the model telling MLflow the input and output schema for the model\n",
    "    # signature = mlflow.models.infer_signature(\n",
    "    #     pre_proc_df[\"ingredients\"][0],\n",
    "    #     mlflow.transformers.generate_signature_output(\n",
    "    #         summarizer, pre_proc_df[\"ingredients\"][0]\n",
    "    #     )\n",
    "    # )\n",
    "    # print(f\"Signature:\\n{signature}\\n\")\n",
    "\n",
    "    # for mlflow.transformers, if there are inference-time configurations,\n",
    "    # those need to be saved specially in the log_model call\n",
    "    # this ensures that the pipeline will use these same configurations when re-loaded\n",
    "    # inference_config = {\n",
    "    #     \"min_length\": min_length,\n",
    "    #     \"max_length\": max_length,\n",
    "    #     \"truncation\": truncation,\n",
    "    #     \"do_sample\": do_sample\n",
    "    # }\n",
    "\n",
    "    # logging a model returns a handle \"model_info\" to the model metadata in the tracking server. This 'model_info' will be useful later in the notebook to retrieve the logged model\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=summarizer,\n",
    "        artifact_path='summarizer',\n",
    "        task='summarization',\n",
    "        inference_config=inference_config,\n",
    "        signature=signature\n",
    "        input_example=\"This is an example of a long this pipeline can summarize\"\n",
    "    )\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn TFIDFVectorizer\n",
    "    tfidf_vectorizer_model = TfidfVectorizer(**cv_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    test_tfidf_transform = tfidf_vectorizer_model.fit_transform(tqdm(pre_proc_df[\"ingredients\"]))\n",
    "\n",
    "    word_matrix = ps.DataFrame(\n",
    "        test_tfidf_transform.toarray()\n",
    "        , columns=tfidf_vectorizer_model.get_feature_names_out()\n",
    "        , index=pre_proc_df.index\n",
    "    )\n",
    "\n",
    "    with open(\"../joblib/tfidf_transformer_small_test.pkl\", \"wb\") as fo:\n",
    "        pickle.dump(tfidf_vectorizer_model, fo)\n",
    "        mlflow.log_artifact(\"../joblib/tfidf_transformer_small_test.pkl\", artifact_path=\"sklearn_dill_pkls\")\n",
    "\n",
    "    with open(\"../joblib/database_word_matrix_small_test.pkl\", \"wb\") as fo:\n",
    "        pickle.dump(word_matrix, fo)\n",
    "        mlflow.log_artifact(\"../joblib/database_word_matrix_small_test.pkl\", artifact_path=\"sklearn_dill_pkls\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pipeline back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_summarizer = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n",
    "loaded_summarizer.predict(xsum_sample['document'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict()` method can handle more than 1 document at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results = loaded_summarizer.predict(xsum_sample.to_pandas()['document'])\n",
    "display(pd.DataFrame(results, columns=['generated_summary']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to Staging\n",
    "\n",
    "Begin by registering the model in the MLflow Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name for the model in the Model Registry\n",
    "# We filter out some special characters that cannot be used in model names\n",
    "\n",
    "model_name = f'summarizer - {DA.username}'\n",
    "model_name = model_name.replace(\"/\", \"_\").replace(\":\",\"-\")\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a new model under the given name, or a new model version if the name exists already\n",
    "mlflow.register_model(model_uri=model_info.model_uri, name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Pipeline\n",
    "During the Staging step of development, our goal is to move code and/or models from Development to Production. In order to do so, we must test the code and/or models to make sure they are ready for Production\n",
    "\n",
    "We track our progress here using the MLflow Model Registry. This metadata and model store organizes models as follows:\n",
    "* **A registered model** is a named model in the registry (here, the summarization model). It may have multiple /*versions*/.\n",
    "  * **A model version** is an instance of a given model. As you update your model, you will create new versions. Each version is designated as being in a particular /*stage*/ of deployment.\n",
    "    * **A stage** is a stage of deployment: `None` (development), `Staging`, `Production`, or `Archived`\n",
    "\n",
    "The model we registered above starts with 1 version in stage `None` (Development).\n",
    "\n",
    "In the workflow below, we programmatically transition the model from development to staging to production. For more information on the Model Registry API, see the [Model Registry docs](link). Alternatively, you can edit the registry and make model stage transitions via the UI. To access the UI, click the Experiments menu option in the left-hand sidebar, and search for your model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.search_registered_models(filter_string=f\"name = {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will run manual tests, but it would be reasonable to run both automated evaluation and human evaluation in practice. Once tests pass, we will promote the model to stage `Production` to mark it ready for user-facing applications\n",
    "\n",
    "/*Model URIs*/: Below, we use model URIs to tell MLflow which model and version we are rferring to. Two common URI patterns for the MLflow Model Registry are:\n",
    "* `f\"models:/{model_name}/{model_version}\"` to refer to a specific model version by number\n",
    "* `f\"models:/{model_name}/{model_stage}\"` to refer to the latest model in a given stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 1\n",
    "dev_model = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{model_version}\")\n",
    "dev_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*Note about model dependencies*/ When you load the model via MLflow above, you may see warnings about the Python environment. Make sure the environments for development, staging, and production match!\n",
    "\n",
    "MLflow saves these libraries and versions alongside the logged model. See the [MLflow docs on model storage](link) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition to Staging\n",
    "We will move the model to `Staging` to indicate that we are actively testing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.transition_model_version_stage(model_name, model_version, \"staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_model = dev_model\n",
    "\n",
    "# an actual CD/CD workflow might load the `staging_model` programmatically, like:\n",
    "# mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{Staging}\")\n",
    "# or\n",
    "# mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{model_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now \"test\" the model manually on sample data. Here, we simply print out the results and compare them with the original data. In a more realistic setting, we might use a set of human evaluators (or a combination of automated metrics and human evaluators) to decide whether the model outperformed the previous model or system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = staging_model.predict(xsum_sample.to_pandas()['document'])\n",
    "display(pd.DataFrame(results, columns=['generated_summary']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the results look good, they can be transitioned to production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition to Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.transition_model_version_stage(model_name, model_version, \"production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a production workflow for batch inference\n",
    "Once the pipeline is in Production, it may be used by one or more production jobs or serving endpoints. Common deployment locations are:\n",
    "* batch or streaming inference jobs\n",
    "* model serving endpoints\n",
    "* edge devices\n",
    "\n",
    "Example shows batch inference using Apache Spark DataFrames with Delta Lake format. Spark allows simple scale-out inference for high-throughput, low-cost jobs, and Delta allows us to append to and modify inference refult tables with ACID transactions. See the [Apache Spark page](link) and the [Delta Lake page](link) for more information on these technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data as a Spark DataFrame\n",
    "# Recall that we saved this as Delta at the start of the notebook\n",
    "# Also note that it has a ground-truth summary column\n",
    "\n",
    "prod_data = spark.read.format(\"delta\").load(prod_data_path).limit(10)\n",
    "display(prod_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we load the model using `mlflow.pyfunc.spark_udf`. This returns the model as a Spark User Defined Function which can be applied efficiently to big data. /*Note that the deployment code is library-agnostic: it never references that the model is a Hugging Face pipeline*/. This simplified deployment is possible because MLflow logs environment metadata and \"knows\" how to load the model and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow lets you grab the latest model version in a given stage. Here, we grab the latest Production version\n",
    "prod_model_udf = mlflow.pyfunc.spark_udf(\n",
    "    spark,\n",
    "    model_uri=f\"models:/{model_name}/Production\",\n",
    "    env_manager=\"virtualenv\",\n",
    "    result_type=\"string\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference by appending a new column to the DataFrame\n",
    "batch_inference_results = prod_data.withColumn(\"generated_summary\", prod_model_udf(\"document\"))\n",
    "display(batch_inference_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write out our inference results to another Delta table. Here, we append the results to an existing table (and create the table if it does not exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_results_path = f\"{DA.paths.working_dir}/m6-inference-results\".replace(\"/dbfs\", \"dbfs:\")\n",
    "batch_inference_results.write.format(\"delta\").mode(\"append\").save(inference_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta is chosen because it works well with Spark and other scale-out technologies, and works well in supporting both batch and streaming pipelines, allowing concurrent reads and writes to and from a table with ACID transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it!\n",
    "\n",
    "To create a productionjob, we could for example take the new lines of code above, put them in a new notebook, and schedule it as an automated workflow. MLflow can be integrated with essentially any deployment system, but for more intformation specific to this Databricks workspace, see the \"Use model for inference\" documentation for AWS, Azure, or GCP.\n",
    "\n",
    "We did not cover model serving for real-time inference, but MLflow models can be deployed to any cloud or on-prem serving systems. For more information, see the [open-source MLflow Model Registry docs](link) or the [Databricks Model Serving docs](link)\n",
    "\n",
    "For other topics not covered, see [\"The Big Book of MLOps\"](link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We have now walked through a full example of going from development to production. Our LLM pipeline was very simple, but LLM Ops for a more complex workflow (such as fune-tuning a custom model) would be very similar. You still follow the basic Ops steps of:\n",
    "* Development: Creating the pipeline or model, tracking the process in the MLflow Tracking server, and seving the final pipeline or model\n",
    "* Staging: Registering a new model or version in the MLflow Model Registry, testing it, and promoting it through Staging to Production. \n",
    "* Production: Creating an inference job, or creating a model serving endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
