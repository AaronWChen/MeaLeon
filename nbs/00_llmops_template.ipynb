{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: test\n",
    "output-file: template.html\n",
    "title: Template\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import dagshub\n",
    "import mlflow\n",
    "import nbdev\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | export\n",
    "def foo():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# this function allows us to get the experiment ID from an experiment name\n",
    "def get_experiment_id(name):\n",
    "    exp = mlflow.get_experiment_by_name(name)\n",
    "    if exp is None:\n",
    "      exp_id = mlflow.create_experiment(name)\n",
    "      return exp_id\n",
    "    return exp.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stages of Pipeline Deployment\n",
    "\n",
    "For LLMs, this is a data augmentation pipeline. Raw data will be augmented to compute one or more new columns. This needs to go through the familiar stages of Development, Staging, and Produciton.\n",
    "\n",
    "### Development\n",
    "\n",
    "LLMOps goals for Development/Evaluation are\n",
    "\n",
    "1. track what is being done carefully for later auditing and reproducibility\n",
    "2. package models or pipelines in a format which will make future deployment easier. \n",
    "\n",
    "We will:\n",
    "* Load data\n",
    "* Build an LLM pipeline\n",
    "* Test applying the pipeline to data and log queries and results to MLflow Tracking\n",
    "* Log the pipeline to the MLflow tracking server as an MLflow model\n",
    "\n",
    "The EDA/desired transformations are not really done in this step. The example video mentions that the processing is done during the **course** and not in the LLMOps video. The video starts the workflow focusing on tracking.\n",
    "\n",
    "### Staging\n",
    "\n",
    "LLMOps goals for staging/testing/QA are\n",
    "1. track the LLM's progress through testing and towards production\n",
    "2. work programmatically to demonstrated the APIs needed for future CI/CD automation\n",
    "\n",
    "We will:\n",
    "* Register the pipeline to the MLflow Model Registry\n",
    "* Test the pipeline on sample data\n",
    "* Promote the registered model (pipeline) to production\n",
    "\n",
    "### Production\n",
    "\n",
    "LLMOps goals for production are \n",
    "1. write scale-out code that can meet scaling demands in the future\n",
    "2. simplify deployment by using MLflow to write model-agnostic deployment code\n",
    "\n",
    "We will:\n",
    "1. Load the latest production LLM pipeline from the Model Registry\n",
    "2. Apply the pipeline to an Apache Spark Dataframe\n",
    "3. Append the results to a Delta Lake Table\n",
    "\n",
    "\n",
    "## Notes about this workflow\n",
    "\n",
    "### Notebook vs modular scripts\n",
    "For a demo, everything in the workflow is divided into notebook sections, but this should really be split into separate notebooks or scripts\n",
    "\n",
    "### Models vs code\n",
    "Since the path here is tracked via MLflow Model Registry, this workflow promotes models over code. See \"The Big Book of MLOps\" for more discussion over the distinction (one difference is Model Registry vs Git)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | Below this are blocks to use DagsHub with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Enter the username of your DAGsHub account:\n",
    "DAGSHUB_USER_NAME = \"AaronWChen\"                        #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the email for your DAGsHub account:\n",
    "DAGSHUB_EMAIL = \"awc33@cornell.edu\"                     #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the repo name \n",
    "DAGSHUB_REPO_NAME= \"\"                                   #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the name of the branch you are working on \n",
    "BRANCH= \"\"                                              #@param {type:\"string\"}\n",
    "dagshub.init(repo_name=DAGSHUB_REPO_NAME\n",
    "             , repo_owner=DAGSHUB_USER_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(f'https://dagshub.com/{DAGSHUB_USER_NAME}/{DAGSHUB_REPO_NAME}.mlflow')\n",
    "\n",
    "# starter idea for making an experiment name can be the git branch, but need more specificity\n",
    "DAGSHUB_TEST_NAME = \"stanza_quadgrams_small_set_v1\"     #@param {type:\"string\"}\n",
    "experiment_name = f\"{DAGSHUB_EMAIL}/{DAGSHUB_TEST_NAME}\"\n",
    "mlflow_exp_id = get_experiment_id(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries to handle raw data\n",
    "import dill as pickle\n",
    "import dvc.api\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer\n",
    "    , TfidfTransformer\n",
    "    , TfidfVectorizer\n",
    "    ,\n",
    ")\n",
    "from src.custom_stanza_mlflow import StanzaWrapper\n",
    "import src.dataframe_preprocessor as dfpp\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data and preprocess/clean\n",
    "data = dvc.api.read(\n",
    "        path='../data/raw/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "        mode='r')\n",
    "raw_df = pd.read_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset for dev\n",
    "dev_df = raw_df[0:50]\n",
    "\n",
    "# pre_proc_df is cleaned dataframe\n",
    "pre_proc_df = dfpp.preprocess_dataframe(dev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to Delta format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "# save and log preprocessed dataframe(s)\n",
    "prod_data_path = \"../../data/processed/prod_data\"\n",
    "test_spark_dataset = ps.from_pandas(pre_proc_df)\n",
    "test_spark_dataset.to_delta(path=prod_data_path,\n",
    "                            mode='overwrite',\n",
    "                            index='id')\n",
    "mlflow.log_artifacts(\"../../data/processed/prod_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipelines relevant to library used\n",
    "# MLflow example uses HuggingFace\n",
    "# below is example for MeaLeon with Stanza and sklearn NLP pipeline\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'language': 'english',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(cv_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=mlflow_exp_id):\n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # LOG MODEL\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn TFIDFVectorizer\n",
    "    tfidf_vectorizer_model = TfidfVectorizer(**cv_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    test_tfidf_transform = tfidf_vectorizer_model.fit_transform(tqdm(pre_proc_df[\"ingredients\"]))\n",
    "\n",
    "    word_matrix = ps.DataFrame(\n",
    "        test_tfidf_transform.toarray()\n",
    "        , columns=tfidf_vectorizer_model.get_feature_names_out()\n",
    "        , index=pre_proc_df.index\n",
    "    )\n",
    "\n",
    "    with open(\"../joblib/tfidf_transformer_small_test.pkl\", \"wb\") as fo:\n",
    "        pickle.dump(tfidf_vectorizer_model, fo)\n",
    "        mlflow.log_artifact(\"../joblib/tfidf_transformer_small_test.pkl\", artifact_path=\"sklearn_dill_pkls\")\n",
    "\n",
    "    with open(\"../joblib/database_word_matrix_small_test.pkl\", \"wb\") as fo:\n",
    "        pickle.dump(word_matrix, fo)\n",
    "        mlflow.log_artifact(\"../joblib/database_word_matrix_small_test.pkl\", artifact_path=\"sklearn_dill_pkls\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
