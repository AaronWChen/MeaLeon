{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *\n",
    "import project_path\n",
    "\n",
    "from bertopic import BERTopic\n",
    "import dagshub\n",
    "# from datetime import datetime\n",
    "from hdbscan import HDBSCAN\n",
    "import joblib \n",
    "import json\n",
    "# import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from sklearn.base import TransformerMixin\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    # TfidfTransformer,\n",
    "    # TfidfVectorizer,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# from spacy.lemmatizer import Lemmatizer\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "from umap import UMAP\n",
    "\n",
    "# import local scripts\n",
    "# import src.nlp_processor as nlpp\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length= 10000000\n",
    "TOKENIZERS_PARALLELISM=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lemmatizer(ingredients: list) -> Any:  # spacy nlp.Doc\n",
    "    \"\"\"This takes in a string representing the recipe and an NLP model and lemmatize with the NER.\n",
    "\n",
    "    Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    Remove punctuation\n",
    "\n",
    "    Args:\n",
    "        ingredients: string\n",
    "        nlp_mod: spacy model (try built in first, by default called nlp)\n",
    "\n",
    "    Returns:\n",
    "        nlp.Doc\n",
    "    \"\"\"\n",
    "    lemmas = [\n",
    "        token.lemma_\n",
    "        for token in ingredients\n",
    "        if (\n",
    "            token.is_alpha\n",
    "            and token.pos_ not in [\"PRON\", \"VERB\"]\n",
    "            and len(token.lemma_) > 1\n",
    "        )\n",
    "    ]\n",
    "    return lemmas\n",
    "    # return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_preprocessor(recipe_ingreds: str) -> list:\n",
    "    \"\"\"This function replaces the default sklearn CountVectorizer preprocessor to use spaCy. sklearn CountVectorizer's preprocessor only performs accent removal and lowercasing.\n",
    "\n",
    "    Args:\n",
    "        A string to tokenize from a recipe representing the ingredients used in the recipe\n",
    "\n",
    "    Returns:\n",
    "        A list of strings that have been de-accented and lowercased to be used in tokenization\n",
    "    \"\"\"\n",
    "    preprocessed = [token for token in nlp(recipe_ingreds)]\n",
    "\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Enter the username of your DAGsHub account:\n",
    "DAGSHUB_USER_NAME = \"AaronWChen\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the email for your DAGsHub account:\n",
    "DAGSHUB_EMAIL = \"awc33@cornell.edu\" #@param {type:\"string\"}\n",
    "\n",
    "DAGSHUB_REPO_NAME=\"MeaLeon\"\n",
    "BRANCH=\"venv4/add-try-mlflow\"\n",
    "dagshub.init(repo_name='MeaLeon', repo_owner='AaronWChen')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_id(name):\n",
    "    exp = mlflow.get_experiment_by_name(name)\n",
    "    if exp is None:\n",
    "      exp_id = mlflow.create_experiment(name)\n",
    "      return exp_id\n",
    "    return exp.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = '../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "food_stopwords_path = \"../food_stopwords.csv\"\n",
    "\n",
    "joblib_basepath = '../joblib/2022.08.23/'\n",
    "\n",
    "cv_path = joblib_basepath + 'countvec.joblib'\n",
    "tfidf_path = joblib_basepath + 'tfidf.joblib'\n",
    "full_df_path = joblib_basepath + 'recipes_with_cv.joblib'\n",
    "reduced_df_path = joblib_basepath + 'reduced_df.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is a redeem for variable naming mixed with a free pun-ish me daddy, flushtrated will be the list of all stopword to exclude so named because we're throwing these words down the drain\n",
    "\n",
    "flushtrated = {x for x in pd.read_csv(food_stopwords_path)}\n",
    "additional_to_exclude = {\n",
    "    \"red\",\n",
    "    \"green\",\n",
    "    \"black\",\n",
    "    \"yellow\",\n",
    "    \"white\",\n",
    "    \"inch\",\n",
    "    \"mince\",\n",
    "    \"chop\",\n",
    "    \"fry\",\n",
    "    \"trim\",\n",
    "    \"flat\",\n",
    "    \"beat\",\n",
    "    \"brown\",\n",
    "    \"golden\",\n",
    "    \"balsamic\",\n",
    "    \"halve\",\n",
    "    \"blue\",\n",
    "    \"divide\",\n",
    "    \"trim\",\n",
    "    \"unbleache\",\n",
    "    \"granulate\",\n",
    "    \"Frank\",\n",
    "    \"alternative\",\n",
    "    \"american\",\n",
    "    \"annie\",\n",
    "    \"asian\",\n",
    "    \"balance\",\n",
    "    \"band\",\n",
    "    \"barrel\",\n",
    "    \"bay\",\n",
    "    \"bayou\",\n",
    "    \"beam\",\n",
    "    \"beard\",\n",
    "    \"bell\",\n",
    "    \"betty\",\n",
    "    \"bird\",\n",
    "    \"blast\",\n",
    "    \"bob\",\n",
    "    \"bone\",\n",
    "    \"breyers\",\n",
    "    \"calore\",\n",
    "    \"carb\",\n",
    "    \"card\",\n",
    "    \"chachere\",\n",
    "    \"change\",\n",
    "    \"circle\",\n",
    "    \"coffee\",\n",
    "    \"coil\",\n",
    "    \"country\",\n",
    "    \"cow\",\n",
    "    \"crack\",\n",
    "    \"cracker\",\n",
    "    \"crocker\",\n",
    "    \"crystal\",\n",
    "    \"dean\",\n",
    "    \"degree\",\n",
    "    \"deluxe\",\n",
    "    \"direction\",\n",
    "    \"duncan\",\n",
    "    \"earth\",\n",
    "    \"eggland\",\n",
    "    \"ener\",\n",
    "    \"envelope\",\n",
    "    \"eye\",\n",
    "    \"fantastic\",\n",
    "    \"far\",\n",
    "    \"fat\",\n",
    "    \"feather\",\n",
    "    \"flake\",\n",
    "    \"foot\",\n",
    "    \"fourth\",\n",
    "    \"frank\",\n",
    "    \"french\",\n",
    "    \"fusion\",\n",
    "    \"genoa\",\n",
    "    \"genovese\",\n",
    "    \"germain\",\n",
    "    \"giada\",\n",
    "    \"gold\",\n",
    "    \"granule\",\n",
    "    \"greek\",\n",
    "    \"hamburger\",\n",
    "    \"helper\",\n",
    "    \"herbe\",\n",
    "    \"hines\",\n",
    "    \"hodgson\",\n",
    "    \"hunt\",\n",
    "    \"instruction\",\n",
    "    \"interval\",\n",
    "    \"italianstyle\",\n",
    "    \"jim\",\n",
    "    \"jimmy\",\n",
    "    \"kellogg\",\n",
    "    \"lagrille\",\n",
    "    \"lake\",\n",
    "    \"land\",\n",
    "    \"laurentiis\",\n",
    "    \"lawry\",\n",
    "    \"lipton\",\n",
    "    \"litre\",\n",
    "    \"ll\",\n",
    "    \"maid\",\n",
    "    \"malt\",\n",
    "    \"mate\",\n",
    "    \"mayer\",\n",
    "    \"meal\",\n",
    "    \"medal\",\n",
    "    \"medallion\",\n",
    "    \"member\",\n",
    "    \"mexicanstyle\",\n",
    "    \"monte\",\n",
    "    \"mori\",\n",
    "    \"nest\",\n",
    "    \"nu\",\n",
    "    \"oounce\",\n",
    "    \"oscar\",\n",
    "    \"ox\",\n",
    "    \"paso\",\n",
    "    \"pasta\",\n",
    "    \"patty\",\n",
    "    \"petal\",\n",
    "    \"pinche\",\n",
    "    \"preserve\",\n",
    "    \"quartere\",\n",
    "    \"ranch\",\n",
    "    \"ranchstyle\",\n",
    "    \"rasher\",\n",
    "    \"redhot\",\n",
    "    \"resemble\",\n",
    "    \"rice\",\n",
    "    \"ro\",\n",
    "    \"roni\",\n",
    "    \"scissor\",\n",
    "    \"scrap\",\n",
    "    \"secret\",\n",
    "    \"semicircle\",\n",
    "    \"shard\",\n",
    "    \"shear\",\n",
    "    \"sixth\",\n",
    "    \"sliver\",\n",
    "    \"smucker\",\n",
    "    \"snicker\",\n",
    "    \"source\",\n",
    "    \"spot\",\n",
    "    \"state\",\n",
    "    \"strand\",\n",
    "    \"sun\",\n",
    "    \"supreme\",\n",
    "    \"tablepoon\",\n",
    "    \"tail\",\n",
    "    \"target\",\n",
    "    \"tm\",\n",
    "    \"tong\",\n",
    "    \"toothpick\",\n",
    "    \"triangle\",\n",
    "    \"trimming\",\n",
    "    \"tweezer\",\n",
    "    \"valley\",\n",
    "    \"vay\",\n",
    "    \"wise\",\n",
    "    \"wishbone\",\n",
    "    \"wrapper\",\n",
    "    \"yoplait\",\n",
    "    \"ziploc\",\n",
    "}\n",
    "\n",
    "flushtrated = flushtrated.union(STOP_WORDS)\n",
    "flushtrated = flushtrated.union(additional_to_exclude)\n",
    "flushtrated_list = list(flushtrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = joblib.load(full_df_path)\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['prepSteps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['prepSteps'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_steps = full_df['prepSteps'].apply(\" \".join).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-calculating sentence mebeddings\n",
    "embedding_model_params = {'embedding_model': 'all-MiniLM-L6-v2'}\n",
    "embedding_model = SentenceTransformer(embedding_model_params['embedding_model'])\n",
    "# embeddings = embedding_model.encode(recipe_steps, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify UMAP dimensionality reductions\n",
    "umap_model_params = {'n_neighbors':15, 'n_components':10, 'random_state':200}\n",
    "umap_model = UMAP(**umap_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster with HDBSCAN\n",
    "hdbscan_model_params = {'min_cluster_size':200, 'prediction_data':True}\n",
    "hdbscan_model = HDBSCAN(**hdbscan_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding custom count vectorization\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'preprocessor':custom_preprocessor,\n",
    "    # 'tokenizer':custom_lemmatizer, # out of memory \n",
    "    'stop_words':flushtrated_list,\n",
    "    'token_pattern':r\"(?u)\\b[a-zA-Z]{2,}\\b\",\n",
    "    'ngram_range':(1, 4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "vectorizer_model = CountVectorizer(**cv_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/processed/bertopic_params.joblib', 'w') as fp:\n",
    "pipeline_params = {\n",
    "    'embedding':{'pretrained_sentence_embeddings': embedding_model_params},\n",
    "    'dimension_reduction': {'UMAP': umap_model_params},\n",
    "    'clustering': {'HDBSCAN': hdbscan_model_params},\n",
    "    'vectorizer': {'sklearn_countvectorizer': cv_params},\n",
    "}\n",
    "joblib.dump(pipeline_params, '../data/processed/bertopic_params.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id=get_experiment_id(\"initial_explicit_spec_run\")):\n",
    "    # mlflow.log_params(pipeline_params)\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        top_n_words=20,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(recipe_steps)\n",
    "\n",
    "    topic_model.get_topic_info().to_json('../data/processed/topic_model_df.json')\n",
    "\n",
    "    mlflow.log_artifact('../data/processed/bertopic_params.joblib')\n",
    "    mlflow.log_artifact('../data/processed/topic_model_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(recipe_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | export\n",
    "def foo():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
