{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: test\n",
    "output-file: template.html\n",
    "title: Template\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import dagshub\n",
    "import mlflow\n",
    "import nbdev\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | export\n",
    "def foo():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# this function allows us to get the experiment ID from an experiment name\n",
    "def get_experiment_id(name):\n",
    "    exp = mlflow.get_experiment_by_name(name)\n",
    "    if exp is None:\n",
    "      exp_id = mlflow.create_experiment(name)\n",
    "      return exp_id\n",
    "    return exp.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | Below this are blocks to use DagsHub with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token Dagshub OAuth token, valid until 2023-12-02 04:53:30+00:00 does not exist in the storage\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Open the following link in your browser to authorize the client:\n",
      "https://dagshub.com/login/oauth/authorize?state=5a600d08-8162-43a0-a644-b3f2a53c89c0&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=ac2d0d3591d0ecabd5d482d46fe51d517cf674717987c5cfbd1e46ae6342a1fc\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31e7f589c0f4008a77aee425a9ec69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@markdown Enter the username of your DAGsHub account:\n",
    "DAGSHUB_USER_NAME = \"AaronWChen\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the email for your DAGsHub account:\n",
    "DAGSHUB_EMAIL = \"awc33@cornell.edu\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the repo name \n",
    "DAGSHUB_REPO_NAME=\"MeaLeon\"\n",
    "\n",
    "#@markdown Enter the name of the branch you are working on \n",
    "BRANCH=\"STANZA-1/refactor-nltk-stanza\"\n",
    "dagshub.init(repo_name=DAGSHUB_REPO_NAME\n",
    "             , repo_owner=DAGSHUB_USER_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "mlflow.environment_variables.MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR='MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR'\n",
    "\n",
    "# starter idea for making an experiment name can be the git branch, but need more specificity\n",
    "experiment_name = f\"{DAGSHUB_EMAIL}/stanza_quadgrams_small_set_v1\"\n",
    "mlflow_exp_id = get_experiment_id(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1ff388d53242caa190a98669426a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 23:28:04 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-12-01 23:28:05 INFO: File exists: /home/awchen/stanza_resources/en/default.zip\n",
      "2023-12-01 23:28:08 INFO: Finished downloading models and saved to /home/awchen/stanza_resources.\n",
      "2023-12-01 23:28:08 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4a088389464fc6b2a791561914ea37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 23:28:09 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2023-12-01 23:28:09 INFO: Using device: cuda\n",
      "2023-12-01 23:28:09 INFO: Loading: tokenize\n",
      "2023-12-01 23:28:09 INFO: Loading: pos\n",
      "2023-12-01 23:28:09 INFO: Loading: lemma\n",
      "2023-12-01 23:28:09 INFO: Loading: constituency\n",
      "2023-12-01 23:28:10 INFO: Loading: depparse\n",
      "2023-12-01 23:28:10 INFO: Loading: sentiment\n",
      "2023-12-01 23:28:10 INFO: Loading: ner\n",
      "2023-12-01 23:28:11 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------\n",
      "Parameters Logged in MLflow\n",
      "\n",
      "\n",
      "--------------\n",
      "Raw Dataframe:\n",
      "                         id  \\\n",
      "0  54a2b6b019925f464b373351   \n",
      "1  54a408a019925f464b3733bc   \n",
      "2  54a408a26529d92b2c003631   \n",
      "3  54a408a66529d92b2c003638   \n",
      "4  54a408a719925f464b3733cc   \n",
      "\n",
      "                                                 dek  \\\n",
      "0  How does fried chicken achieve No. 1 status? B...   \n",
      "1                                Spinaci all'Ebraica   \n",
      "2  This majestic, moist, and richly spiced honey ...   \n",
      "3  The idea for this sandwich came to me when my ...   \n",
      "4  In 1930, Simon Agranat, the chief justice of t...   \n",
      "\n",
      "                                     hed                   pubDate  \\\n",
      "0            Pickle-Brined Fried Chicken  2014-08-19T04:00:00.000Z   \n",
      "1                   Spinach Jewish Style  2008-09-09T04:00:00.000Z   \n",
      "2                  New Year’s Honey Cake  2008-09-10T04:00:00.000Z   \n",
      "3  The B.L.A.Bagel with Lox and Avocado  2008-09-08T04:00:00.000Z   \n",
      "4        Shakshuka a la Doktor Shakshuka  2008-09-09T04:00:00.000Z   \n",
      "\n",
      "                             author    type  \\\n",
      "0                                []  recipe   \n",
      "1  [{'name': 'Edda Servi Machlin'}]  recipe   \n",
      "2       [{'name': 'Marcy Goldman'}]  recipe   \n",
      "3           [{'name': 'Faye Levy'}]  recipe   \n",
      "4         [{'name': 'Joan Nathan'}]  recipe   \n",
      "\n",
      "                                                 url  \\\n",
      "0  /recipes/food/views/pickle-brined-fried-chicke...   \n",
      "1    /recipes/food/views/spinach-jewish-style-350152   \n",
      "2  /recipes/food/views/majestic-and-moist-new-yea...   \n",
      "3  /recipes/food/views/the-b-l-a-bagel-with-lox-a...   \n",
      "4  /recipes/food/views/shakshuka-a-la-doktor-shak...   \n",
      "\n",
      "                                           photoData  \\\n",
      "0  {'id': '54a2b64a6529d92b2c003409', 'filename':...   \n",
      "1  {'id': '56746182accb4c9831e45e0a', 'filename':...   \n",
      "2  {'id': '55e85ba4cf90d6663f728014', 'filename':...   \n",
      "3  {'id': '5674617e47d1a28026045e4f', 'filename':...   \n",
      "4  {'id': '56746183b47c050a284a4e15', 'filename':...   \n",
      "\n",
      "                                                 tag  aggregateRating  \\\n",
      "0  {'category': 'ingredient', 'name': 'Chicken', ...             3.11   \n",
      "1  {'category': 'cuisine', 'name': 'Italian', 'ur...             3.22   \n",
      "2  {'category': 'cuisine', 'name': 'Jewish', 'url...             3.62   \n",
      "3  {'category': 'cuisine', 'name': 'Jewish', 'url...             4.00   \n",
      "4  {'category': 'cuisine', 'name': 'Jewish', 'url...             2.71   \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  [1 tablespoons yellow mustard seeds, 1 tablesp...   \n",
      "1  [3 pounds small-leaved bulk spinach, Salt, 1/2...   \n",
      "2  [3 1/2 cups all-purpose flour, 1 tablespoon ba...   \n",
      "3  [1 small ripe avocado, preferably Hass (see No...   \n",
      "4  [2 pounds fresh tomatoes, unpeeled and cut in ...   \n",
      "\n",
      "                                           prepSteps  reviewsCount  \\\n",
      "0  [Toast mustard and coriander seeds in a dry me...             7   \n",
      "1  [Remove the stems and roots from the spinach. ...             5   \n",
      "2  [I like this cake best baked in a 9-inch angel...           105   \n",
      "3  [A short time before serving, mash avocado and...             7   \n",
      "4  [1. Place the tomatoes, garlic, salt, paprika,...             7   \n",
      "\n",
      "   willMakeAgainPct  dateCrawled  \n",
      "0               100   1498547035  \n",
      "1                80   1498547740  \n",
      "2                88   1498547738  \n",
      "3               100   1498547740  \n",
      "4                83   1498547740  \n",
      "(34756, 15)\n",
      "\n",
      "\n",
      "--------------\n",
      "Preprocessed Dataframe:\n",
      "                                                                        dek  \\\n",
      "id                                                                            \n",
      "54a2b6b019925f464b373351  How does fried chicken achieve No. 1 status? B...   \n",
      "54a408a019925f464b3733bc                                Spinaci all'Ebraica   \n",
      "54a408a26529d92b2c003631  This majestic, moist, and richly spiced honey ...   \n",
      "54a408a66529d92b2c003638  The idea for this sandwich came to me when my ...   \n",
      "54a408a719925f464b3733cc  In 1930, Simon Agranat, the chief justice of t...   \n",
      "\n",
      "                                                            hed  \\\n",
      "id                                                                \n",
      "54a2b6b019925f464b373351            Pickle-Brined Fried Chicken   \n",
      "54a408a019925f464b3733bc                   Spinach Jewish Style   \n",
      "54a408a26529d92b2c003631                  New Year’s Honey Cake   \n",
      "54a408a66529d92b2c003638  The B.L.A.Bagel with Lox and Avocado   \n",
      "54a408a719925f464b3733cc        Shakshuka a la Doktor Shakshuka   \n",
      "\n",
      "                          aggregateRating  \\\n",
      "id                                          \n",
      "54a2b6b019925f464b373351             3.11   \n",
      "54a408a019925f464b3733bc             3.22   \n",
      "54a408a26529d92b2c003631             3.62   \n",
      "54a408a66529d92b2c003638             4.00   \n",
      "54a408a719925f464b3733cc             2.71   \n",
      "\n",
      "                                                                ingredients  \\\n",
      "id                                                                            \n",
      "54a2b6b019925f464b373351  [1 tablespoons yellow mustard seeds, 1 tablesp...   \n",
      "54a408a019925f464b3733bc  [3 pounds small-leaved bulk spinach, Salt, 1/2...   \n",
      "54a408a26529d92b2c003631  [3 1/2 cups all-purpose flour, 1 tablespoon ba...   \n",
      "54a408a66529d92b2c003638  [1 small ripe avocado, preferably Hass (see No...   \n",
      "54a408a719925f464b3733cc  [2 pounds fresh tomatoes, unpeeled and cut in ...   \n",
      "\n",
      "                                                                  prepSteps  \\\n",
      "id                                                                            \n",
      "54a2b6b019925f464b373351  [Toast mustard and coriander seeds in a dry me...   \n",
      "54a408a019925f464b3733bc  [Remove the stems and roots from the spinach. ...   \n",
      "54a408a26529d92b2c003631  [I like this cake best baked in a 9-inch angel...   \n",
      "54a408a66529d92b2c003638  [A short time before serving, mash avocado and...   \n",
      "54a408a719925f464b3733cc  [1. Place the tomatoes, garlic, salt, paprika,...   \n",
      "\n",
      "                          reviewsCount  willMakeAgainPct     cuisine_name  \\\n",
      "id                                                                          \n",
      "54a2b6b019925f464b373351             7               100  Missing Cuisine   \n",
      "54a408a019925f464b3733bc             5                80          Italian   \n",
      "54a408a26529d92b2c003631           105                88           Kosher   \n",
      "54a408a66529d92b2c003638             7               100           Kosher   \n",
      "54a408a719925f464b3733cc             7                83           Kosher   \n",
      "\n",
      "                                               photo_filename  \\\n",
      "id                                                              \n",
      "54a2b6b019925f464b373351       51247610_fried-chicken_1x1.jpg   \n",
      "54a408a019925f464b3733bc  EP_12162015_placeholders_rustic.jpg   \n",
      "54a408a26529d92b2c003631          EP_09022015_honeycake-2.jpg   \n",
      "54a408a66529d92b2c003638  EP_12162015_placeholders_casual.jpg   \n",
      "54a408a719925f464b3733cc  EP_12162015_placeholders_formal.jpg   \n",
      "\n",
      "                                                               photo_credit  \\\n",
      "id                                                                            \n",
      "54a2b6b019925f464b373351                Michael Graydon and Nikole Herriott   \n",
      "54a408a019925f464b3733bc  Photo by Chelsea Kyle, Prop Styling by Anna St...   \n",
      "54a408a26529d92b2c003631  Photo by Chelsea Kyle, Food Styling by Anna St...   \n",
      "54a408a66529d92b2c003638  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a408a719925f464b3733cc  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "\n",
      "                                  author_name            date_published  \\\n",
      "id                                                                        \n",
      "54a2b6b019925f464b373351  Missing Author Name 2014-08-19 04:00:00+00:00   \n",
      "54a408a019925f464b3733bc   Edda Servi Machlin 2008-09-09 04:00:00+00:00   \n",
      "54a408a26529d92b2c003631        Marcy Goldman 2008-09-10 04:00:00+00:00   \n",
      "54a408a66529d92b2c003638            Faye Levy 2008-09-08 04:00:00+00:00   \n",
      "54a408a719925f464b3733cc          Joan Nathan 2008-09-09 04:00:00+00:00   \n",
      "\n",
      "                                                                 recipe_url  \n",
      "id                                                                           \n",
      "54a2b6b019925f464b373351  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a408a019925f464b3733bc  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a408a26529d92b2c003631  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a408a66529d92b2c003638  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a408a719925f464b3733cc  https://www.epicurious.com/recipes/food/views/...  \n",
      "(34656, 13)\n",
      "\n",
      "\n",
      "--------------\n",
      "Dataframes logged as MLflow artifacts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:30<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------\n",
      "sklearn dill/pkls logged as MLflow artifacts\n"
     ]
    }
   ],
   "source": [
    "# create pipelines relevant to library used\n",
    "# MLflow example uses HuggingFace\n",
    "# below is example for MeaLeon with Stanza and sklearn NLP pipeline\n",
    "\n",
    "with mlflow.start_run(experiment_id=mlflow_exp_id):\n",
    "    # import necessary libraries to handle raw data\n",
    "    import dill as pickle\n",
    "    import dvc.api\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import (\n",
    "        CountVectorizer\n",
    "        , TfidfTransformer\n",
    "        , TfidfVectorizer\n",
    "        ,\n",
    "    )\n",
    "    from src.custom_stanza_mlflow import StanzaWrapper\n",
    "    import src.dataframe_preprocessor as dfpp\n",
    "    import stanza\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    stanza.download('en')\n",
    "    nlp = stanza.Pipeline('en')\n",
    "    \n",
    "    # cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "    cv_params = {\n",
    "        'strip_accents':\"unicode\",\n",
    "        'lowercase':True,\n",
    "        'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "        'min_df':10,\n",
    "    }\n",
    "\n",
    "    # pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "    pipeline_params = {\n",
    "        'stanza_model': 'en',\n",
    "        'language': 'english',\n",
    "        'sklearn-transformer': 'TfidfVectorizer'\n",
    "    }\n",
    "\n",
    "    # update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "    pipeline_params.update(cv_params)\n",
    "\n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(\"Parameters Logged in MLflow\")\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Raw Dataframe:', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    to_nlp_df = pre_proc_df[0:50]\n",
    "    to_nlp_df\n",
    "\n",
    "    # save and log preprocessed dataframe(s)\n",
    "    pre_proc_df.to_json('../data/processed/preprocessed_dataframe.json')\n",
    "    mlflow.log_artifact('../data/processed/preprocessed_dataframe.json', artifact_path=\"preprocessed_dataframes\")\n",
    "    \n",
    "    to_nlp_df.to_json('../data/processed/preprocessed_subset_dataframe.json')\n",
    "    mlflow.log_artifact('../data/processed/preprocessed_subset_dataframe.json', artifact_path=\"preprocessed_dataframes\")\n",
    "    \n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Dataframes logged as MLflow artifacts')\n",
    "\n",
    "    # LOG MODEL\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn TFIDFVectorizer\n",
    "    tfidf_vectorizer_model = TfidfVectorizer(**cv_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    test_tfidf_transform = tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"ingredients\"]))\n",
    "\n",
    "    word_matrix = pd.DataFrame(\n",
    "        test_tfidf_transform.toarray()\n",
    "        , columns=tfidf_vectorizer_model.get_feature_names_out()\n",
    "        , index=to_nlp_df.index\n",
    "    )\n",
    "\n",
    "    with open(\"../joblib/tfidf_transformer_small_test.pkl\", \"wb\") as fo:\n",
    "        pickle.dump(tfidf_vectorizer_model, fo)\n",
    "        mlflow.log_artifact(\"../joblib/tfidf_transformer_small_test.pkl\", artifact_path=\"sklearn_dill_pkls\")\n",
    "\n",
    "    with open(\"../joblib/database_word_matrix_small_test.pkl\", \"wb\") as fo:\n",
    "        pickle.dump(word_matrix, fo)\n",
    "        mlflow.log_artifact(\"../joblib/database_word_matrix_small_test.pkl\", artifact_path=\"sklearn_dill_pkls\")\n",
    "\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('sklearn dill/pkls logged as MLflow artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "mlflow.environment_variables.MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR='MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR'\n",
    "\n",
    "# starter idea for making an experiment name can be the git branch, but need more specificity\n",
    "experiment_name = f\"{DAGSHUB_EMAIL}/stanza_quadgrams_full_set_v1\"\n",
    "mlflow_exp_id = get_experiment_id(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee4916abefb44208b5726d13919b32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 21:59:14 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-12-02 21:59:16 INFO: File exists: /home/awchen/stanza_resources/en/default.zip\n",
      "2023-12-02 21:59:20 INFO: Finished downloading models and saved to /home/awchen/stanza_resources.\n",
      "2023-12-02 21:59:20 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5018e83bbcc46a4a73e6384e06f48d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 21:59:21 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2023-12-02 21:59:21 INFO: Using device: cuda\n",
      "2023-12-02 21:59:21 INFO: Loading: tokenize\n",
      "2023-12-02 21:59:25 INFO: Loading: pos\n",
      "2023-12-02 21:59:26 INFO: Loading: lemma\n",
      "2023-12-02 21:59:26 INFO: Loading: constituency\n",
      "2023-12-02 21:59:26 INFO: Loading: depparse\n",
      "2023-12-02 21:59:26 INFO: Loading: sentiment\n",
      "2023-12-02 21:59:27 INFO: Loading: ner\n",
      "2023-12-02 21:59:27 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------\n",
      "Parameters Logged in MLflow\n",
      "\n",
      "\n",
      "--------------\n",
      "Raw Dataframe:\n",
      "(34756, 15)\n",
      "\n",
      "\n",
      "--------------\n",
      "Preprocessed Dataframe:\n",
      "(34656, 13)\n",
      "\n",
      "\n",
      "--------------\n",
      "Dataframes logged as MLflow artifacts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34656/34656 [4:27:23<00:00,  2.16it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------\n",
      "Word Matrix:\n",
      "\n",
      "\n",
      "--------------\n",
      "sklearn dill/pkls logged as MLflow artifacts\n"
     ]
    }
   ],
   "source": [
    "# create pipelines relevant to library used\n",
    "# MLflow example uses HuggingFace\n",
    "# below is example for MeaLeon with Stanza and sklearn NLP pipeline\n",
    "\n",
    "with mlflow.start_run(experiment_id=mlflow_exp_id):\n",
    "    # import necessary libraries to handle raw data\n",
    "    import dill as pickle\n",
    "    import dvc.api\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import (\n",
    "        CountVectorizer\n",
    "        , TfidfTransformer\n",
    "        , TfidfVectorizer\n",
    "        ,\n",
    "    )\n",
    "    from src.custom_stanza_mlflow import StanzaWrapper\n",
    "    import src.dataframe_preprocessor as dfpp\n",
    "    import stanza\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    stanza.download('en')\n",
    "    nlp = stanza.Pipeline('en')\n",
    "    \n",
    "    # cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "    cv_params = {\n",
    "        'strip_accents':\"unicode\",\n",
    "        'lowercase':True,\n",
    "        'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "        'min_df':10,\n",
    "    }\n",
    "\n",
    "    # pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "    pipeline_params = {\n",
    "        'stanza_model': 'en',\n",
    "        'language': 'english',\n",
    "        'sklearn-transformer': 'TfidfVectorizer'\n",
    "    }\n",
    "\n",
    "    # update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "    pipeline_params.update(cv_params)\n",
    "\n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(\"Parameters Logged in MLflow\")\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Raw Dataframe:', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    # to_nlp_df = pre_proc_df[0:50]\n",
    "    # to_nlp_df\n",
    "\n",
    "    # save and log preprocessed dataframe(s)\n",
    "    pre_proc_df.to_json('../data/processed/preprocessed_dataframe.json')\n",
    "    mlflow.log_artifact('../data/processed/preprocessed_dataframe.json', artifact_path=\"preprocessed_dataframes\")\n",
    "    \n",
    "    # to_nlp_df.to_json('../data/processed/preprocessed_subset_dataframe.json')\n",
    "    # mlflow.log_artifact('../data/processed/preprocessed_subset_dataframe.json', artifact_path=\"preprocessed_dataframes\")\n",
    "    \n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Dataframes logged as MLflow artifacts')\n",
    "\n",
    "    # LOG MODEL\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn TFIDFVectorizer\n",
    "    tfidf_vectorizer_model = TfidfVectorizer(**cv_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    test_tfidf_transform = tfidf_vectorizer_model.fit_transform(tqdm(pre_proc_df[\"ingredients\"]))\n",
    "\n",
    "    word_matrix = pd.DataFrame(\n",
    "        test_tfidf_transform.toarray()\n",
    "        , columns=tfidf_vectorizer_model.get_feature_names_out()\n",
    "        , index=pre_proc_df.index\n",
    "    )\n",
    "\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Word Matrix:', end='\\n')\n",
    "    print(word_matrix.head())\n",
    "\n",
    "    with open(\"../joblib/tfidf_transformer.pkl\", \"wb\") as fo:\n",
    "        pickle.dump(tfidf_vectorizer_model, fo)\n",
    "        mlflow.log_artifact(\"../joblib/tfidf_transformer.pkl\", artifact_path=\"sklearn_dill_pkls\")\n",
    "\n",
    "    with open(\"../joblib/database_word_matrix.pkl\", \"wb\") as fo:\n",
    "        pickle.dump(word_matrix, fo)\n",
    "        mlflow.log_artifact(\"../joblib/database_word_matrix.pkl\", artifact_path=\"sklearn_dill_pkls\")\n",
    "\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('sklearn dill/pkls logged as MLflow artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
