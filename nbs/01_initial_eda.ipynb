{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis of Epicurious Scrape in a JSON file\n",
        "\n",
        "This is an idealized workflow for Aaron Chen in looking at data science problems. It likely isn't the best path, nor has he rigidly applied or stuck to this ideal, but he wishes that he worked this way more frequently.\n",
        "\n",
        "## Purpose: Work through some exploratory data analysis of the Epicurious scrape on stream. Try to write some functions to help process the data.\n",
        "\n",
        "### Author: Aaron Chen"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "---"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "### If needed, run shell commands here"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# !python -m spacy download en_core_web_sm\n",
        "!dvc pull -r origin\n",
        "!dvc commit -f ../data.dvc"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0% Checkout|                                       |0/1 [00:00<?,     ?file/s]\n",
            "!\u001b[A\n",
            "Building data objects from ../data                    |0.00 [00:00,      ?obj/s]\u001b[A\n",
            "Everything is up to date.                                                       \u001b[A\n",
            "\u001b[0m                                                                            "
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "---"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## External Resources\n",
        "\n",
        "List out references or documentation that has helped you with this notebook\n",
        "\n",
        "### Code\n",
        "Regex Checker: https://regex101.com/\n",
        "\n",
        "#### Scikit-learn\n",
        "1. https://scikit-learn.org/stable/modules/decomposition.html#latent-dirichlet-allocation-lda\n",
        "\n",
        "\n",
        "### Data\n",
        "\n",
        "For this notebook, the data is stored in the repo base folder/data/raw\n",
        "\n",
        "### Process\n",
        "\n",
        "Are there steps or tutorials you are following? Those are things I try to list in Process"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "---"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Import necessary libraries"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# | hide\n",
        "import project_path\n",
        "from datetime import datetime\n",
        "import dvc.api\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from tqdm import tqdm\n",
        "from typing import Dict\n",
        "import unicodedata\n",
        "\n",
        "# import local scripts\n",
        "import src.dataframe_preprocessor as dfpp"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "---"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Define helper functions\n",
        "\n",
        "My workflow is to try things with code cells, then when the code cells get messy and repetitive, to convert into helper functions that can be called.\n",
        "\n",
        "When the helper functions are getting used a lot, it is usually better to convert them to scripts or classes that can be called/instantiated"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "---"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Define global variables \n",
        "\n",
        "**Remember to refactor these out, not ideal**"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# | hide\n",
        "data_path = \"data/recipes-en-201706/epicurious-recipes_m2.json\"\n",
        "\n",
        "repo_url = \"https://dagshub.com/AaronWChen/MeaLeon\""
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "---"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Running Commentary\n",
        "\n",
        "1. I used numbered lists to keep track of things I noticed\n",
        "\n",
        "### To Do\n",
        "\n",
        "1. Try to determine consistency of nested data structures\n",
        "   1. Is the photoData or number of things inside photoData the same from record to record\n",
        "   2. What about for tag?\n",
        "\n",
        "Data wasn't fully consistent but logic in helper function helped handle nulls\n",
        "\n",
        "2. How to handle nulls?\n",
        "   1. Author      Filled in with \"Missing Author\"\n",
        "   2. Tag         Filled in with \"Missing Cuisine\"\n",
        "3. ~~Convert pubDate to actual timestamp~~  \n",
        "4. ~~Convert ScrapeDate to actual timestamp~~\n",
        "   1. This was ignored as the datestamp was not useful (generally within minutes of the origin of UNIX time)\n",
        "   \n",
        "**5. Append new columns for relevant nested structures and unfold them**\n",
        "\n",
        "6. Determine actual types of `ingredients` and `prepSteps`\n",
        "7. Continue working through test example of single recipe to feed into spaCy and then sklearn.feature_extraction.text stack\n",
        "8. Will need to remove numbers, punctuation"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "---"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Importing and viewing the data as a dataframe"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# with dvc.api.open(data_path, repo=repo_url) as f:\n",
        "data = dvc.api.read(path=data_path, repo=repo_url, rev=\"dev\", remote=\"origin-s3\")\n",
        "epic_dataframe = pd.read_json(data)\n",
        "epic_dataframe.head()"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "ename": "PathMissingError",
          "evalue": "The path 'data/recipes-en-201706/epicurious-recipes_m2.json' does not exist in the target repository 'https://dagshub.com/AaronWChen/MeaLeon' neither as a DVC output nor as a Git-tracked file.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/scmrepo/fs.py:215\u001b[0m, in \u001b[0;36mGitFileSystem.info\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     \u001b[39m# NOTE: to avoid wasting time computing object size, trie.info\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[39m# will return a LazyDict instance, that will compute compute size\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# only when it is accessed.\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrie\u001b[39m.\u001b[39;49minfo(key)\n\u001b[1;32m    216\u001b[0m     ret[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m path\n",
            "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/scmrepo/git/objects.py:141\u001b[0m, in \u001b[0;36mGitTrie.info\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscmrepo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyDict\n\u001b[0;32m--> 141\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrie[key]\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msize\u001b[39m():\n",
            "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pygtrie.py:937\u001b[0m, in \u001b[0;36mTrie.__getitem__\u001b[0;34m(self, key_or_slice)\u001b[0m\n\u001b[1;32m    936\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitervalues(key_or_slice\u001b[39m.\u001b[39mstart)\n\u001b[0;32m--> 937\u001b[0m node, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_node(key_or_slice)\n\u001b[1;32m    938\u001b[0m \u001b[39mif\u001b[39;00m node\u001b[39m.\u001b[39mvalue \u001b[39mis\u001b[39;00m _EMPTY:\n",
            "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pygtrie.py:630\u001b[0m, in \u001b[0;36mTrie._get_node\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[39mif\u001b[39;00m node \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 630\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[1;32m    631\u001b[0m trace\u001b[39m.\u001b[39mappend((step, node))\n",
            "\u001b[0;31mKeyError\u001b[0m: ('data', 'recipes-en-201706', 'epicurious-recipes_m2.json')",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/dvc/repo/__init__.py:538\u001b[0m, in \u001b[0;36mRepo.open_by_relpath\u001b[0;34m(self, path, remote, mode, encoding)\u001b[0m\n\u001b[1;32m    537\u001b[0m remote_odb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcloud\u001b[39m.\u001b[39mget_remote_odb(name\u001b[39m=\u001b[39mremote)\n\u001b[0;32m--> 538\u001b[0m oid \u001b[39m=\u001b[39m fs\u001b[39m.\u001b[39;49minfo(fs_path)[\u001b[39m\"\u001b[39m\u001b[39mdvc_info\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmd5\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    539\u001b[0m fs \u001b[39m=\u001b[39m remote_odb\u001b[39m.\u001b[39mfs\n",
            "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/dvc_objects/fs/base.py:481\u001b[0m, in \u001b[0;36mFileSystem.info\u001b[0;34m(self, path, callback, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 481\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfs\u001b[39m.\u001b[39;49minfo(path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    482\u001b[0m callback\u001b[39m.\u001b[39mset_size(\u001b[39mlen\u001b[39m(path))\n",
            "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/dvc/fs/dvc.py:317\u001b[0m, in \u001b[0;36m_DVCFileSystem.info\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m ignore_subrepos \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mignore_subrepos\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info(key, path, ignore_subrepos\u001b[39m=\u001b[39;49mignore_subrepos)\n",
            "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/dvc/fs/dvc.py:335\u001b[0m, in \u001b[0;36m_DVCFileSystem._info\u001b[0;34m(self, key, path, ignore_subrepos, check_ignored)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     fs_info \u001b[39m=\u001b[39m fs\u001b[39m.\u001b[39;49minfo(fs_path)\n\u001b[1;32m    336\u001b[0m     \u001b[39mif\u001b[39;00m check_ignored \u001b[39mand\u001b[39;00m repo\u001b[39m.\u001b[39mdvcignore\u001b[39m.\u001b[39mis_ignored(\n\u001b[1;32m    337\u001b[0m         fs, fs_path, ignore_subrepos\u001b[39m=\u001b[39mignore_subrepos\n\u001b[1;32m    338\u001b[0m     ):\n",
            "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/dvc_objects/fs/base.py:481\u001b[0m, in \u001b[0;36mFileSystem.info\u001b[0;34m(self, path, callback, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 481\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfs\u001b[39m.\u001b[39;49minfo(path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    482\u001b[0m callback\u001b[39m.\u001b[39mset_size(\u001b[39mlen\u001b[39m(path))\n",
            "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/scmrepo/fs.py:219\u001b[0m, in \u001b[0;36mGitFileSystem.info\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(errno\u001b[39m.\u001b[39mENOENT, os\u001b[39m.\u001b[39mstrerror(errno\u001b[39m.\u001b[39mENOENT), path)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/recipes-en-201706/epicurious-recipes_m2.json'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mFileMissingError\u001b[0m                          Traceback (most recent call last)",
            "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/dvc/external_repo.py:86\u001b[0m, in \u001b[0;36mexternal_repo\u001b[0;34m(url, rev, for_write, cache_dir, cache_types, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     \u001b[39myield\u001b[39;00m repo\n\u001b[1;32m     87\u001b[0m \u001b[39mexcept\u001b[39;00m NoRemoteError \u001b[39mas\u001b[39;00m exc:\n",
            "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/dvc/api/data.py:198\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(path, repo, rev, remote, mode, encoding)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mwith\u001b[39;00m Repo\u001b[39m.\u001b[39mopen(repo, rev\u001b[39m=\u001b[39mrev, subrepos\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, uninitialized\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m _repo:\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mwith\u001b[39;00m _repo\u001b[39m.\u001b[39mopen_by_relpath(\n\u001b[1;32m    199\u001b[0m         path, remote\u001b[39m=\u001b[39mremote, mode\u001b[39m=\u001b[39mmode, encoding\u001b[39m=\u001b[39mencoding\n\u001b[1;32m    200\u001b[0m     ) \u001b[39mas\u001b[39;00m fd:\n\u001b[1;32m    201\u001b[0m         \u001b[39myield\u001b[39;00m fd\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.10.10/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
            "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/dvc/repo/__init__.py:549\u001b[0m, in \u001b[0;36mRepo.open_by_relpath\u001b[0;34m(self, path, remote, mode, encoding)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m--> 549\u001b[0m     \u001b[39mraise\u001b[39;00m FileMissingError(path) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIsADirectoryError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
            "\u001b[0;31mFileMissingError\u001b[0m: Can't find 'data/recipes-en-201706/epicurious-recipes_m2.json' neither locally nor on remote",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mPathMissingError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# with dvc.api.open(data_path, repo=repo_url) as f:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[39m=\u001b[39m dvc\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mread(path\u001b[39m=\u001b[39;49mdata_path, repo\u001b[39m=\u001b[39;49mrepo_url, rev\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdev\u001b[39;49m\u001b[39m'\u001b[39;49m, remote\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39morigin-s3\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m epic_dataframe \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_json(data)\n\u001b[1;32m      4\u001b[0m epic_dataframe\u001b[39m.\u001b[39mhead()\n",
            "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/dvc/api/data.py:210\u001b[0m, in \u001b[0;36mread\u001b[0;34m(path, repo, rev, remote, mode, encoding)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(path, repo\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, rev\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, remote\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m    Returns the contents of a tracked file (by DVC or Git). For Git repos, HEAD\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    is used unless a rev argument is supplied. The default remote is tried\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m    unless a remote argument is supplied.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    211\u001b[0m         path, repo\u001b[39m=\u001b[39mrepo, rev\u001b[39m=\u001b[39mrev, remote\u001b[39m=\u001b[39mremote, mode\u001b[39m=\u001b[39mmode, encoding\u001b[39m=\u001b[39mencoding\n\u001b[1;32m    212\u001b[0m     ) \u001b[39mas\u001b[39;00m fd:\n\u001b[1;32m    213\u001b[0m         \u001b[39mreturn\u001b[39;00m fd\u001b[39m.\u001b[39mread()\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.10.10/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
            "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/dvc/api/data.py:197\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(path, repo, rev, remote, mode, encoding)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open\u001b[39m(path, repo\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, rev\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, remote\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 197\u001b[0m     \u001b[39mwith\u001b[39;00m Repo\u001b[39m.\u001b[39mopen(repo, rev\u001b[39m=\u001b[39mrev, subrepos\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, uninitialized\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m _repo:\n\u001b[1;32m    198\u001b[0m         \u001b[39mwith\u001b[39;00m _repo\u001b[39m.\u001b[39mopen_by_relpath(\n\u001b[1;32m    199\u001b[0m             path, remote\u001b[39m=\u001b[39mremote, mode\u001b[39m=\u001b[39mmode, encoding\u001b[39m=\u001b[39mencoding\n\u001b[1;32m    200\u001b[0m         ) \u001b[39mas\u001b[39;00m fd:\n\u001b[1;32m    201\u001b[0m             \u001b[39myield\u001b[39;00m fd\n",
            "File \u001b[0;32m~/.asdf/installs/python/3.10.10/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m     value \u001b[39m=\u001b[39m typ()\n\u001b[1;32m    152\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[1;32m    154\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m value\n",
            "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/dvc/external_repo.py:94\u001b[0m, in \u001b[0;36mexternal_repo\u001b[0;34m(url, rev, for_write, cache_dir, cache_types, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m FileMissingError \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m---> 94\u001b[0m     \u001b[39mraise\u001b[39;00m PathMissingError(exc\u001b[39m.\u001b[39mpath, url) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     repo\u001b[39m.\u001b[39mclose()\n",
            "\u001b[0;31mPathMissingError\u001b[0m: The path 'data/recipes-en-201706/epicurious-recipes_m2.json' does not exist in the target repository 'https://dagshub.com/AaronWChen/MeaLeon' neither as a DVC output nor as a Git-tracked file."
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe[\"type\"].value_counts()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe.shape"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe.describe()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe.info()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe[\"aggregateRating\"].value_counts()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "Columns:\n",
        "\n",
        "    Index\n",
        "\n",
        "    ID: string\n",
        "\n",
        "    dek: appears to be description of the recipe, string\n",
        "\n",
        "    hed: Appears to be title, string\n",
        "\n",
        "    pubDate: appears to be publication date, may need to reformat to datetime objects\n",
        "\n",
        "    author: appears that each record contains an array (list), inside each list is a dictionary with 'name' as the key and author name as the value. Notably, not a unique identifier for the value. Because the data is technically nested, may need to extract and transform and add columns to dataframe\n",
        "\n",
        "    type: string, but all the values are exactly the same and they are all in the category of \"recipe\". Drop column\n",
        "\n",
        "    url: Appears to be a long string leading to where the recipe can be found on Epicurious's website\n",
        "\n",
        "    photoData: nested structure, inside each record is a dictionary\n",
        "\n",
        "    tag: each record contains a dictionary. may need to extract and transform and add columns to dataframe\n",
        "\n",
        "    aggregateRating: float, let's say it's out of 4.0\n",
        "\n",
        "    ingredients: appears to be a list, does look like a list of strings\n",
        "\n",
        "    prepSteps: appears to be a list, does look like a list of strings\n",
        "\n",
        "    reviewsCount: int\n",
        "\n",
        "    willMakeAgainPct: integer\t\n",
        "    \n",
        "    dateCrawled: appears to be a unix timestamp"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "Let's take a look at the possibly problematic columns and see if the data structures make sense or how we can approach transforming them into new columns for the dataset"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe.loc[0]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe.loc[0][\"photoData\"]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "It looks like photoData contains:\n",
        "    1. photo ID, string\n",
        "    2. photo filename, string\n",
        "    3. photo caption, string\n",
        "    4. photo credit, string\n",
        "    5. promoTitle, string\n",
        "    6. title, string\n",
        "       1. caption, promoTitle, and title could be all the same\n",
        "    7. orientation, string\n",
        "    8. restrictCropping: boolean\n",
        "\n",
        "Of these, maybe we should keep\n",
        "id => photoID\n",
        "filename => photoFilename\n",
        "caption => photoCaption\n",
        "credit => photoCredit\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe.loc[0][\"tag\"]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe.loc[100][\"tag\"]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe.loc[10][\"tag\"]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe.loc[1][\"tag\"]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe.loc[1][\"ingredients\"]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "type(epic_dataframe.loc[1][\"ingredients\"])"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "Save this aside for CountVectorization/Natural Language Processing later, continue doing feature exploration"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe.loc[1][\"dek\"]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe[\"tag\"][2]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe[\"cuisine_name\"] = epic_dataframe[\"tag\"].apply(\n",
        "    lambda x: x[\"name\"]\n",
        "    if not pd.isna(x) and x[\"category\"] == \"cuisine\"\n",
        "    else \"Cuisine Missing\"\n",
        ")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe[\"cuisine_name\"].head()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe[\"cuisine_name\"][0]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "print(epic_dataframe.shape)\n",
        "print(epic_dataframe[epic_dataframe[\"cuisine_name\"] != \"Missing\"].shape)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe[epic_dataframe[\"cuisine_name\"] == \"Cuisine Missing\"]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# this lambda function goes into the photo data column and extracts just the filename from the dictionary\n",
        "epic_dataframe[\"photo_filename\"] = epic_dataframe[\"photoData\"].apply(\n",
        "    lambda x: x[\"filename\"] if not pd.isna(x) else \"Missing photo\"\n",
        ")\n",
        "\n",
        "# This lambda function goes into the photo data column and extracts just the photo credit from the dictionary\n",
        "epic_dataframe[\"photo_credit\"] = epic_dataframe[\"photoData\"].apply(\n",
        "    lambda x: x[\"credit\"] if not pd.isna(x) else \"Missing credit\"\n",
        ")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe.head()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# This lambda function cleans up the column and adds a new column dataframe\n",
        "epic_dataframe[\"author_name\"] = epic_dataframe[\"author\"].apply(\n",
        "    lambda x: x[0][\"name\"] if not pd.isna(x) else \"Missing author name\"\n",
        ")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "This function did not work, what happened?"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "first_five_epic = epic_dataframe.head()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "first_five_epic"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "first_five_epic.iloc[0][\"author\"][0]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "first_five_epic.iloc[1][\"author\"][0]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "first_five_epic[\"author\"].apply(lambda x: x[0][\"name\"] if x else \"Missing author name\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "This lambda function now works enough! It goes into author column and extracts the author as long as the record isn't an empty list. This can be refactored into a helper function. But we need to apply to the whole dataset"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe[\"author_name\"] = epic_dataframe[\"author\"].apply(\n",
        "    lambda x: x[0][\"name\"] if x else \"Missing author name\"\n",
        ")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Let's add a feature to fix the datetimes"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "test_pubdate_array = epic_dataframe[\"pubDate\"][0:5]\n",
        "test_pubdate_array"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "print(type(test_pubdate_array[0][:10]))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "test_pubdate_array[0][:10]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe[\"publication_date\"] = epic_dataframe[\"date_published\"].apply(\n",
        "    lambda x: datetime.strptime(x[:10], \"%Y-%m-%d\")\n",
        ")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe[\"publication_date\"]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe[\"publication_date_todt\"] = pd.to_datetime(\n",
        "    epic_dataframe[\"pubDate\"], infer_datetime_format=True\n",
        ")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe[\"publication_date_todt\"]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "Don't need the apply with lambda function anymore because to_datetime succesfully resolved the odd string"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe[\"date_scraped\"] = pd.to_datetime(\n",
        "    epic_dataframe[\"dateCrawled\"], infer_datetime_format=True\n",
        ")\n",
        "epic_dataframe[\"date_scraped\"]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe[\"date_scraped\"].describe()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "epic_dataframe[\"tag\"]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "Based on the timestamps, it seems like we can drop the crawled/scraped column because the values don't really make sense and would not help\n",
        "\n",
        "### Next Steps\n",
        "- Refactor datetime processing into functions\n",
        "- Consider deploying as a pd.pipe()\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "### Checking how to get the correct path inside the project_path script"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "sys.path"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "module_path"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "os.pardir"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}