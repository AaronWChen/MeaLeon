{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Stanza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae1a5a3a6ed4b0e8b9daced5014b239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 21:29:37 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-11-08 21:29:38 INFO: File exists: /home/awchen/stanza_resources/en/default.zip\n",
      "2023-11-08 21:29:41 INFO: Finished downloading models and saved to /home/awchen/stanza_resources.\n",
      "2023-11-08 21:29:41 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98046ed7e0114917bfe8b6439f7b6c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 21:29:41 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2023-11-08 21:29:41 INFO: Using device: cuda\n",
      "2023-11-08 21:29:41 INFO: Loading: tokenize\n",
      "2023-11-08 21:29:44 INFO: Loading: pos\n",
      "2023-11-08 21:29:45 INFO: Loading: lemma\n",
      "2023-11-08 21:29:45 INFO: Loading: constituency\n",
      "2023-11-08 21:29:45 INFO: Loading: depparse\n",
      "2023-11-08 21:29:45 INFO: Loading: sentiment\n",
      "2023-11-08 21:29:45 INFO: Loading: ner\n",
      "2023-11-08 21:29:46 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *\n",
    "import project_path\n",
    "\n",
    "# from bertopic import BERTopic\n",
    "import dagshub\n",
    "dagshub.init(repo_name='MeaLeon', repo_owner='AaronWChen')\n",
    "# from datetime import datetime\n",
    "# from hdbscan import HDBSCAN\n",
    "from itertools import tee, islice\n",
    "import joblib \n",
    "import json\n",
    "# import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "# from sklearn.base import TransformerMixin\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfTransformer,\n",
    "    TfidfVectorizer,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# import spacy\n",
    "# import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# from spacy.lemmatizer import Lemmatizer\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "# from umap import UMAP\n",
    "\n",
    "# import local scripts\n",
    "# import src.nlp_processor as nlpp\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@markdown Enter the username of your DAGsHub account:\n",
    "DAGSHUB_USER_NAME = \"AaronWChen\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the email for your DAGsHub account:\n",
    "DAGSHUB_EMAIL = \"awc33@cornell.edu\" #@param {type:\"string\"}\n",
    "\n",
    "DAGSHUB_REPO_NAME=\"MeaLeon\"\n",
    "BRANCH=\"STANZA-1/refactor-nltk-stanza\"\n",
    "dagshub.init(repo_name='MeaLeon', repo_owner='AaronWChen')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_id(name):\n",
    "    exp = mlflow.get_experiment_by_name(name)\n",
    "    if exp is None:\n",
    "      exp_id = mlflow.create_experiment(name)\n",
    "      return exp_id\n",
    "    return exp.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_preprocessor(stanza_pipeline, ingredients_list):\n",
    "    # This function takes in a Stanza pipeline and a recipe's ingredients in list form and returns a Stanza transformed document to be used in the lemmatizer\n",
    "    lowered = \" brk \".join(ingredients_list).lower()\n",
    "    # lowered = ingredients_list.apply(\" brk \".join).str.lower()\n",
    "    print(type(stanza_pipeline(lowered)))\n",
    "    print(stanza_pipeline(lowered))\n",
    "    return stanza_pipeline(lowered)\n",
    "\n",
    "# print(*[f'word: {word.text+\" \"}\\t \\t lemma: {word.lemma}, \\t \\t upos: {word.upos}' for sent in doc2.sentences for word in sent.words], sep='\\n')\n",
    "\n",
    "# This will be the tokenizer?\n",
    "# lemma_test_recipe_2 = \" \".join([word.lemma for sent in doc2.sentences for word in sent.words if (\n",
    "#     word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\"]\n",
    "# )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_lemmatizer(stanza_preprocessed):\n",
    "    # This function takes in the preprocessed Stanza document from preprocessor and performs lemmatization and filtering\n",
    "    \n",
    "    return \" \".join([word.lemma \n",
    "                      for sent in stanza_preprocessed.sentences \n",
    "                      for word in sent.words if (\n",
    "                          word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\"]\n",
    "                          )\n",
    "                    ])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom ngram analyzer function, matching only ngrams that belong to the same line\n",
    "def gen_analyzer(minNgramLength, maxNgramLength):\n",
    "    def ngrams_per_line(doc):\n",
    "\n",
    "        # analyze each line of the input string seperately\n",
    "        for ln in doc.split('brk'):\n",
    "\n",
    "            # tokenize the input string (customize the regex as desired)\n",
    "            terms = re.findall(u'(?u)\\\\b\\\\w+\\\\b', ln)\n",
    "\n",
    "            # loop ngram creation for every number between min and max ngram length\n",
    "            for ngramLength in range(minNgramLength, maxNgramLength+1):\n",
    "\n",
    "                # find and return all ngrams\n",
    "                # for ngram in zip(*[terms[i:] for i in range(3)]): <-- solution without a generator (works the same but has higher memory usage)\n",
    "                for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]): # <-- solution using a generator\n",
    "                    ngram = ' '.join(ngram)\n",
    "                    yield ngram\n",
    "    return ngrams_per_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = '../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "food_stopwords_path = \"../food_stopwords.csv\"\n",
    "\n",
    "joblib_basepath = '../joblib/2022.08.23/'\n",
    "\n",
    "cv_path = joblib_basepath + 'countvec.joblib'\n",
    "tfidf_path = joblib_basepath + 'tfidf.joblib'\n",
    "full_df_path = joblib_basepath + 'recipes_with_cv.joblib'\n",
    "reduced_df_path = joblib_basepath + 'reduced_df.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a redeem for variable naming mixed with a free pun-ish me daddy, flushtrated will be the list of all stopword to exclude so named because we're throwing these words down the drain\n",
    "\n",
    "flushtrated = {x for x in pd.read_csv(food_stopwords_path)}\n",
    "additional_to_exclude = {\n",
    "    \"red\",\n",
    "    \"green\",\n",
    "    \"black\",\n",
    "    \"yellow\",\n",
    "    \"white\",\n",
    "    \"inch\",\n",
    "    \"mince\",\n",
    "    \"chop\",\n",
    "    \"fry\",\n",
    "    \"trim\",\n",
    "    \"flat\",\n",
    "    \"beat\",\n",
    "    \"brown\",\n",
    "    \"golden\",\n",
    "    \"balsamic\",\n",
    "    \"halve\",\n",
    "    \"blue\",\n",
    "    \"divide\",\n",
    "    \"trim\",\n",
    "    \"unbleache\",\n",
    "    \"granulate\",\n",
    "    \"Frank\",\n",
    "    \"alternative\",\n",
    "    \"american\",\n",
    "    \"annie\",\n",
    "    \"asian\",\n",
    "    \"balance\",\n",
    "    \"band\",\n",
    "    \"barrel\",\n",
    "    \"bay\",\n",
    "    \"bayou\",\n",
    "    \"beam\",\n",
    "    \"beard\",\n",
    "    \"bell\",\n",
    "    \"betty\",\n",
    "    \"bird\",\n",
    "    \"blast\",\n",
    "    \"bob\",\n",
    "    \"bone\",\n",
    "    \"breyers\",\n",
    "    \"calore\",\n",
    "    \"carb\",\n",
    "    \"card\",\n",
    "    \"chachere\",\n",
    "    \"change\",\n",
    "    \"circle\",\n",
    "    \"coffee\",\n",
    "    \"coil\",\n",
    "    \"country\",\n",
    "    \"cow\",\n",
    "    \"crack\",\n",
    "    \"cracker\",\n",
    "    \"crocker\",\n",
    "    \"crystal\",\n",
    "    \"dean\",\n",
    "    \"degree\",\n",
    "    \"deluxe\",\n",
    "    \"direction\",\n",
    "    \"duncan\",\n",
    "    \"earth\",\n",
    "    \"eggland\",\n",
    "    \"ener\",\n",
    "    \"envelope\",\n",
    "    \"eye\",\n",
    "    \"fantastic\",\n",
    "    \"far\",\n",
    "    \"fat\",\n",
    "    \"feather\",\n",
    "    \"flake\",\n",
    "    \"foot\",\n",
    "    \"fourth\",\n",
    "    \"frank\",\n",
    "    \"french\",\n",
    "    \"fusion\",\n",
    "    \"genoa\",\n",
    "    \"genovese\",\n",
    "    \"germain\",\n",
    "    \"giada\",\n",
    "    \"gold\",\n",
    "    \"granule\",\n",
    "    \"greek\",\n",
    "    \"hamburger\",\n",
    "    \"helper\",\n",
    "    \"herbe\",\n",
    "    \"hines\",\n",
    "    \"hodgson\",\n",
    "    \"hunt\",\n",
    "    \"instruction\",\n",
    "    \"interval\",\n",
    "    \"italianstyle\",\n",
    "    \"jim\",\n",
    "    \"jimmy\",\n",
    "    \"kellogg\",\n",
    "    \"lagrille\",\n",
    "    \"lake\",\n",
    "    \"land\",\n",
    "    \"laurentiis\",\n",
    "    \"lawry\",\n",
    "    \"lipton\",\n",
    "    \"litre\",\n",
    "    \"ll\",\n",
    "    \"maid\",\n",
    "    \"malt\",\n",
    "    \"mate\",\n",
    "    \"mayer\",\n",
    "    \"meal\",\n",
    "    \"medal\",\n",
    "    \"medallion\",\n",
    "    \"member\",\n",
    "    \"mexicanstyle\",\n",
    "    \"monte\",\n",
    "    \"mori\",\n",
    "    \"nest\",\n",
    "    \"nu\",\n",
    "    \"oounce\",\n",
    "    \"oscar\",\n",
    "    \"ox\",\n",
    "    \"paso\",\n",
    "    \"pasta\",\n",
    "    \"patty\",\n",
    "    \"petal\",\n",
    "    \"pinche\",\n",
    "    \"preserve\",\n",
    "    \"quartere\",\n",
    "    \"ranch\",\n",
    "    \"ranchstyle\",\n",
    "    \"rasher\",\n",
    "    \"redhot\",\n",
    "    \"resemble\",\n",
    "    \"rice\",\n",
    "    \"ro\",\n",
    "    \"roni\",\n",
    "    \"scissor\",\n",
    "    \"scrap\",\n",
    "    \"secret\",\n",
    "    \"semicircle\",\n",
    "    \"shard\",\n",
    "    \"shear\",\n",
    "    \"sixth\",\n",
    "    \"sliver\",\n",
    "    \"smucker\",\n",
    "    \"snicker\",\n",
    "    \"source\",\n",
    "    \"spot\",\n",
    "    \"state\",\n",
    "    \"strand\",\n",
    "    \"sun\",\n",
    "    \"supreme\",\n",
    "    \"tablepoon\",\n",
    "    \"tail\",\n",
    "    \"target\",\n",
    "    \"tm\",\n",
    "    \"tong\",\n",
    "    \"toothpick\",\n",
    "    \"triangle\",\n",
    "    \"trimming\",\n",
    "    \"tweezer\",\n",
    "    \"valley\",\n",
    "    \"vay\",\n",
    "    \"wise\",\n",
    "    \"wishbone\",\n",
    "    \"wrapper\",\n",
    "    \"yoplait\",\n",
    "    \"ziploc\",\n",
    "}\n",
    "\n",
    "flushtrated = flushtrated.union(STOP_WORDS)\n",
    "flushtrated = flushtrated.union(additional_to_exclude)\n",
    "flushtrated_list = list(flushtrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dek</th>\n",
       "      <th>hed</th>\n",
       "      <th>aggregateRating</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>prepSteps</th>\n",
       "      <th>reviewsCount</th>\n",
       "      <th>willMakeAgainPct</th>\n",
       "      <th>cuisine_name</th>\n",
       "      <th>photo_filename</th>\n",
       "      <th>...</th>\n",
       "      <th>zest pith</th>\n",
       "      <th>zest vegetable</th>\n",
       "      <th>zinfandel</th>\n",
       "      <th>ziti</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zucchini blossom</th>\n",
       "      <th>zucchini crookneck</th>\n",
       "      <th>zucchini squash</th>\n",
       "      <th>árbol</th>\n",
       "      <th>árbol pepper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54a2b6b019925f464b373351</td>\n",
       "      <td>How does fried chicken achieve No. 1 status? B...</td>\n",
       "      <td>Pickle-Brined Fried Chicken</td>\n",
       "      <td>3.11</td>\n",
       "      <td>[1 tablespoons yellow mustard seeds, 1 tablesp...</td>\n",
       "      <td>[Toast mustard and coriander seeds in a dry me...</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>51247610_fried-chicken_1x1.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54a408a019925f464b3733bc</td>\n",
       "      <td>Spinaci all'Ebraica</td>\n",
       "      <td>Spinach Jewish Style</td>\n",
       "      <td>3.22</td>\n",
       "      <td>[3 pounds small-leaved bulk spinach, Salt, 1/2...</td>\n",
       "      <td>[Remove the stems and roots from the spinach. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>Italian</td>\n",
       "      <td>EP_12162015_placeholders_rustic.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54a408a26529d92b2c003631</td>\n",
       "      <td>This majestic, moist, and richly spiced honey ...</td>\n",
       "      <td>New Year’s Honey Cake</td>\n",
       "      <td>3.62</td>\n",
       "      <td>[3 1/2 cups all-purpose flour, 1 tablespoon ba...</td>\n",
       "      <td>[I like this cake best baked in a 9-inch angel...</td>\n",
       "      <td>105</td>\n",
       "      <td>88</td>\n",
       "      <td>Kosher</td>\n",
       "      <td>EP_09022015_honeycake-2.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54a408a66529d92b2c003638</td>\n",
       "      <td>The idea for this sandwich came to me when my ...</td>\n",
       "      <td>The B.L.A.Bagel with Lox and Avocado</td>\n",
       "      <td>4.00</td>\n",
       "      <td>[1 small ripe avocado, preferably Hass (see No...</td>\n",
       "      <td>[A short time before serving, mash avocado and...</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>Kosher</td>\n",
       "      <td>EP_12162015_placeholders_casual.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54a408a719925f464b3733cc</td>\n",
       "      <td>In 1930, Simon Agranat, the chief justice of t...</td>\n",
       "      <td>Shakshuka a la Doktor Shakshuka</td>\n",
       "      <td>2.71</td>\n",
       "      <td>[2 pounds fresh tomatoes, unpeeled and cut in ...</td>\n",
       "      <td>[1. Place the tomatoes, garlic, salt, paprika,...</td>\n",
       "      <td>7</td>\n",
       "      <td>83</td>\n",
       "      <td>Kosher</td>\n",
       "      <td>EP_12162015_placeholders_formal.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34751</th>\n",
       "      <td>59541a31bff3052847ae2107</td>\n",
       "      <td>Buttering the bread before you waffle it ensur...</td>\n",
       "      <td>Waffled Ham and Cheese Melt with Maple Butter</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[1 tablespoon unsalted butter, at room tempera...</td>\n",
       "      <td>[Preheat the waffle iron on low., Spread a thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>waffle-ham-and-cheese-melt-062817.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34752</th>\n",
       "      <td>5954233ad52ca90dc28200e7</td>\n",
       "      <td>Spread this easy compound butter on waffles, p...</td>\n",
       "      <td>Maple Butter</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[8 tablespoons (1 stick) salted butter, at roo...</td>\n",
       "      <td>[Combine the ingredients in a medium-size bowl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>EP_12162015_placeholders_bright.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34753</th>\n",
       "      <td>595424c2109c972493636f83</td>\n",
       "      <td>Leftover mac and cheese is not exactly one of ...</td>\n",
       "      <td>Waffled Macaroni and Cheese</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[3 tablespoons unsalted butter, plus more for ...</td>\n",
       "      <td>[Preheat the oven to 375°F. Butter a 9x5-inch ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>waffle-mac-n-cheese-062816.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34754</th>\n",
       "      <td>5956638625dc3d1d829b7166</td>\n",
       "      <td>A classic Mexican beer cocktail you can sip al...</td>\n",
       "      <td>Classic Michelada</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[Coarse salt, 2 lime wedges, 2 ounces tomato j...</td>\n",
       "      <td>[Place about 1/4 cup salt on a small plate. Ru...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>Classic Michelada 07292017.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34755</th>\n",
       "      <td>59566daa25dc3d1d829b7169</td>\n",
       "      <td>A grapefruit beer that's one of the most refre...</td>\n",
       "      <td>So Radler</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[1 bottle (375 ml) sour beer, such as Almanac ...</td>\n",
       "      <td>[Combine the water, honey, rosemary, and grape...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>EP_12162015_placeholders_bright.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34656 rows × 3365 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  \\\n",
       "0      54a2b6b019925f464b373351   \n",
       "1      54a408a019925f464b3733bc   \n",
       "2      54a408a26529d92b2c003631   \n",
       "3      54a408a66529d92b2c003638   \n",
       "4      54a408a719925f464b3733cc   \n",
       "...                         ...   \n",
       "34751  59541a31bff3052847ae2107   \n",
       "34752  5954233ad52ca90dc28200e7   \n",
       "34753  595424c2109c972493636f83   \n",
       "34754  5956638625dc3d1d829b7166   \n",
       "34755  59566daa25dc3d1d829b7169   \n",
       "\n",
       "                                                     dek  \\\n",
       "0      How does fried chicken achieve No. 1 status? B...   \n",
       "1                                    Spinaci all'Ebraica   \n",
       "2      This majestic, moist, and richly spiced honey ...   \n",
       "3      The idea for this sandwich came to me when my ...   \n",
       "4      In 1930, Simon Agranat, the chief justice of t...   \n",
       "...                                                  ...   \n",
       "34751  Buttering the bread before you waffle it ensur...   \n",
       "34752  Spread this easy compound butter on waffles, p...   \n",
       "34753  Leftover mac and cheese is not exactly one of ...   \n",
       "34754  A classic Mexican beer cocktail you can sip al...   \n",
       "34755  A grapefruit beer that's one of the most refre...   \n",
       "\n",
       "                                                 hed  aggregateRating  \\\n",
       "0                        Pickle-Brined Fried Chicken             3.11   \n",
       "1                               Spinach Jewish Style             3.22   \n",
       "2                              New Year’s Honey Cake             3.62   \n",
       "3              The B.L.A.Bagel with Lox and Avocado             4.00   \n",
       "4                    Shakshuka a la Doktor Shakshuka             2.71   \n",
       "...                                              ...              ...   \n",
       "34751  Waffled Ham and Cheese Melt with Maple Butter             0.00   \n",
       "34752                                   Maple Butter             0.00   \n",
       "34753                    Waffled Macaroni and Cheese             0.00   \n",
       "34754                              Classic Michelada             0.00   \n",
       "34755                                      So Radler             0.00   \n",
       "\n",
       "                                             ingredients  \\\n",
       "0      [1 tablespoons yellow mustard seeds, 1 tablesp...   \n",
       "1      [3 pounds small-leaved bulk spinach, Salt, 1/2...   \n",
       "2      [3 1/2 cups all-purpose flour, 1 tablespoon ba...   \n",
       "3      [1 small ripe avocado, preferably Hass (see No...   \n",
       "4      [2 pounds fresh tomatoes, unpeeled and cut in ...   \n",
       "...                                                  ...   \n",
       "34751  [1 tablespoon unsalted butter, at room tempera...   \n",
       "34752  [8 tablespoons (1 stick) salted butter, at roo...   \n",
       "34753  [3 tablespoons unsalted butter, plus more for ...   \n",
       "34754  [Coarse salt, 2 lime wedges, 2 ounces tomato j...   \n",
       "34755  [1 bottle (375 ml) sour beer, such as Almanac ...   \n",
       "\n",
       "                                               prepSteps  reviewsCount  \\\n",
       "0      [Toast mustard and coriander seeds in a dry me...             7   \n",
       "1      [Remove the stems and roots from the spinach. ...             5   \n",
       "2      [I like this cake best baked in a 9-inch angel...           105   \n",
       "3      [A short time before serving, mash avocado and...             7   \n",
       "4      [1. Place the tomatoes, garlic, salt, paprika,...             7   \n",
       "...                                                  ...           ...   \n",
       "34751  [Preheat the waffle iron on low., Spread a thi...             0   \n",
       "34752  [Combine the ingredients in a medium-size bowl...             0   \n",
       "34753  [Preheat the oven to 375°F. Butter a 9x5-inch ...             0   \n",
       "34754  [Place about 1/4 cup salt on a small plate. Ru...             0   \n",
       "34755  [Combine the water, honey, rosemary, and grape...             0   \n",
       "\n",
       "       willMakeAgainPct     cuisine_name  \\\n",
       "0                   100  Missing Cuisine   \n",
       "1                    80          Italian   \n",
       "2                    88           Kosher   \n",
       "3                   100           Kosher   \n",
       "4                    83           Kosher   \n",
       "...                 ...              ...   \n",
       "34751                 0  Missing Cuisine   \n",
       "34752                 0  Missing Cuisine   \n",
       "34753                 0  Missing Cuisine   \n",
       "34754                 0  Missing Cuisine   \n",
       "34755                 0  Missing Cuisine   \n",
       "\n",
       "                              photo_filename  ... zest pith zest vegetable  \\\n",
       "0             51247610_fried-chicken_1x1.jpg  ...       0.0            0.0   \n",
       "1        EP_12162015_placeholders_rustic.jpg  ...       0.0            0.0   \n",
       "2                EP_09022015_honeycake-2.jpg  ...       0.0            0.0   \n",
       "3        EP_12162015_placeholders_casual.jpg  ...       0.0            0.0   \n",
       "4        EP_12162015_placeholders_formal.jpg  ...       0.0            0.0   \n",
       "...                                      ...  ...       ...            ...   \n",
       "34751  waffle-ham-and-cheese-melt-062817.jpg  ...       0.0            0.0   \n",
       "34752    EP_12162015_placeholders_bright.jpg  ...       0.0            0.0   \n",
       "34753         waffle-mac-n-cheese-062816.jpg  ...       0.0            0.0   \n",
       "34754         Classic Michelada 07292017.jpg  ...       0.0            0.0   \n",
       "34755    EP_12162015_placeholders_bright.jpg  ...       0.0            0.0   \n",
       "\n",
       "      zinfandel ziti  zucchini  zucchini blossom  zucchini crookneck  \\\n",
       "0           0.0  0.0       0.0               0.0                 0.0   \n",
       "1           0.0  0.0       0.0               0.0                 0.0   \n",
       "2           0.0  0.0       0.0               0.0                 0.0   \n",
       "3           0.0  0.0       0.0               0.0                 0.0   \n",
       "4           0.0  0.0       0.0               0.0                 0.0   \n",
       "...         ...  ...       ...               ...                 ...   \n",
       "34751       0.0  0.0       0.0               0.0                 0.0   \n",
       "34752       0.0  0.0       0.0               0.0                 0.0   \n",
       "34753       0.0  0.0       0.0               0.0                 0.0   \n",
       "34754       0.0  0.0       0.0               0.0                 0.0   \n",
       "34755       0.0  0.0       0.0               0.0                 0.0   \n",
       "\n",
       "       zucchini squash  árbol  árbol pepper  \n",
       "0                  0.0    0.0           0.0  \n",
       "1                  0.0    0.0           0.0  \n",
       "2                  0.0    0.0           0.0  \n",
       "3                  0.0    0.0           0.0  \n",
       "4                  0.0    0.0           0.0  \n",
       "...                ...    ...           ...  \n",
       "34751              0.0    0.0           0.0  \n",
       "34752              0.0    0.0           0.0  \n",
       "34753              0.0    0.0           0.0  \n",
       "34754              0.0    0.0           0.0  \n",
       "34755              0.0    0.0           0.0  \n",
       "\n",
       "[34656 rows x 3365 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = joblib.load(full_df_path)\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 tablespoons yellow mustard seeds',\n",
       " '1 tablespoons brown mustard seeds',\n",
       " '1 1/2 teaspoons coriander seeds',\n",
       " '1 cup apple cider vinegar',\n",
       " '2/3 cup kosher salt',\n",
       " '1/3 cup sugar',\n",
       " '1/4 cup chopped fresh dill',\n",
       " '8 skinless, boneless chicken thighs (about 3 pounds), halved, quartered if large',\n",
       " 'Vegetable oil (for frying; about 10 cups)',\n",
       " '2 cups buttermilk',\n",
       " '2 cups all-purpose flour',\n",
       " 'Kosher salt',\n",
       " 'Honey, flaky sea salt (such as Maldon), toasted benne or sesame seeds, hot sauce (for serving)',\n",
       " 'A deep-fry thermometer']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['ingredients'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 tablespoons yellow mustard seeds. 1 tablespoons brown mustard seeds. 1 1/2 teaspoons coriander seeds. 1 cup apple cider vinegar. 2/3 cup kosher salt. 1/3 cup sugar. 1/4 cup chopped fresh dill. 8 skinless, boneless chicken thighs (about 3 pounds), halved, quartered if large. vegetable oil (for frying; about 10 cups). 2 cups buttermilk. 2 cups all-purpose flour. kosher salt. honey, flaky sea salt (such as maldon), toasted benne or sesame seeds, hot sauce (for serving). a deep-fry thermometer'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_recipe = \". \".join(full_df['ingredients'][0]).lower()\n",
    "test_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(test_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: 1\n",
      "id: (2,)\ttext: tablespoons\n",
      "id: (3,)\ttext: yellow\n",
      "id: (4,)\ttext: mustard\n",
      "id: (5,)\ttext: seeds\n",
      "id: (6,)\ttext: .\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: 1\n",
      "id: (2,)\ttext: tablespoons\n",
      "id: (3,)\ttext: brown\n",
      "id: (4,)\ttext: mustard\n",
      "id: (5,)\ttext: seeds\n",
      "id: (6,)\ttext: .\n",
      "====== Sentence 3 tokens =======\n",
      "id: (1,)\ttext: 1\n",
      "id: (2,)\ttext: 1/2\n",
      "id: (3,)\ttext: teaspoons\n",
      "id: (4,)\ttext: coriander\n",
      "id: (5,)\ttext: seeds\n",
      "id: (6,)\ttext: .\n",
      "====== Sentence 4 tokens =======\n",
      "id: (1,)\ttext: 1\n",
      "id: (2,)\ttext: cup\n",
      "id: (3,)\ttext: apple\n",
      "id: (4,)\ttext: cider\n",
      "id: (5,)\ttext: vinegar\n",
      "id: (6,)\ttext: .\n",
      "====== Sentence 5 tokens =======\n",
      "id: (1,)\ttext: 2/3\n",
      "id: (2,)\ttext: cup\n",
      "id: (3,)\ttext: kosher\n",
      "id: (4,)\ttext: salt\n",
      "id: (5,)\ttext: .\n",
      "====== Sentence 6 tokens =======\n",
      "id: (1,)\ttext: 1/3\n",
      "id: (2,)\ttext: cup\n",
      "id: (3,)\ttext: sugar\n",
      "id: (4,)\ttext: .\n",
      "====== Sentence 7 tokens =======\n",
      "id: (1,)\ttext: 1/4\n",
      "id: (2,)\ttext: cup\n",
      "id: (3,)\ttext: chopped\n",
      "id: (4,)\ttext: fresh\n",
      "id: (5,)\ttext: dill\n",
      "id: (6,)\ttext: .\n",
      "====== Sentence 8 tokens =======\n",
      "id: (1,)\ttext: 8\n",
      "id: (2,)\ttext: skinless\n",
      "id: (3,)\ttext: ,\n",
      "id: (4,)\ttext: boneless\n",
      "id: (5,)\ttext: chicken\n",
      "id: (6,)\ttext: thighs\n",
      "id: (7,)\ttext: (\n",
      "id: (8,)\ttext: about\n",
      "id: (9,)\ttext: 3\n",
      "id: (10,)\ttext: pounds\n",
      "id: (11,)\ttext: )\n",
      "id: (12,)\ttext: ,\n",
      "id: (13,)\ttext: halved\n",
      "id: (14,)\ttext: ,\n",
      "id: (15,)\ttext: quartered\n",
      "id: (16,)\ttext: if\n",
      "id: (17,)\ttext: large\n",
      "id: (18,)\ttext: .\n",
      "====== Sentence 9 tokens =======\n",
      "id: (1,)\ttext: vegetable\n",
      "id: (2,)\ttext: oil\n",
      "id: (3,)\ttext: (\n",
      "id: (4,)\ttext: for\n",
      "id: (5,)\ttext: frying\n",
      "id: (6,)\ttext: ;\n",
      "id: (7,)\ttext: about\n",
      "id: (8,)\ttext: 10\n",
      "id: (9,)\ttext: cups\n",
      "id: (10,)\ttext: )\n",
      "id: (11,)\ttext: .\n",
      "====== Sentence 10 tokens =======\n",
      "id: (1,)\ttext: 2\n",
      "id: (2,)\ttext: cups\n",
      "id: (3,)\ttext: buttermilk\n",
      "id: (4,)\ttext: .\n",
      "====== Sentence 11 tokens =======\n",
      "id: (1,)\ttext: 2\n",
      "id: (2,)\ttext: cups\n",
      "id: (3,)\ttext: all\n",
      "id: (4,)\ttext: -\n",
      "id: (5,)\ttext: purpose\n",
      "id: (6,)\ttext: flour\n",
      "id: (7,)\ttext: .\n",
      "====== Sentence 12 tokens =======\n",
      "id: (1,)\ttext: kosher\n",
      "id: (2,)\ttext: salt\n",
      "id: (3,)\ttext: .\n",
      "====== Sentence 13 tokens =======\n",
      "id: (1,)\ttext: honey\n",
      "id: (2,)\ttext: ,\n",
      "id: (3,)\ttext: flaky\n",
      "id: (4,)\ttext: sea\n",
      "id: (5,)\ttext: salt\n",
      "id: (6,)\ttext: (\n",
      "id: (7,)\ttext: such\n",
      "id: (8,)\ttext: as\n",
      "id: (9,)\ttext: maldon\n",
      "id: (10,)\ttext: )\n",
      "id: (11,)\ttext: ,\n",
      "id: (12,)\ttext: toasted\n",
      "id: (13,)\ttext: benne\n",
      "id: (14,)\ttext: or\n",
      "id: (15,)\ttext: sesame\n",
      "id: (16,)\ttext: seeds\n",
      "id: (17,)\ttext: ,\n",
      "id: (18,)\ttext: hot\n",
      "id: (19,)\ttext: sauce\n",
      "id: (20,)\ttext: (\n",
      "id: (21,)\ttext: for\n",
      "id: (22,)\ttext: serving\n",
      "id: (23,)\ttext: )\n",
      "id: (24,)\ttext: .\n",
      "====== Sentence 14 tokens =======\n",
      "id: (1,)\ttext: a\n",
      "id: (2,)\ttext: deep\n",
      "id: (3,)\ttext: -\n",
      "id: (4,)\ttext: fry\n",
      "id: (5,)\ttext: thermometer\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 tablespoons yellow mustard seeds.', '1 tablespoons brown mustard seeds.', '1 1/2 teaspoons coriander seeds.', '1 cup apple cider vinegar.', '2/3 cup kosher salt.', '1/3 cup sugar.', '1/4 cup chopped fresh dill.', '8 skinless, boneless chicken thighs (about 3 pounds), halved, quartered if large.', 'vegetable oil (for frying; about 10 cups).', '2 cups buttermilk.', '2 cups all-purpose flour.', 'kosher salt.', 'honey, flaky sea salt (such as maldon), toasted benne or sesame seeds, hot sauce (for serving).', 'a deep-fry thermometer']\n"
     ]
    }
   ],
   "source": [
    "print([sentence.text for sentence in doc.sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: 1 \t \t lemma: 1, \t \t upos: NUM\n",
      "word: tablespoons \t \t lemma: tablespoon, \t \t upos: NOUN\n",
      "word: yellow \t \t lemma: yellow, \t \t upos: ADJ\n",
      "word: mustard \t \t lemma: mustard, \t \t upos: NOUN\n",
      "word: seeds \t \t lemma: seed, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 1 \t \t lemma: 1, \t \t upos: NUM\n",
      "word: tablespoons \t \t lemma: tablespoon, \t \t upos: NOUN\n",
      "word: brown \t \t lemma: brown, \t \t upos: ADJ\n",
      "word: mustard \t \t lemma: mustard, \t \t upos: NOUN\n",
      "word: seeds \t \t lemma: seed, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 1 \t \t lemma: 1, \t \t upos: NUM\n",
      "word: 1/2 \t \t lemma: 1/2, \t \t upos: NUM\n",
      "word: teaspoons \t \t lemma: teaspoon, \t \t upos: NOUN\n",
      "word: coriander \t \t lemma: coriander, \t \t upos: NOUN\n",
      "word: seeds \t \t lemma: seed, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 1 \t \t lemma: 1, \t \t upos: NUM\n",
      "word: cup \t \t lemma: cup, \t \t upos: NOUN\n",
      "word: apple \t \t lemma: apple, \t \t upos: NOUN\n",
      "word: cider \t \t lemma: cider, \t \t upos: NOUN\n",
      "word: vinegar \t \t lemma: vinegar, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 2/3 \t \t lemma: 2/3, \t \t upos: NUM\n",
      "word: cup \t \t lemma: cup, \t \t upos: NOUN\n",
      "word: kosher \t \t lemma: kosher, \t \t upos: NOUN\n",
      "word: salt \t \t lemma: salt, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 1/3 \t \t lemma: 1/3, \t \t upos: NUM\n",
      "word: cup \t \t lemma: cup, \t \t upos: NOUN\n",
      "word: sugar \t \t lemma: sugar, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 1/4 \t \t lemma: 1/4, \t \t upos: NUM\n",
      "word: cup \t \t lemma: cup, \t \t upos: NOUN\n",
      "word: chopped \t \t lemma: chop, \t \t upos: VERB\n",
      "word: fresh \t \t lemma: fresh, \t \t upos: ADJ\n",
      "word: dill \t \t lemma: dill, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 8 \t \t lemma: 8, \t \t upos: NUM\n",
      "word: skinless \t \t lemma: skinless, \t \t upos: NOUN\n",
      "word: , \t \t lemma: ,, \t \t upos: PUNCT\n",
      "word: boneless \t \t lemma: boneless, \t \t upos: ADJ\n",
      "word: chicken \t \t lemma: chicken, \t \t upos: NOUN\n",
      "word: thighs \t \t lemma: thigh, \t \t upos: NOUN\n",
      "word: ( \t \t lemma: (, \t \t upos: PUNCT\n",
      "word: about \t \t lemma: about, \t \t upos: ADV\n",
      "word: 3 \t \t lemma: 3, \t \t upos: NUM\n",
      "word: pounds \t \t lemma: pound, \t \t upos: NOUN\n",
      "word: ) \t \t lemma: ), \t \t upos: PUNCT\n",
      "word: , \t \t lemma: ,, \t \t upos: PUNCT\n",
      "word: halved \t \t lemma: halve, \t \t upos: VERB\n",
      "word: , \t \t lemma: ,, \t \t upos: PUNCT\n",
      "word: quartered \t \t lemma: quarter, \t \t upos: VERB\n",
      "word: if \t \t lemma: if, \t \t upos: SCONJ\n",
      "word: large \t \t lemma: large, \t \t upos: ADJ\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: vegetable \t \t lemma: vegetable, \t \t upos: NOUN\n",
      "word: oil \t \t lemma: oil, \t \t upos: NOUN\n",
      "word: ( \t \t lemma: (, \t \t upos: PUNCT\n",
      "word: for \t \t lemma: for, \t \t upos: ADP\n",
      "word: frying \t \t lemma: frying, \t \t upos: NOUN\n",
      "word: ; \t \t lemma: ;, \t \t upos: PUNCT\n",
      "word: about \t \t lemma: about, \t \t upos: ADV\n",
      "word: 10 \t \t lemma: 10, \t \t upos: NUM\n",
      "word: cups \t \t lemma: cup, \t \t upos: NOUN\n",
      "word: ) \t \t lemma: ), \t \t upos: PUNCT\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 2 \t \t lemma: 2, \t \t upos: NUM\n",
      "word: cups \t \t lemma: cup, \t \t upos: NOUN\n",
      "word: buttermilk \t \t lemma: buttermilk, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 2 \t \t lemma: 2, \t \t upos: NUM\n",
      "word: cups \t \t lemma: cup, \t \t upos: NOUN\n",
      "word: all \t \t lemma: all, \t \t upos: DET\n",
      "word: - \t \t lemma: -, \t \t upos: PUNCT\n",
      "word: purpose \t \t lemma: purpose, \t \t upos: NOUN\n",
      "word: flour \t \t lemma: flour, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: kosher \t \t lemma: kosher, \t \t upos: NOUN\n",
      "word: salt \t \t lemma: salt, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: honey \t \t lemma: honey, \t \t upos: NOUN\n",
      "word: , \t \t lemma: ,, \t \t upos: PUNCT\n",
      "word: flaky \t \t lemma: flaky, \t \t upos: ADJ\n",
      "word: sea \t \t lemma: sea, \t \t upos: NOUN\n",
      "word: salt \t \t lemma: salt, \t \t upos: NOUN\n",
      "word: ( \t \t lemma: (, \t \t upos: PUNCT\n",
      "word: such \t \t lemma: such, \t \t upos: ADJ\n",
      "word: as \t \t lemma: as, \t \t upos: ADP\n",
      "word: maldon \t \t lemma: maldon, \t \t upos: NOUN\n",
      "word: ) \t \t lemma: ), \t \t upos: PUNCT\n",
      "word: , \t \t lemma: ,, \t \t upos: PUNCT\n",
      "word: toasted \t \t lemma: toast, \t \t upos: VERB\n",
      "word: benne \t \t lemma: benne, \t \t upos: NOUN\n",
      "word: or \t \t lemma: or, \t \t upos: CCONJ\n",
      "word: sesame \t \t lemma: sesame, \t \t upos: NOUN\n",
      "word: seeds \t \t lemma: seed, \t \t upos: NOUN\n",
      "word: , \t \t lemma: ,, \t \t upos: PUNCT\n",
      "word: hot \t \t lemma: hot, \t \t upos: ADJ\n",
      "word: sauce \t \t lemma: sauce, \t \t upos: NOUN\n",
      "word: ( \t \t lemma: (, \t \t upos: PUNCT\n",
      "word: for \t \t lemma: for, \t \t upos: ADP\n",
      "word: serving \t \t lemma: serving, \t \t upos: NOUN\n",
      "word: ) \t \t lemma: ), \t \t upos: PUNCT\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: a \t \t lemma: a, \t \t upos: DET\n",
      "word: deep \t \t lemma: deep, \t \t upos: ADJ\n",
      "word: - \t \t lemma: -, \t \t upos: PUNCT\n",
      "word: fry \t \t lemma: fry, \t \t upos: NOUN\n",
      "word: thermometer \t \t lemma: thermometer, \t \t upos: NOUN\n"
     ]
    }
   ],
   "source": [
    "print(*[f'word: {word.text+\" \"}\\t \\t lemma: {word.lemma}, \\t \\t upos: {word.upos}' for sent in doc.sentences for word in sent.words], sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma: tablespoon\n",
      "lemma: yellow\n",
      "lemma: mustard\n",
      "lemma: seed\n",
      "lemma: tablespoon\n",
      "lemma: brown\n",
      "lemma: mustard\n",
      "lemma: seed\n",
      "lemma: teaspoon\n",
      "lemma: coriander\n",
      "lemma: seed\n",
      "lemma: cup\n",
      "lemma: apple\n",
      "lemma: cider\n",
      "lemma: vinegar\n",
      "lemma: cup\n",
      "lemma: kosher\n",
      "lemma: salt\n",
      "lemma: cup\n",
      "lemma: sugar\n",
      "lemma: cup\n",
      "lemma: chop\n",
      "lemma: fresh\n",
      "lemma: dill\n",
      "lemma: skinless\n",
      "lemma: boneless\n",
      "lemma: chicken\n",
      "lemma: thigh\n",
      "lemma: pound\n",
      "lemma: halve\n",
      "lemma: quarter\n",
      "lemma: large\n",
      "lemma: vegetable\n",
      "lemma: oil\n",
      "lemma: frying\n",
      "lemma: cup\n",
      "lemma: cup\n",
      "lemma: buttermilk\n",
      "lemma: cup\n",
      "lemma: purpose\n",
      "lemma: flour\n",
      "lemma: kosher\n",
      "lemma: salt\n",
      "lemma: honey\n",
      "lemma: flaky\n",
      "lemma: sea\n",
      "lemma: salt\n",
      "lemma: such\n",
      "lemma: maldon\n",
      "lemma: toast\n",
      "lemma: benne\n",
      "lemma: sesame\n",
      "lemma: seed\n",
      "lemma: hot\n",
      "lemma: sauce\n",
      "lemma: serving\n",
      "lemma: deep\n",
      "lemma: fry\n",
      "lemma: thermometer\n"
     ]
    }
   ],
   "source": [
    "print(*[f'lemma: {word.lemma}' for sent in doc.sentences for word in sent.words if (\n",
    "    word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"PUNCT\", \"SCONJ\"]\n",
    ")], sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tablespoon',\n",
       " 'yellow',\n",
       " 'mustard',\n",
       " 'seed',\n",
       " 'tablespoon',\n",
       " 'brown',\n",
       " 'mustard',\n",
       " 'seed',\n",
       " 'teaspoon',\n",
       " 'coriander',\n",
       " 'seed',\n",
       " 'cup',\n",
       " 'apple',\n",
       " 'cider',\n",
       " 'vinegar',\n",
       " 'cup',\n",
       " 'kosher',\n",
       " 'salt',\n",
       " 'cup',\n",
       " 'sugar',\n",
       " 'cup',\n",
       " 'chop',\n",
       " 'fresh',\n",
       " 'dill',\n",
       " 'skinless',\n",
       " 'boneless',\n",
       " 'chicken',\n",
       " 'thigh',\n",
       " 'pound',\n",
       " 'halve',\n",
       " 'quarter',\n",
       " 'large',\n",
       " 'vegetable',\n",
       " 'oil',\n",
       " 'frying',\n",
       " 'cup',\n",
       " 'cup',\n",
       " 'buttermilk',\n",
       " 'cup',\n",
       " 'purpose',\n",
       " 'flour',\n",
       " 'kosher',\n",
       " 'salt',\n",
       " 'honey',\n",
       " 'flaky',\n",
       " 'sea',\n",
       " 'salt',\n",
       " 'such',\n",
       " 'maldon',\n",
       " 'toast',\n",
       " 'benne',\n",
       " 'sesame',\n",
       " 'seed',\n",
       " 'hot',\n",
       " 'sauce',\n",
       " 'serving',\n",
       " 'deep',\n",
       " 'fry',\n",
       " 'thermometer']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.lemma for sent in doc.sentences for word in sent.words if (\n",
    "    word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"PUNCT\", \"SCONJ\"]\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tablespoon', 'yellow', 'mustard', 'seed'],\n",
       " ['tablespoon', 'brown', 'mustard', 'seed'],\n",
       " ['teaspoon', 'coriander', 'seed'],\n",
       " ['cup', 'apple', 'cider', 'vinegar'],\n",
       " ['cup', 'kosher', 'salt'],\n",
       " ['cup', 'sugar'],\n",
       " ['cup', 'chop', 'fresh', 'dill'],\n",
       " ['skinless',\n",
       "  'boneless',\n",
       "  'chicken',\n",
       "  'thigh',\n",
       "  'pound',\n",
       "  'halve',\n",
       "  'quarter',\n",
       "  'large'],\n",
       " ['vegetable', 'oil', 'frying', 'cup'],\n",
       " ['cup', 'buttermilk'],\n",
       " ['cup', 'purpose', 'flour'],\n",
       " ['kosher', 'salt'],\n",
       " ['honey',\n",
       "  'flaky',\n",
       "  'sea',\n",
       "  'salt',\n",
       "  'such',\n",
       "  'maldon',\n",
       "  'toast',\n",
       "  'benne',\n",
       "  'sesame',\n",
       "  'seed',\n",
       "  'hot',\n",
       "  'sauce',\n",
       "  'serving'],\n",
       " ['deep', 'fry', 'thermometer']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe = []\n",
    "for sent in doc.sentences:\n",
    "    ingredients = []\n",
    "    for word in sent.words:\n",
    "        if word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"PUNCT\", \"SCONJ\"]:\n",
    "            ingredients.append(word.lemma)\n",
    "        else:\n",
    "            pass\n",
    "    recipe.append(ingredients)\n",
    "\n",
    "recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found [this resource](https://stackoverflow.com/questions/26907309/create-ngrams-only-for-words-on-the-same-line-disregarding-line-breaks-with-sc), trying custom analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tablespoon yellow mustard seed brk tablespoon brown mustard seed brk teaspoon coriander seed brk cup apple cider vinegar brk cup kosher salt brk cup sugar brk cup chop fresh dill brk skinless , boneless chicken thigh ( pound ) , halve , quarter large brk vegetable oil ( frying ; cup ) brk cup buttermilk brk cup - purpose flour brk kosher salt brk honey , flaky sea salt ( such maldon ) , toast benne sesame seed , hot sauce ( serving ) brk deep - fry thermometer']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is probably going to be the preprocessor\n",
    "test_recipe_2 = \" brk \".join(full_df['ingredients'][0]).lower()\n",
    "doc2 = nlp(test_recipe_2)\n",
    "\n",
    "# print(*[f'word: {word.text+\" \"}\\t \\t lemma: {word.lemma}, \\t \\t upos: {word.upos}' for sent in doc2.sentences for word in sent.words], sep='\\n')\n",
    "\n",
    "# This will be the tokenizer?\n",
    "# lemma_test_recipe_2 = \" \".join([word.lemma for sent in doc2.sentences for word in sent.words if (\n",
    "#     word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\"]\n",
    "# )])\n",
    "lemma_test_recipe_2 = [\" \".join([word.lemma for sent in doc2.sentences for word in sent.words if (\n",
    "    word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\"]\n",
    ")])]\n",
    "lemma_test_recipe_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(analyzer=&lt;function gen_analyzer.&lt;locals&gt;.ngrams_per_line at 0x7f7fce4e0dc0&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&lt;function gen_analyzer.&lt;locals&gt;.ngrams_per_line at 0x7f7fce4e0dc0&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(analyzer=<function gen_analyzer.<locals>.ngrams_per_line>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(analyzer=gen_analyzer(1, 4))\n",
    "cv.fit(lemma_test_recipe_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['apple', 'apple cider', 'apple cider vinegar', 'benne',\n",
       "       'benne sesame', 'benne sesame seed', 'benne sesame seed hot',\n",
       "       'boneless', 'boneless chicken', 'boneless chicken thigh',\n",
       "       'boneless chicken thigh pound', 'brown', 'brown mustard',\n",
       "       'brown mustard seed', 'buttermilk', 'chicken', 'chicken thigh',\n",
       "       'chicken thigh pound', 'chicken thigh pound halve', 'chop',\n",
       "       'chop fresh', 'chop fresh dill', 'cider', 'cider vinegar',\n",
       "       'coriander', 'coriander seed', 'cup', 'cup apple',\n",
       "       'cup apple cider', 'cup apple cider vinegar', 'cup buttermilk',\n",
       "       'cup chop', 'cup chop fresh', 'cup chop fresh dill', 'cup kosher',\n",
       "       'cup kosher salt', 'cup purpose', 'cup purpose flour', 'cup sugar',\n",
       "       'deep', 'deep fry', 'deep fry thermometer', 'dill', 'flaky',\n",
       "       'flaky sea', 'flaky sea salt', 'flaky sea salt such', 'flour',\n",
       "       'fresh', 'fresh dill', 'fry', 'fry thermometer', 'frying',\n",
       "       'frying cup', 'halve', 'halve quarter', 'halve quarter large',\n",
       "       'honey', 'honey flaky', 'honey flaky sea', 'honey flaky sea salt',\n",
       "       'hot', 'hot sauce', 'hot sauce serving', 'kosher', 'kosher salt',\n",
       "       'large', 'maldon', 'maldon toast', 'maldon toast benne',\n",
       "       'maldon toast benne sesame', 'mustard', 'mustard seed', 'oil',\n",
       "       'oil frying', 'oil frying cup', 'pound', 'pound halve',\n",
       "       'pound halve quarter', 'pound halve quarter large', 'purpose',\n",
       "       'purpose flour', 'quarter', 'quarter large', 'salt', 'salt such',\n",
       "       'salt such maldon', 'salt such maldon toast', 'sauce',\n",
       "       'sauce serving', 'sea', 'sea salt', 'sea salt such',\n",
       "       'sea salt such maldon', 'seed', 'seed hot', 'seed hot sauce',\n",
       "       'seed hot sauce serving', 'serving', 'sesame', 'sesame seed',\n",
       "       'sesame seed hot', 'sesame seed hot sauce', 'skinless',\n",
       "       'skinless boneless', 'skinless boneless chicken',\n",
       "       'skinless boneless chicken thigh', 'such', 'such maldon',\n",
       "       'such maldon toast', 'such maldon toast benne', 'sugar',\n",
       "       'tablespoon', 'tablespoon brown', 'tablespoon brown mustard',\n",
       "       'tablespoon brown mustard seed', 'tablespoon yellow',\n",
       "       'tablespoon yellow mustard', 'tablespoon yellow mustard seed',\n",
       "       'teaspoon', 'teaspoon coriander', 'teaspoon coriander seed',\n",
       "       'thermometer', 'thigh', 'thigh pound', 'thigh pound halve',\n",
       "       'thigh pound halve quarter', 'toast', 'toast benne',\n",
       "       'toast benne sesame', 'toast benne sesame seed', 'vegetable',\n",
       "       'vegetable oil', 'vegetable oil frying',\n",
       "       'vegetable oil frying cup', 'vinegar', 'yellow', 'yellow mustard',\n",
       "       'yellow mustard seed'], dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = cv.fit_transform(lemma_test_recipe_2)\n",
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:531: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\n",
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:560: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m cv_params \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mstrip_accents\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39municode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmin_df\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m10\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m vectorizer_model \u001b[39m=\u001b[39m CountVectorizer(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcv_params)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m test_transform \u001b[39m=\u001b[39m vectorizer_model\u001b[39m.\u001b[39;49mfit_transform(test_recipe_pipeline)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1402\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[39mif\u001b[39;00m max_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1401\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort_features(X, vocabulary)\n\u001b[0;32m-> 1402\u001b[0m X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_words_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_limit_features(\n\u001b[1;32m   1403\u001b[0m     X, vocabulary, max_doc_count, min_doc_count, max_features\n\u001b[1;32m   1404\u001b[0m )\n\u001b[1;32m   1405\u001b[0m \u001b[39mif\u001b[39;00m max_features \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1406\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort_features(X, vocabulary)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1254\u001b[0m, in \u001b[0;36mCountVectorizer._limit_features\u001b[0;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[1;32m   1252\u001b[0m kept_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(mask)[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1253\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(kept_indices) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1254\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1255\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAfter pruning, no terms remain. Try a lower min_df or a higher max_df.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1256\u001b[0m     )\n\u001b[1;32m   1257\u001b[0m \u001b[39mreturn\u001b[39;00m X[:, kept_indices], removed_terms\n",
      "\u001b[0;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
     ]
    }
   ],
   "source": [
    "test_recipe_pipeline = full_df['ingredients'][0]\n",
    "\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'preprocessor':stanza_preprocessor,\n",
    "    'tokenizer':stanza_lemmatizer, # out of memory \n",
    "    # 'stop_words':flushtrated_list,\n",
    "    'analyzer': gen_analyzer(1, 4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "vectorizer_model = CountVectorizer(**cv_params)\n",
    "\n",
    "test_transform = vectorizer_model.fit_transform(test_recipe_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stanza.models.common.doc.Document'>\n",
      "[\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"1\",\n",
      "      \"lemma\": \"1\",\n",
      "      \"upos\": \"NUM\",\n",
      "      \"xpos\": \"CD\",\n",
      "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"nummod\",\n",
      "      \"start_char\": 0,\n",
      "      \"end_char\": 1,\n",
      "      \"ner\": \"S-CARDINAL\",\n",
      "      \"multi_ner\": [\n",
      "        \"S-CARDINAL\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"tablespoons\",\n",
      "      \"lemma\": \"tablespoon\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NNS\",\n",
      "      \"feats\": \"Number=Plur\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"obl:npmod\",\n",
      "      \"start_char\": 2,\n",
      "      \"end_char\": 13,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"yellow\",\n",
      "      \"lemma\": \"yellow\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 14,\n",
      "      \"end_char\": 20,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"mustard\",\n",
      "      \"lemma\": \"mustard\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 21,\n",
      "      \"end_char\": 28,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"seeds\",\n",
      "      \"lemma\": \"seed\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NNS\",\n",
      "      \"feats\": \"Number=Plur\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 29,\n",
      "      \"end_char\": 34,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \"brk\",\n",
      "      \"lemma\": \"brk\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 8,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 35,\n",
      "      \"end_char\": 38,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"text\": \"1\",\n",
      "      \"lemma\": \"1\",\n",
      "      \"upos\": \"NUM\",\n",
      "      \"xpos\": \"CD\",\n",
      "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
      "      \"head\": 8,\n",
      "      \"deprel\": \"nummod\",\n",
      "      \"start_char\": 39,\n",
      "      \"end_char\": 40,\n",
      "      \"ner\": \"S-CARDINAL\",\n",
      "      \"multi_ner\": [\n",
      "        \"S-CARDINAL\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"text\": \"tablespoons\",\n",
      "      \"lemma\": \"tablespoon\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NNS\",\n",
      "      \"feats\": \"Number=Plur\",\n",
      "      \"head\": 11,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 41,\n",
      "      \"end_char\": 52,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 9,\n",
      "      \"text\": \"brown\",\n",
      "      \"lemma\": \"brown\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 11,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 53,\n",
      "      \"end_char\": 58,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 10,\n",
      "      \"text\": \"mustard\",\n",
      "      \"lemma\": \"mustard\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 11,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 59,\n",
      "      \"end_char\": 66,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 11,\n",
      "      \"text\": \"seeds\",\n",
      "      \"lemma\": \"seed\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NNS\",\n",
      "      \"feats\": \"Number=Plur\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 67,\n",
      "      \"end_char\": 72,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 12,\n",
      "      \"text\": \"brk\",\n",
      "      \"lemma\": \"brk\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 17,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 73,\n",
      "      \"end_char\": 76,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 13,\n",
      "      \"text\": \"1\",\n",
      "      \"lemma\": \"1\",\n",
      "      \"upos\": \"NUM\",\n",
      "      \"xpos\": \"CD\",\n",
      "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
      "      \"head\": 15,\n",
      "      \"deprel\": \"nummod\",\n",
      "      \"start_char\": 77,\n",
      "      \"end_char\": 78,\n",
      "      \"ner\": \"B-QUANTITY\",\n",
      "      \"multi_ner\": [\n",
      "        \"B-QUANTITY\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 14,\n",
      "      \"text\": \"1/2\",\n",
      "      \"lemma\": \"1/2\",\n",
      "      \"upos\": \"NUM\",\n",
      "      \"xpos\": \"CD\",\n",
      "      \"feats\": \"NumForm=Word|NumType=Card\",\n",
      "      \"head\": 15,\n",
      "      \"deprel\": \"nummod\",\n",
      "      \"start_char\": 79,\n",
      "      \"end_char\": 82,\n",
      "      \"ner\": \"I-QUANTITY\",\n",
      "      \"multi_ner\": [\n",
      "        \"I-QUANTITY\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 15,\n",
      "      \"text\": \"teaspoons\",\n",
      "      \"lemma\": \"teaspoon\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NNS\",\n",
      "      \"feats\": \"Number=Plur\",\n",
      "      \"head\": 17,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 83,\n",
      "      \"end_char\": 92,\n",
      "      \"ner\": \"E-QUANTITY\",\n",
      "      \"multi_ner\": [\n",
      "        \"E-QUANTITY\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 16,\n",
      "      \"text\": \"coriander\",\n",
      "      \"lemma\": \"coriander\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 17,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 93,\n",
      "      \"end_char\": 102,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 17,\n",
      "      \"text\": \"seeds\",\n",
      "      \"lemma\": \"seed\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NNS\",\n",
      "      \"feats\": \"Number=Plur\",\n",
      "      \"head\": 11,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 103,\n",
      "      \"end_char\": 108,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 18,\n",
      "      \"text\": \"brk\",\n",
      "      \"lemma\": \"brk\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 11,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 109,\n",
      "      \"end_char\": 112,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 19,\n",
      "      \"text\": \"1\",\n",
      "      \"lemma\": \"1\",\n",
      "      \"upos\": \"NUM\",\n",
      "      \"xpos\": \"CD\",\n",
      "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
      "      \"head\": 20,\n",
      "      \"deprel\": \"nummod\",\n",
      "      \"start_char\": 113,\n",
      "      \"end_char\": 114,\n",
      "      \"ner\": \"S-CARDINAL\",\n",
      "      \"multi_ner\": [\n",
      "        \"S-CARDINAL\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 20,\n",
      "      \"text\": \"cup\",\n",
      "      \"lemma\": \"cup\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 17,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 115,\n",
      "      \"end_char\": 118,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 21,\n",
      "      \"text\": \"apple\",\n",
      "      \"lemma\": \"apple\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 22,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 119,\n",
      "      \"end_char\": 124,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 22,\n",
      "      \"text\": \"cider\",\n",
      "      \"lemma\": \"cider\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 23,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 125,\n",
      "      \"end_char\": 130,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 23,\n",
      "      \"text\": \"vinegar\",\n",
      "      \"lemma\": \"vinegar\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 17,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 131,\n",
      "      \"end_char\": 138,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 24,\n",
      "      \"text\": \"brk\",\n",
      "      \"lemma\": \"brk\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 28,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 139,\n",
      "      \"end_char\": 142,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 25,\n",
      "      \"text\": \"2/3\",\n",
      "      \"lemma\": \"2/3\",\n",
      "      \"upos\": \"NUM\",\n",
      "      \"xpos\": \"CD\",\n",
      "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
      "      \"head\": 26,\n",
      "      \"deprel\": \"nummod\",\n",
      "      \"start_char\": 143,\n",
      "      \"end_char\": 146,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 26,\n",
      "      \"text\": \"cup\",\n",
      "      \"lemma\": \"cup\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 28,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 147,\n",
      "      \"end_char\": 150,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 27,\n",
      "      \"text\": \"kosher\",\n",
      "      \"lemma\": \"kosher\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 28,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 151,\n",
      "      \"end_char\": 157,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 28,\n",
      "      \"text\": \"salt\",\n",
      "      \"lemma\": \"salt\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 32,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 158,\n",
      "      \"end_char\": 162,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 29,\n",
      "      \"text\": \"brk\",\n",
      "      \"lemma\": \"brk\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 32,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 163,\n",
      "      \"end_char\": 166,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 30,\n",
      "      \"text\": \"1/3\",\n",
      "      \"lemma\": \"1/3\",\n",
      "      \"upos\": \"NUM\",\n",
      "      \"xpos\": \"CD\",\n",
      "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
      "      \"head\": 31,\n",
      "      \"deprel\": \"nummod\",\n",
      "      \"start_char\": 167,\n",
      "      \"end_char\": 170,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 31,\n",
      "      \"text\": \"cup\",\n",
      "      \"lemma\": \"cup\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 32,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 171,\n",
      "      \"end_char\": 174,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 32,\n",
      "      \"text\": \"sugar\",\n",
      "      \"lemma\": \"sugar\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 23,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 175,\n",
      "      \"end_char\": 180,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 33,\n",
      "      \"text\": \"brk\",\n",
      "      \"lemma\": \"brk\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 35,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 181,\n",
      "      \"end_char\": 184,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 34,\n",
      "      \"text\": \"1/4\",\n",
      "      \"lemma\": \"1/4\",\n",
      "      \"upos\": \"NUM\",\n",
      "      \"xpos\": \"CD\",\n",
      "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
      "      \"head\": 35,\n",
      "      \"deprel\": \"nummod\",\n",
      "      \"start_char\": 185,\n",
      "      \"end_char\": 188,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 35,\n",
      "      \"text\": \"cup\",\n",
      "      \"lemma\": \"cup\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 39,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 189,\n",
      "      \"end_char\": 192,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 36,\n",
      "      \"text\": \"chopped\",\n",
      "      \"lemma\": \"chop\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBN\",\n",
      "      \"feats\": \"Tense=Past|VerbForm=Part\",\n",
      "      \"head\": 38,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 193,\n",
      "      \"end_char\": 200,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 37,\n",
      "      \"text\": \"fresh\",\n",
      "      \"lemma\": \"fresh\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 38,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 201,\n",
      "      \"end_char\": 206,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 38,\n",
      "      \"text\": \"dill\",\n",
      "      \"lemma\": \"dill\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 39,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 207,\n",
      "      \"end_char\": 211,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 39,\n",
      "      \"text\": \"brk\",\n",
      "      \"lemma\": \"brk\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 32,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 212,\n",
      "      \"end_char\": 215,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 40,\n",
      "      \"text\": \"8\",\n",
      "      \"lemma\": \"8\",\n",
      "      \"upos\": \"NUM\",\n",
      "      \"xpos\": \"CD\",\n",
      "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
      "      \"head\": 41,\n",
      "      \"deprel\": \"nummod\",\n",
      "      \"start_char\": 216,\n",
      "      \"end_char\": 217,\n",
      "      \"ner\": \"S-CARDINAL\",\n",
      "      \"multi_ner\": [\n",
      "        \"S-CARDINAL\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 41,\n",
      "      \"text\": \"skinless\",\n",
      "      \"lemma\": \"skinless\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 39,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 218,\n",
      "      \"end_char\": 226,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 42,\n",
      "      \"text\": \",\",\n",
      "      \"lemma\": \",\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \",\",\n",
      "      \"head\": 45,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 226,\n",
      "      \"end_char\": 227,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 43,\n",
      "      \"text\": \"boneless\",\n",
      "      \"lemma\": \"boneless\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 45,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 228,\n",
      "      \"end_char\": 236,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 44,\n",
      "      \"text\": \"chicken\",\n",
      "      \"lemma\": \"chicken\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 45,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 237,\n",
      "      \"end_char\": 244,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 45,\n",
      "      \"text\": \"thighs\",\n",
      "      \"lemma\": \"thigh\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NNS\",\n",
      "      \"feats\": \"Number=Plur\",\n",
      "      \"head\": 39,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 245,\n",
      "      \"end_char\": 251,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 46,\n",
      "      \"text\": \"(\",\n",
      "      \"lemma\": \"(\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \"-LRB-\",\n",
      "      \"head\": 49,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 252,\n",
      "      \"end_char\": 253,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 47,\n",
      "      \"text\": \"about\",\n",
      "      \"lemma\": \"about\",\n",
      "      \"upos\": \"ADV\",\n",
      "      \"xpos\": \"RB\",\n",
      "      \"head\": 48,\n",
      "      \"deprel\": \"advmod\",\n",
      "      \"start_char\": 253,\n",
      "      \"end_char\": 258,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 48,\n",
      "      \"text\": \"3\",\n",
      "      \"lemma\": \"3\",\n",
      "      \"upos\": \"NUM\",\n",
      "      \"xpos\": \"CD\",\n",
      "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
      "      \"head\": 49,\n",
      "      \"deprel\": \"nummod\",\n",
      "      \"start_char\": 259,\n",
      "      \"end_char\": 260,\n",
      "      \"ner\": \"B-QUANTITY\",\n",
      "      \"multi_ner\": [\n",
      "        \"B-QUANTITY\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 49,\n",
      "      \"text\": \"pounds\",\n",
      "      \"lemma\": \"pound\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NNS\",\n",
      "      \"feats\": \"Number=Plur\",\n",
      "      \"head\": 45,\n",
      "      \"deprel\": \"parataxis\",\n",
      "      \"start_char\": 261,\n",
      "      \"end_char\": 267,\n",
      "      \"ner\": \"E-QUANTITY\",\n",
      "      \"multi_ner\": [\n",
      "        \"E-QUANTITY\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 50,\n",
      "      \"text\": \")\",\n",
      "      \"lemma\": \")\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \"-RRB-\",\n",
      "      \"head\": 49,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 267,\n",
      "      \"end_char\": 268,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 51,\n",
      "      \"text\": \",\",\n",
      "      \"lemma\": \",\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \",\",\n",
      "      \"head\": 52,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 268,\n",
      "      \"end_char\": 269,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 52,\n",
      "      \"text\": \"halved\",\n",
      "      \"lemma\": \"halve\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBN\",\n",
      "      \"feats\": \"Tense=Past|VerbForm=Part\",\n",
      "      \"head\": 54,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 270,\n",
      "      \"end_char\": 276,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 53,\n",
      "      \"text\": \",\",\n",
      "      \"lemma\": \",\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \",\",\n",
      "      \"head\": 54,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 276,\n",
      "      \"end_char\": 277,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 54,\n",
      "      \"text\": \"quartered\",\n",
      "      \"lemma\": \"quarter\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBN\",\n",
      "      \"feats\": \"Tense=Past|VerbForm=Part\",\n",
      "      \"head\": 39,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 278,\n",
      "      \"end_char\": 287,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 55,\n",
      "      \"text\": \"if\",\n",
      "      \"lemma\": \"if\",\n",
      "      \"upos\": \"SCONJ\",\n",
      "      \"xpos\": \"IN\",\n",
      "      \"head\": 59,\n",
      "      \"deprel\": \"mark\",\n",
      "      \"start_char\": 288,\n",
      "      \"end_char\": 290,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 56,\n",
      "      \"text\": \"large\",\n",
      "      \"lemma\": \"large\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 59,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 291,\n",
      "      \"end_char\": 296,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 57,\n",
      "      \"text\": \"brk\",\n",
      "      \"lemma\": \"brk\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 59,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 297,\n",
      "      \"end_char\": 300,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 58,\n",
      "      \"text\": \"vegetable\",\n",
      "      \"lemma\": \"vegetable\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 59,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 301,\n",
      "      \"end_char\": 310,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 59,\n",
      "      \"text\": \"oil\",\n",
      "      \"lemma\": \"oil\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 54,\n",
      "      \"deprel\": \"advcl\",\n",
      "      \"start_char\": 311,\n",
      "      \"end_char\": 314,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 60,\n",
      "      \"text\": \"(\",\n",
      "      \"lemma\": \"(\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \"-LRB-\",\n",
      "      \"head\": 66,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 315,\n",
      "      \"end_char\": 316,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 61,\n",
      "      \"text\": \"for\",\n",
      "      \"lemma\": \"for\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"IN\",\n",
      "      \"head\": 62,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 316,\n",
      "      \"end_char\": 319,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 62,\n",
      "      \"text\": \"frying\",\n",
      "      \"lemma\": \"frying\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 59,\n",
      "      \"deprel\": \"nmod\",\n",
      "      \"start_char\": 320,\n",
      "      \"end_char\": 326,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 63,\n",
      "      \"text\": \";\",\n",
      "      \"lemma\": \";\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \":\",\n",
      "      \"head\": 66,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 326,\n",
      "      \"end_char\": 327,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 64,\n",
      "      \"text\": \"about\",\n",
      "      \"lemma\": \"about\",\n",
      "      \"upos\": \"ADV\",\n",
      "      \"xpos\": \"RB\",\n",
      "      \"head\": 65,\n",
      "      \"deprel\": \"advmod\",\n",
      "      \"start_char\": 328,\n",
      "      \"end_char\": 333,\n",
      "      \"ner\": \"B-QUANTITY\",\n",
      "      \"multi_ner\": [\n",
      "        \"B-QUANTITY\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 65,\n",
      "      \"text\": \"10\",\n",
      "      \"lemma\": \"10\",\n",
      "      \"upos\": \"NUM\",\n",
      "      \"xpos\": \"CD\",\n",
      "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
      "      \"head\": 66,\n",
      "      \"deprel\": \"nummod\",\n",
      "      \"start_char\": 334,\n",
      "      \"end_char\": 336,\n",
      "      \"ner\": \"I-QUANTITY\",\n",
      "      \"multi_ner\": [\n",
      "        \"I-QUANTITY\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 66,\n",
      "      \"text\": \"cups\",\n",
      "      \"lemma\": \"cup\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NNS\",\n",
      "      \"feats\": \"Number=Plur\",\n",
      "      \"head\": 59,\n",
      "      \"deprel\": \"parataxis\",\n",
      "      \"start_char\": 337,\n",
      "      \"end_char\": 341,\n",
      "      \"ner\": \"I-QUANTITY\",\n",
      "      \"multi_ner\": [\n",
      "        \"I-QUANTITY\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 67,\n",
      "      \"text\": \")\",\n",
      "      \"lemma\": \")\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \"-RRB-\",\n",
      "      \"head\": 66,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 341,\n",
      "      \"end_char\": 342,\n",
      "      \"ner\": \"E-QUANTITY\",\n",
      "      \"multi_ner\": [\n",
      "        \"E-QUANTITY\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 68,\n",
      "      \"text\": \"brk\",\n",
      "      \"lemma\": \"brk\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 72,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 343,\n",
      "      \"end_char\": 346,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 69,\n",
      "      \"text\": \"2\",\n",
      "      \"lemma\": \"2\",\n",
      "      \"upos\": \"NUM\",\n",
      "      \"xpos\": \"CD\",\n",
      "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
      "      \"head\": 70,\n",
      "      \"deprel\": \"nummod\",\n",
      "      \"start_char\": 347,\n",
      "      \"end_char\": 348,\n",
      "      \"ner\": \"S-CARDINAL\",\n",
      "      \"multi_ner\": [\n",
      "        \"S-CARDINAL\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 70,\n",
      "      \"text\": \"cups\",\n",
      "      \"lemma\": \"cup\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NNS\",\n",
      "      \"feats\": \"Number=Plur\",\n",
      "      \"head\": 71,\n",
      "      \"deprel\": \"nmod:npmod\",\n",
      "      \"start_char\": 349,\n",
      "      \"end_char\": 353,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 71,\n",
      "      \"text\": \"buttermilk\",\n",
      "      \"lemma\": \"buttermilk\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 72,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 354,\n",
      "      \"end_char\": 364,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 72,\n",
      "      \"text\": \"brk\",\n",
      "      \"lemma\": \"brk\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 365,\n",
      "      \"end_char\": 368,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 73,\n",
      "      \"text\": \"2\",\n",
      "      \"lemma\": \"2\",\n",
      "      \"upos\": \"NUM\",\n",
      "      \"xpos\": \"CD\",\n",
      "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
      "      \"head\": 74,\n",
      "      \"deprel\": \"nummod\",\n",
      "      \"start_char\": 369,\n",
      "      \"end_char\": 370,\n",
      "      \"ner\": \"S-CARDINAL\",\n",
      "      \"multi_ner\": [\n",
      "        \"S-CARDINAL\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 74,\n",
      "      \"text\": \"cups\",\n",
      "      \"lemma\": \"cup\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NNS\",\n",
      "      \"feats\": \"Number=Plur\",\n",
      "      \"head\": 72,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 371,\n",
      "      \"end_char\": 375,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 75,\n",
      "      \"text\": \"all\",\n",
      "      \"lemma\": \"all\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"head\": 77,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 376,\n",
      "      \"end_char\": 379,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 76,\n",
      "      \"text\": \"-\",\n",
      "      \"lemma\": \"-\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \"HYPH\",\n",
      "      \"head\": 75,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 379,\n",
      "      \"end_char\": 380,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 77,\n",
      "      \"text\": \"purpose\",\n",
      "      \"lemma\": \"purpose\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 78,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 380,\n",
      "      \"end_char\": 387,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 78,\n",
      "      \"text\": \"flour\",\n",
      "      \"lemma\": \"flour\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 388,\n",
      "      \"end_char\": 393,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 79,\n",
      "      \"text\": \"brk\",\n",
      "      \"lemma\": \"brk\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 81,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 394,\n",
      "      \"end_char\": 397,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 80,\n",
      "      \"text\": \"kosher\",\n",
      "      \"lemma\": \"kosher\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 81,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 398,\n",
      "      \"end_char\": 404,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 81,\n",
      "      \"text\": \"salt\",\n",
      "      \"lemma\": \"salt\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 83,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 405,\n",
      "      \"end_char\": 409,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 82,\n",
      "      \"text\": \"brk\",\n",
      "      \"lemma\": \"brk\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 83,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 410,\n",
      "      \"end_char\": 413,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 83,\n",
      "      \"text\": \"honey\",\n",
      "      \"lemma\": \"honey\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 78,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 414,\n",
      "      \"end_char\": 419,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 84,\n",
      "      \"text\": \",\",\n",
      "      \"lemma\": \",\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \",\",\n",
      "      \"head\": 87,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 419,\n",
      "      \"end_char\": 420,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 85,\n",
      "      \"text\": \"flaky\",\n",
      "      \"lemma\": \"flaky\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 87,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 421,\n",
      "      \"end_char\": 426,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 86,\n",
      "      \"text\": \"sea\",\n",
      "      \"lemma\": \"sea\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 87,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 427,\n",
      "      \"end_char\": 430,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 87,\n",
      "      \"text\": \"salt\",\n",
      "      \"lemma\": \"salt\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 78,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 431,\n",
      "      \"end_char\": 435,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 88,\n",
      "      \"text\": \"(\",\n",
      "      \"lemma\": \"(\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \"-LRB-\",\n",
      "      \"head\": 91,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 436,\n",
      "      \"end_char\": 437,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 89,\n",
      "      \"text\": \"such\",\n",
      "      \"lemma\": \"such\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 91,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 437,\n",
      "      \"end_char\": 441,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 90,\n",
      "      \"text\": \"as\",\n",
      "      \"lemma\": \"as\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"IN\",\n",
      "      \"head\": 89,\n",
      "      \"deprel\": \"fixed\",\n",
      "      \"start_char\": 442,\n",
      "      \"end_char\": 444,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 91,\n",
      "      \"text\": \"maldon\",\n",
      "      \"lemma\": \"maldon\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 87,\n",
      "      \"deprel\": \"nmod\",\n",
      "      \"start_char\": 445,\n",
      "      \"end_char\": 451,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 92,\n",
      "      \"text\": \")\",\n",
      "      \"lemma\": \")\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \"-RRB-\",\n",
      "      \"head\": 91,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 451,\n",
      "      \"end_char\": 452,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 93,\n",
      "      \"text\": \",\",\n",
      "      \"lemma\": \",\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \",\",\n",
      "      \"head\": 95,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 452,\n",
      "      \"end_char\": 453,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 94,\n",
      "      \"text\": \"toasted\",\n",
      "      \"lemma\": \"toast\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBN\",\n",
      "      \"feats\": \"Tense=Past|VerbForm=Part\",\n",
      "      \"head\": 95,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 454,\n",
      "      \"end_char\": 461,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 95,\n",
      "      \"text\": \"benne\",\n",
      "      \"lemma\": \"benne\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 11,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 462,\n",
      "      \"end_char\": 467,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 96,\n",
      "      \"text\": \"or\",\n",
      "      \"lemma\": \"or\",\n",
      "      \"upos\": \"CCONJ\",\n",
      "      \"xpos\": \"CC\",\n",
      "      \"head\": 98,\n",
      "      \"deprel\": \"cc\",\n",
      "      \"start_char\": 468,\n",
      "      \"end_char\": 470,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 97,\n",
      "      \"text\": \"sesame\",\n",
      "      \"lemma\": \"sesame\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 98,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 471,\n",
      "      \"end_char\": 477,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 98,\n",
      "      \"text\": \"seeds\",\n",
      "      \"lemma\": \"seed\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NNS\",\n",
      "      \"feats\": \"Number=Plur\",\n",
      "      \"head\": 95,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 478,\n",
      "      \"end_char\": 483,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 99,\n",
      "      \"text\": \",\",\n",
      "      \"lemma\": \",\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \",\",\n",
      "      \"head\": 23,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 483,\n",
      "      \"end_char\": 484,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 100,\n",
      "      \"text\": \"hot\",\n",
      "      \"lemma\": \"hot\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 101,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 485,\n",
      "      \"end_char\": 488,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 101,\n",
      "      \"text\": \"sauce\",\n",
      "      \"lemma\": \"sauce\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"conj\",\n",
      "      \"start_char\": 489,\n",
      "      \"end_char\": 494,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 102,\n",
      "      \"text\": \"(\",\n",
      "      \"lemma\": \"(\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \"-LRB-\",\n",
      "      \"head\": 104,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 495,\n",
      "      \"end_char\": 496,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 103,\n",
      "      \"text\": \"for\",\n",
      "      \"lemma\": \"for\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"IN\",\n",
      "      \"head\": 104,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 496,\n",
      "      \"end_char\": 499,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 104,\n",
      "      \"text\": \"serving\",\n",
      "      \"lemma\": \"serving\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 106,\n",
      "      \"deprel\": \"obl\",\n",
      "      \"start_char\": 500,\n",
      "      \"end_char\": 507,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 105,\n",
      "      \"text\": \")\",\n",
      "      \"lemma\": \")\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \"-RRB-\",\n",
      "      \"head\": 104,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 507,\n",
      "      \"end_char\": 508,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 106,\n",
      "      \"text\": \"brk\",\n",
      "      \"lemma\": \"brk\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VB\",\n",
      "      \"feats\": \"VerbForm=Fin\",\n",
      "      \"head\": 11,\n",
      "      \"deprel\": \"parataxis\",\n",
      "      \"start_char\": 509,\n",
      "      \"end_char\": 512,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 107,\n",
      "      \"text\": \"a\",\n",
      "      \"lemma\": \"a\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Definite=Ind|PronType=Art\",\n",
      "      \"head\": 111,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 513,\n",
      "      \"end_char\": 514,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 108,\n",
      "      \"text\": \"deep\",\n",
      "      \"lemma\": \"deep\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 110,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 515,\n",
      "      \"end_char\": 519,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 109,\n",
      "      \"text\": \"-\",\n",
      "      \"lemma\": \"-\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \"HYPH\",\n",
      "      \"head\": 108,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 519,\n",
      "      \"end_char\": 520,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 110,\n",
      "      \"text\": \"fry\",\n",
      "      \"lemma\": \"fry\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 111,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 520,\n",
      "      \"end_char\": 523,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 111,\n",
      "      \"text\": \"thermometer\",\n",
      "      \"lemma\": \"thermometer\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 106,\n",
      "      \"deprel\": \"obj\",\n",
      "      \"start_char\": 524,\n",
      "      \"end_char\": 535,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "test_recipe_pipeline = full_df['ingredients'][0]\n",
    "\n",
    "test_recipe_preproc = stanza_preprocessor(nlp, test_recipe_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tablespoon yellow mustard seed brk tablespoon brown mustard seed brk teaspoon coriander seed brk cup apple cider vinegar brk cup kosher salt brk cup sugar brk cup chop fresh dill brk skinless , boneless chicken thigh ( pound ) , halve , quarter large brk vegetable oil ( frying ; cup ) brk cup buttermilk brk cup - purpose flour brk kosher salt brk honey , flaky sea salt ( such maldon ) , toast benne sesame seed , hot sauce ( serving ) brk deep - fry thermometer'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_recipe_lemma = stanza_lemmatizer(test_recipe_preproc)\n",
    "test_recipe_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yellow mustard seed'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ln in test_recipe_lemma.split('brk'):\n",
    "\n",
    "    # tokenize the input string (customize the regex as desired)\n",
    "    terms = re.findall(u'(?u)\\\\b\\\\w+\\\\b', ln)\n",
    "\n",
    "    # loop ngram creation for every number between min and max ngram length\n",
    "    for ngramLength in range(1, 5):\n",
    "\n",
    "        # find and return all ngrams\n",
    "        for ngram in zip(*[terms[i:] for i in range(3)]): #<-- solution without a generator (works the same but has higher memory usage)\n",
    "        # for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]): # <-- solution using a generator\n",
    "            ngram = ' '.join(ngram)\n",
    "            # yield ngram\n",
    "ngram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tablespoon yellow mustard seed brk tablespoon brown mustard seed brk teaspoon coriander seed brk cup apple cider vinegar brk cup kosher salt brk cup sugar brk cup chop fresh dill brk skinless , boneless chicken thigh ( pound ) , halve , quarter large brk vegetable oil ( frying ; cup ) brk cup buttermilk brk cup - purpose flour brk kosher salt brk honey , flaky sea salt ( such maldon ) , toast benne sesame seed , hot sauce ( serving ) brk deep - fry thermometer'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_recipe_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tablespoon yellow mustard seed',\n",
       " 'tablespoon brown mustard seed',\n",
       " 'teaspoon coriander seed',\n",
       " 'cup apple cider vinegar',\n",
       " 'cup kosher salt',\n",
       " 'cup sugar',\n",
       " 'cup chop fresh dill',\n",
       " 'skinless , boneless chicken thigh ( pound ) , halve , quarter large',\n",
       " 'vegetable oil ( frying ; cup )',\n",
       " 'cup buttermilk',\n",
       " 'cup - purpose flour',\n",
       " 'kosher salt',\n",
       " 'honey , flaky sea salt ( such maldon ) , toast benne sesame seed , hot sauce ( serving )',\n",
       " 'deep - fry thermometer']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_recipe_lemma.split(' brk ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.gen_analyzer_tester.<locals>.ngrams_per_line(doc=['tablespoon yellow mustard seed brk tablespoon brown mustard seed brk teaspoon coriander seed brk cup apple cider vinegar brk cup kosher salt brk cup sugar brk cup chop fresh dill brk skinless , boneless chicken thigh ( pound ) , halve , quarter large brk vegetable oil ( frying ; cup ) brk cup buttermilk brk cup - purpose flour brk kosher salt brk honey , flaky sea salt ( such maldon ) , toast benne sesame seed , hot sauce ( serving ) brk deep - fry thermometer'])>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_analyzer_tester(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:531: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\n",
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:560: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['1', '1 1', '1 1 2', '1 1 2 cups', '1 2', '1 2 cup', '1 2 cups',\n",
       "       '1 2 teaspoon', '1 3', '1 4', '1 4 cup', '1 4 teaspoon', '1 cup',\n",
       "       '1 large', '1 tablespoon', '1 teaspoon', '2', '2 cup', '2 cups',\n",
       "       '2 inch', '2 tablespoons', '2 teaspoon', '2 teaspoons', '3', '3 4',\n",
       "       '3 4 cup', '3 cups', '4', '4 cup', '4 teaspoon', '5', '6', '8',\n",
       "       'about', 'all', 'all purpose', 'all purpose flour', 'and', 'black',\n",
       "       'butter', 'chopped', 'chopped fresh', 'coarse', 'coarse kosher',\n",
       "       'coarse kosher salt', 'cream', 'cup', 'cups', 'cut', 'cut into',\n",
       "       'cut into 1', 'divided', 'dried', 'egg', 'extra', 'extra virgin',\n",
       "       'extra virgin olive', 'extra virgin olive oil', 'finely', 'flour',\n",
       "       'fresh', 'garlic', 'ground', 'halved', 'inch', 'into', 'into 1',\n",
       "       'juice', 'kosher', 'kosher salt', 'large', 'lemon', 'of', 'oil',\n",
       "       'olive', 'olive oil', 'onion', 'or', 'ounce', 'ounces', 'packed',\n",
       "       'peeled', 'pound', 'pounds', 'purpose', 'purpose flour', 'red',\n",
       "       'salt', 'salt 1', 'sliced', 'slices', 'sugar', 'sugar 1',\n",
       "       'tablespoon', 'tablespoons', 'teaspoon', 'teaspoon salt',\n",
       "       'teaspoons', 'to', 'tomatoes', 'unsalted', 'unsalted butter',\n",
       "       'vanilla', 'virgin', 'virgin olive', 'virgin olive oil', 'water',\n",
       "       'whole'], dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = full_df[\"ingredients\"][0:50].apply(\"|\".join)\n",
    "\n",
    "\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'preprocessor':stanza_preprocessor,\n",
    "    'tokenizer':stanza_lemmatizer, # out of memory \n",
    "    # 'stop_words':flushtrated_list,\n",
    "    'analyzer': gen_analyzer(1, 4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "vectorizer_model = CountVectorizer(**cv_params)\n",
    "\n",
    "test_transform = vectorizer_model.fit_transform(temp)#:5])\n",
    "vectorizer_model.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the warnings (preprocessor not used since analyzer is callable and tokenizer not used since analyzer is not word), may need to move preprocessing into analyzer. And based on [this](https://stackoverflow.com/questions/63185843/scikit-learn-countvectorizer-customizing-preprocessor-tokenizer-and-analyzer), will need to incorporate stopwords into the analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom ngram analyzer function, matching only ngrams that belong to the same line\n",
    "def gen_analyzer_2(stanza_pipeline, minNgramLength, maxNgramLength):\n",
    "    def ngrams_per_line(ingredients_list):\n",
    "\n",
    "        lowered = \" brk \".join(ingredients_list).lower()\n",
    "        preproc = stanza_pipeline(lowered)\n",
    "        \n",
    "        lemmad = \" \".join([word.lemma \n",
    "                      for sent in preproc.sentences \n",
    "                      for word in sent.words if (\n",
    "                          word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\"]\n",
    "                        #   and word not in STOP_WORDS\n",
    "                          )\n",
    "                    ])\n",
    "\n",
    "        # analyze each line of the input string seperately\n",
    "        for ln in lemmad.split(' brk '):\n",
    "\n",
    "            # tokenize the input string (customize the regex as desired)\n",
    "            # terms = re.findall(u'(?u)\\\\b\\\\w+\\\\b', ln)\n",
    "            # terms = re.findall(\"(?u)\\b[a-zA-Z]{2,}\\b\", ln)\n",
    "\n",
    "            # loop ngram creation for every number between min and max ngram length\n",
    "            for ngramLength in range(minNgramLength, maxNgramLength+1):\n",
    "\n",
    "                # find and return all ngrams\n",
    "                # for ngram in zip(*[terms[i:] for i in range(3)]): <-- solution without a generator (works the same but has higher memory usage)\n",
    "                # for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]): # <-- solution using a generator\n",
    "                for ngram in zip(*[islice(seq, i, len(ln)) for i, seq in enumerate(tee(ln, ngramLength))]): # <-- solution using a generator\n",
    "                    ngram = ' '.join(ngram)\n",
    "                    yield ngram\n",
    "                    \n",
    "    return ngrams_per_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 37: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb Cell 36\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m vectorizer_model \u001b[39m=\u001b[39m CountVectorizer(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcv_params)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# test_transform = vectorizer_model.fit_transform(tqdm(temp))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m test_transform \u001b[39m=\u001b[39m vectorizer_model\u001b[39m.\u001b[39;49mfit_transform(temp)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m vectorizer_model\u001b[39m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1277\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "\u001b[1;32m/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb Cell 36\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m lowered \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m brk \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(ingredients_list)\u001b[39m.\u001b[39mlower()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m preproc \u001b[39m=\u001b[39m stanza_pipeline(lowered)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m lemmad \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin([word\u001b[39m.\u001b[39;49mlemma \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m               \u001b[39mfor\u001b[39;49;00m sent \u001b[39min\u001b[39;49;00m preproc\u001b[39m.\u001b[39;49msentences \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m               \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m sent\u001b[39m.\u001b[39;49mwords \u001b[39mif\u001b[39;49;00m (\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m                   word\u001b[39m.\u001b[39;49mupos \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m [\u001b[39m\"\u001b[39;49m\u001b[39mNUM\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mDET\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mADV\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mCCONJ\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mADP\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mSCONJ\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m                 \u001b[39m#   and word not in STOP_WORDS\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m                   )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m             ])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# analyze each line of the input string seperately\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m ln \u001b[39min\u001b[39;00m lemmad\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m brk \u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# tokenize the input string (customize the regex as desired)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# loop ngram creation for every number between min and max ngram length\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 37: expected str instance, NoneType found"
     ]
    }
   ],
   "source": [
    "temp = full_df[\"ingredients\"][0:500]#.apply(\"|\".join)\n",
    "\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': gen_analyzer_2(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "vectorizer_model = CountVectorizer(**cv_params)\n",
    "\n",
    "# test_transform = vectorizer_model.fit_transform(tqdm(temp))\n",
    "test_transform = vectorizer_model.fit_transform(temp)\n",
    "vectorizer_model.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dek</th>\n",
       "      <th>hed</th>\n",
       "      <th>aggregateRating</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>prepSteps</th>\n",
       "      <th>reviewsCount</th>\n",
       "      <th>willMakeAgainPct</th>\n",
       "      <th>cuisine_name</th>\n",
       "      <th>photo_filename</th>\n",
       "      <th>...</th>\n",
       "      <th>zest pith</th>\n",
       "      <th>zest vegetable</th>\n",
       "      <th>zinfandel</th>\n",
       "      <th>ziti</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zucchini blossom</th>\n",
       "      <th>zucchini crookneck</th>\n",
       "      <th>zucchini squash</th>\n",
       "      <th>árbol</th>\n",
       "      <th>árbol pepper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 3365 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, dek, hed, aggregateRating, ingredients, prepSteps, reviewsCount, willMakeAgainPct, cuisine_name, photo_filename, photo_credit, author_name, date_published, recipe_url, achiote, acid, addition, adobo, adobo adobo, adobo adobo sauce, adobo sauce, adobo sauce chipotle, african, agave, agave nectar, agave syrup, agave syrup nectar, ahi, ahi tuna, aioli, aji, ají, albacore, albacore tuna, ale, aleppo, aleppo pepper, aleppo pepper pepper, alfalfa, alfalfa sprout, allspice, allspice berry, almond, almond almond, almond butter, almond extract, almond flour, almond flour almond, almond hazelnut, almond liqueur, almond macaroon, almond milk, almond oil, almond paste, almond paste marzipan, almond pistachio, almond walnut, amaranth, amaretti, amaretti cookie, amaretti cookie macaroon, amaretto, amaretto almond, amaretto almond liqueur, amaretto liqueur, amarillo, amber, amber rum, amontillado, anaheim, anaheim chile, anaheim chilie, ancho, ancho chile, ancho chile seed, ancho chili, ancho chilie, anchovy, anchovy fillet, anchovy fillet oil, anchovy oil, anchovy paste, andouille, andouille sausage, anglaise, angostura, angostura bitter, anise, anise bulb, anise flush, anise flush bulb, anise flush bulb bulb, anise frond, anise liqueur, anise seed, aniseed, anisette, anjou, anjou bartlett, anjou bosc, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 3365 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# was getting an error (TypeError: sequence item 37: expected str instance, NoneType found) saying that some recipes have NoneType, and wondered if there were recipes with no ingredients for some reason\n",
    "full_df[full_df[\"ingredients\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1 tablespoons yellow mustard seeds brk 1 table...\n",
       "1    3 pounds small-leaved bulk spinach brk salt br...\n",
       "2    3 1/2 cups all-purpose flour brk 1 tablespoon ...\n",
       "3    1 small ripe avocado, preferably hass (see not...\n",
       "4    2 pounds fresh tomatoes, unpeeled and cut in q...\n",
       "Name: ingredients, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = full_df['ingredients'][0:5].apply(\" brk \".join).str.lower()\n",
    "subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m test_preproc \u001b[39m=\u001b[39m full_df[\u001b[39m'\u001b[39;49m\u001b[39mingredients\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m:\u001b[39m5\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(stanza_preprocessor, args\u001b[39m=\u001b[39;49m(nlp,))\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m test_preproc\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1175\u001b[0m             values,\n\u001b[1;32m   1176\u001b[0m             f,\n\u001b[1;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[1;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/apply.py:142\u001b[0m, in \u001b[0;36mApply.__init__.<locals>.f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(x):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m func(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb Cell 28\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstanza_preprocessor\u001b[39m(stanza_pipeline, ingredients_list):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# This function takes in a Stanza pipeline and a recipe's ingredients in list form and returns a Stanza transformed document to be used in the lemmatizer\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m# lowered = \" brk \".join(ingredients_list).lower()\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     lowered \u001b[39m=\u001b[39m ingredients_list\u001b[39m.\u001b[39;49mapply(\u001b[39m\"\u001b[39m\u001b[39m brk \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin)\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mlower()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m stanza_pipeline(lowered)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "test_preproc = full_df['ingredients'][0:5].apply(stanza_preprocessor, args=(nlp,))\n",
    "test_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"1\",\n",
       "      \"lemma\": \"1\",\n",
       "      \"upos\": \"NUM\",\n",
       "      \"xpos\": \"CD\",\n",
       "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"nummod\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 1,\n",
       "      \"ner\": \"S-CARDINAL\",\n",
       "      \"multi_ner\": [\n",
       "        \"S-CARDINAL\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 2,\n",
       "      \"text\": \"tablespoons\",\n",
       "      \"lemma\": \"tablespoon\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NNS\",\n",
       "      \"feats\": \"Number=Plur\",\n",
       "      \"head\": 3,\n",
       "      \"deprel\": \"obl:npmod\",\n",
       "      \"start_char\": 2,\n",
       "      \"end_char\": 13,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 3,\n",
       "      \"text\": \"yellow\",\n",
       "      \"lemma\": \"yellow\",\n",
       "      \"upos\": \"ADJ\",\n",
       "      \"xpos\": \"JJ\",\n",
       "      \"feats\": \"Degree=Pos\",\n",
       "      \"head\": 5,\n",
       "      \"deprel\": \"amod\",\n",
       "      \"start_char\": 14,\n",
       "      \"end_char\": 20,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 4,\n",
       "      \"text\": \"mustard\",\n",
       "      \"lemma\": \"mustard\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 5,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 21,\n",
       "      \"end_char\": 28,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 5,\n",
       "      \"text\": \"seeds\",\n",
       "      \"lemma\": \"seed\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NNS\",\n",
       "      \"feats\": \"Number=Plur\",\n",
       "      \"head\": 0,\n",
       "      \"deprel\": \"root\",\n",
       "      \"start_char\": 29,\n",
       "      \"end_char\": 34,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 6,\n",
       "      \"text\": \"brk\",\n",
       "      \"lemma\": \"brk\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 8,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 35,\n",
       "      \"end_char\": 38,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 7,\n",
       "      \"text\": \"1\",\n",
       "      \"lemma\": \"1\",\n",
       "      \"upos\": \"NUM\",\n",
       "      \"xpos\": \"CD\",\n",
       "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
       "      \"head\": 8,\n",
       "      \"deprel\": \"nummod\",\n",
       "      \"start_char\": 39,\n",
       "      \"end_char\": 40,\n",
       "      \"ner\": \"S-CARDINAL\",\n",
       "      \"multi_ner\": [\n",
       "        \"S-CARDINAL\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 8,\n",
       "      \"text\": \"tablespoons\",\n",
       "      \"lemma\": \"tablespoon\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NNS\",\n",
       "      \"feats\": \"Number=Plur\",\n",
       "      \"head\": 11,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 41,\n",
       "      \"end_char\": 52,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 9,\n",
       "      \"text\": \"brown\",\n",
       "      \"lemma\": \"brown\",\n",
       "      \"upos\": \"ADJ\",\n",
       "      \"xpos\": \"JJ\",\n",
       "      \"feats\": \"Degree=Pos\",\n",
       "      \"head\": 11,\n",
       "      \"deprel\": \"amod\",\n",
       "      \"start_char\": 53,\n",
       "      \"end_char\": 58,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 10,\n",
       "      \"text\": \"mustard\",\n",
       "      \"lemma\": \"mustard\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 11,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 59,\n",
       "      \"end_char\": 66,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 11,\n",
       "      \"text\": \"seeds\",\n",
       "      \"lemma\": \"seed\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NNS\",\n",
       "      \"feats\": \"Number=Plur\",\n",
       "      \"head\": 5,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 67,\n",
       "      \"end_char\": 72,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 12,\n",
       "      \"text\": \"brk\",\n",
       "      \"lemma\": \"brk\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 17,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 73,\n",
       "      \"end_char\": 76,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 13,\n",
       "      \"text\": \"1\",\n",
       "      \"lemma\": \"1\",\n",
       "      \"upos\": \"NUM\",\n",
       "      \"xpos\": \"CD\",\n",
       "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
       "      \"head\": 15,\n",
       "      \"deprel\": \"nummod\",\n",
       "      \"start_char\": 77,\n",
       "      \"end_char\": 78,\n",
       "      \"ner\": \"B-QUANTITY\",\n",
       "      \"multi_ner\": [\n",
       "        \"B-QUANTITY\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 14,\n",
       "      \"text\": \"1/2\",\n",
       "      \"lemma\": \"1/2\",\n",
       "      \"upos\": \"NUM\",\n",
       "      \"xpos\": \"CD\",\n",
       "      \"feats\": \"NumForm=Word|NumType=Card\",\n",
       "      \"head\": 15,\n",
       "      \"deprel\": \"nummod\",\n",
       "      \"start_char\": 79,\n",
       "      \"end_char\": 82,\n",
       "      \"ner\": \"I-QUANTITY\",\n",
       "      \"multi_ner\": [\n",
       "        \"I-QUANTITY\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 15,\n",
       "      \"text\": \"teaspoons\",\n",
       "      \"lemma\": \"teaspoon\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NNS\",\n",
       "      \"feats\": \"Number=Plur\",\n",
       "      \"head\": 17,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 83,\n",
       "      \"end_char\": 92,\n",
       "      \"ner\": \"E-QUANTITY\",\n",
       "      \"multi_ner\": [\n",
       "        \"E-QUANTITY\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 16,\n",
       "      \"text\": \"coriander\",\n",
       "      \"lemma\": \"coriander\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 17,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 93,\n",
       "      \"end_char\": 102,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 17,\n",
       "      \"text\": \"seeds\",\n",
       "      \"lemma\": \"seed\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NNS\",\n",
       "      \"feats\": \"Number=Plur\",\n",
       "      \"head\": 11,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 103,\n",
       "      \"end_char\": 108,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 18,\n",
       "      \"text\": \"brk\",\n",
       "      \"lemma\": \"brk\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 11,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 109,\n",
       "      \"end_char\": 112,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 19,\n",
       "      \"text\": \"1\",\n",
       "      \"lemma\": \"1\",\n",
       "      \"upos\": \"NUM\",\n",
       "      \"xpos\": \"CD\",\n",
       "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
       "      \"head\": 20,\n",
       "      \"deprel\": \"nummod\",\n",
       "      \"start_char\": 113,\n",
       "      \"end_char\": 114,\n",
       "      \"ner\": \"S-CARDINAL\",\n",
       "      \"multi_ner\": [\n",
       "        \"S-CARDINAL\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 20,\n",
       "      \"text\": \"cup\",\n",
       "      \"lemma\": \"cup\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 17,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 115,\n",
       "      \"end_char\": 118,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 21,\n",
       "      \"text\": \"apple\",\n",
       "      \"lemma\": \"apple\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 22,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 119,\n",
       "      \"end_char\": 124,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 22,\n",
       "      \"text\": \"cider\",\n",
       "      \"lemma\": \"cider\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 23,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 125,\n",
       "      \"end_char\": 130,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 23,\n",
       "      \"text\": \"vinegar\",\n",
       "      \"lemma\": \"vinegar\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 17,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 131,\n",
       "      \"end_char\": 138,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 24,\n",
       "      \"text\": \"brk\",\n",
       "      \"lemma\": \"brk\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 28,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 139,\n",
       "      \"end_char\": 142,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 25,\n",
       "      \"text\": \"2/3\",\n",
       "      \"lemma\": \"2/3\",\n",
       "      \"upos\": \"NUM\",\n",
       "      \"xpos\": \"CD\",\n",
       "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
       "      \"head\": 26,\n",
       "      \"deprel\": \"nummod\",\n",
       "      \"start_char\": 143,\n",
       "      \"end_char\": 146,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 26,\n",
       "      \"text\": \"cup\",\n",
       "      \"lemma\": \"cup\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 28,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 147,\n",
       "      \"end_char\": 150,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 27,\n",
       "      \"text\": \"kosher\",\n",
       "      \"lemma\": \"kosher\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 28,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 151,\n",
       "      \"end_char\": 157,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 28,\n",
       "      \"text\": \"salt\",\n",
       "      \"lemma\": \"salt\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 32,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 158,\n",
       "      \"end_char\": 162,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 29,\n",
       "      \"text\": \"brk\",\n",
       "      \"lemma\": \"brk\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 32,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 163,\n",
       "      \"end_char\": 166,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 30,\n",
       "      \"text\": \"1/3\",\n",
       "      \"lemma\": \"1/3\",\n",
       "      \"upos\": \"NUM\",\n",
       "      \"xpos\": \"CD\",\n",
       "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
       "      \"head\": 31,\n",
       "      \"deprel\": \"nummod\",\n",
       "      \"start_char\": 167,\n",
       "      \"end_char\": 170,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 31,\n",
       "      \"text\": \"cup\",\n",
       "      \"lemma\": \"cup\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 32,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 171,\n",
       "      \"end_char\": 174,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 32,\n",
       "      \"text\": \"sugar\",\n",
       "      \"lemma\": \"sugar\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 23,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 175,\n",
       "      \"end_char\": 180,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 33,\n",
       "      \"text\": \"brk\",\n",
       "      \"lemma\": \"brk\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 35,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 181,\n",
       "      \"end_char\": 184,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 34,\n",
       "      \"text\": \"1/4\",\n",
       "      \"lemma\": \"1/4\",\n",
       "      \"upos\": \"NUM\",\n",
       "      \"xpos\": \"CD\",\n",
       "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
       "      \"head\": 35,\n",
       "      \"deprel\": \"nummod\",\n",
       "      \"start_char\": 185,\n",
       "      \"end_char\": 188,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 35,\n",
       "      \"text\": \"cup\",\n",
       "      \"lemma\": \"cup\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 39,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 189,\n",
       "      \"end_char\": 192,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 36,\n",
       "      \"text\": \"chopped\",\n",
       "      \"lemma\": \"chop\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VBN\",\n",
       "      \"feats\": \"Tense=Past|VerbForm=Part\",\n",
       "      \"head\": 38,\n",
       "      \"deprel\": \"amod\",\n",
       "      \"start_char\": 193,\n",
       "      \"end_char\": 200,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 37,\n",
       "      \"text\": \"fresh\",\n",
       "      \"lemma\": \"fresh\",\n",
       "      \"upos\": \"ADJ\",\n",
       "      \"xpos\": \"JJ\",\n",
       "      \"feats\": \"Degree=Pos\",\n",
       "      \"head\": 38,\n",
       "      \"deprel\": \"amod\",\n",
       "      \"start_char\": 201,\n",
       "      \"end_char\": 206,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 38,\n",
       "      \"text\": \"dill\",\n",
       "      \"lemma\": \"dill\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 39,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 207,\n",
       "      \"end_char\": 211,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 39,\n",
       "      \"text\": \"brk\",\n",
       "      \"lemma\": \"brk\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 32,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 212,\n",
       "      \"end_char\": 215,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 40,\n",
       "      \"text\": \"8\",\n",
       "      \"lemma\": \"8\",\n",
       "      \"upos\": \"NUM\",\n",
       "      \"xpos\": \"CD\",\n",
       "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
       "      \"head\": 41,\n",
       "      \"deprel\": \"nummod\",\n",
       "      \"start_char\": 216,\n",
       "      \"end_char\": 217,\n",
       "      \"ner\": \"S-CARDINAL\",\n",
       "      \"multi_ner\": [\n",
       "        \"S-CARDINAL\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 41,\n",
       "      \"text\": \"skinless\",\n",
       "      \"lemma\": \"skinless\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 39,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 218,\n",
       "      \"end_char\": 226,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 42,\n",
       "      \"text\": \",\",\n",
       "      \"lemma\": \",\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \",\",\n",
       "      \"head\": 45,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 226,\n",
       "      \"end_char\": 227,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 43,\n",
       "      \"text\": \"boneless\",\n",
       "      \"lemma\": \"boneless\",\n",
       "      \"upos\": \"ADJ\",\n",
       "      \"xpos\": \"JJ\",\n",
       "      \"feats\": \"Degree=Pos\",\n",
       "      \"head\": 45,\n",
       "      \"deprel\": \"amod\",\n",
       "      \"start_char\": 228,\n",
       "      \"end_char\": 236,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 44,\n",
       "      \"text\": \"chicken\",\n",
       "      \"lemma\": \"chicken\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 45,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 237,\n",
       "      \"end_char\": 244,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 45,\n",
       "      \"text\": \"thighs\",\n",
       "      \"lemma\": \"thigh\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NNS\",\n",
       "      \"feats\": \"Number=Plur\",\n",
       "      \"head\": 39,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 245,\n",
       "      \"end_char\": 251,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 46,\n",
       "      \"text\": \"(\",\n",
       "      \"lemma\": \"(\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \"-LRB-\",\n",
       "      \"head\": 49,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 252,\n",
       "      \"end_char\": 253,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 47,\n",
       "      \"text\": \"about\",\n",
       "      \"lemma\": \"about\",\n",
       "      \"upos\": \"ADV\",\n",
       "      \"xpos\": \"RB\",\n",
       "      \"head\": 48,\n",
       "      \"deprel\": \"advmod\",\n",
       "      \"start_char\": 253,\n",
       "      \"end_char\": 258,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 48,\n",
       "      \"text\": \"3\",\n",
       "      \"lemma\": \"3\",\n",
       "      \"upos\": \"NUM\",\n",
       "      \"xpos\": \"CD\",\n",
       "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
       "      \"head\": 49,\n",
       "      \"deprel\": \"nummod\",\n",
       "      \"start_char\": 259,\n",
       "      \"end_char\": 260,\n",
       "      \"ner\": \"B-QUANTITY\",\n",
       "      \"multi_ner\": [\n",
       "        \"B-QUANTITY\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 49,\n",
       "      \"text\": \"pounds\",\n",
       "      \"lemma\": \"pound\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NNS\",\n",
       "      \"feats\": \"Number=Plur\",\n",
       "      \"head\": 45,\n",
       "      \"deprel\": \"parataxis\",\n",
       "      \"start_char\": 261,\n",
       "      \"end_char\": 267,\n",
       "      \"ner\": \"E-QUANTITY\",\n",
       "      \"multi_ner\": [\n",
       "        \"E-QUANTITY\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 50,\n",
       "      \"text\": \")\",\n",
       "      \"lemma\": \")\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \"-RRB-\",\n",
       "      \"head\": 49,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 267,\n",
       "      \"end_char\": 268,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 51,\n",
       "      \"text\": \",\",\n",
       "      \"lemma\": \",\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \",\",\n",
       "      \"head\": 52,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 268,\n",
       "      \"end_char\": 269,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 52,\n",
       "      \"text\": \"halved\",\n",
       "      \"lemma\": \"halve\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VBN\",\n",
       "      \"feats\": \"Tense=Past|VerbForm=Part\",\n",
       "      \"head\": 54,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 270,\n",
       "      \"end_char\": 276,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 53,\n",
       "      \"text\": \",\",\n",
       "      \"lemma\": \",\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \",\",\n",
       "      \"head\": 54,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 276,\n",
       "      \"end_char\": 277,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 54,\n",
       "      \"text\": \"quartered\",\n",
       "      \"lemma\": \"quarter\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VBN\",\n",
       "      \"feats\": \"Tense=Past|VerbForm=Part\",\n",
       "      \"head\": 39,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 278,\n",
       "      \"end_char\": 287,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 55,\n",
       "      \"text\": \"if\",\n",
       "      \"lemma\": \"if\",\n",
       "      \"upos\": \"SCONJ\",\n",
       "      \"xpos\": \"IN\",\n",
       "      \"head\": 59,\n",
       "      \"deprel\": \"mark\",\n",
       "      \"start_char\": 288,\n",
       "      \"end_char\": 290,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 56,\n",
       "      \"text\": \"large\",\n",
       "      \"lemma\": \"large\",\n",
       "      \"upos\": \"ADJ\",\n",
       "      \"xpos\": \"JJ\",\n",
       "      \"feats\": \"Degree=Pos\",\n",
       "      \"head\": 59,\n",
       "      \"deprel\": \"amod\",\n",
       "      \"start_char\": 291,\n",
       "      \"end_char\": 296,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 57,\n",
       "      \"text\": \"brk\",\n",
       "      \"lemma\": \"brk\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 59,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 297,\n",
       "      \"end_char\": 300,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 58,\n",
       "      \"text\": \"vegetable\",\n",
       "      \"lemma\": \"vegetable\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 59,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 301,\n",
       "      \"end_char\": 310,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 59,\n",
       "      \"text\": \"oil\",\n",
       "      \"lemma\": \"oil\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 54,\n",
       "      \"deprel\": \"advcl\",\n",
       "      \"start_char\": 311,\n",
       "      \"end_char\": 314,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 60,\n",
       "      \"text\": \"(\",\n",
       "      \"lemma\": \"(\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \"-LRB-\",\n",
       "      \"head\": 66,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 315,\n",
       "      \"end_char\": 316,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 61,\n",
       "      \"text\": \"for\",\n",
       "      \"lemma\": \"for\",\n",
       "      \"upos\": \"ADP\",\n",
       "      \"xpos\": \"IN\",\n",
       "      \"head\": 62,\n",
       "      \"deprel\": \"case\",\n",
       "      \"start_char\": 316,\n",
       "      \"end_char\": 319,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 62,\n",
       "      \"text\": \"frying\",\n",
       "      \"lemma\": \"frying\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 59,\n",
       "      \"deprel\": \"nmod\",\n",
       "      \"start_char\": 320,\n",
       "      \"end_char\": 326,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 63,\n",
       "      \"text\": \";\",\n",
       "      \"lemma\": \";\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \":\",\n",
       "      \"head\": 66,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 326,\n",
       "      \"end_char\": 327,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 64,\n",
       "      \"text\": \"about\",\n",
       "      \"lemma\": \"about\",\n",
       "      \"upos\": \"ADV\",\n",
       "      \"xpos\": \"RB\",\n",
       "      \"head\": 65,\n",
       "      \"deprel\": \"advmod\",\n",
       "      \"start_char\": 328,\n",
       "      \"end_char\": 333,\n",
       "      \"ner\": \"B-QUANTITY\",\n",
       "      \"multi_ner\": [\n",
       "        \"B-QUANTITY\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 65,\n",
       "      \"text\": \"10\",\n",
       "      \"lemma\": \"10\",\n",
       "      \"upos\": \"NUM\",\n",
       "      \"xpos\": \"CD\",\n",
       "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
       "      \"head\": 66,\n",
       "      \"deprel\": \"nummod\",\n",
       "      \"start_char\": 334,\n",
       "      \"end_char\": 336,\n",
       "      \"ner\": \"I-QUANTITY\",\n",
       "      \"multi_ner\": [\n",
       "        \"I-QUANTITY\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 66,\n",
       "      \"text\": \"cups\",\n",
       "      \"lemma\": \"cup\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NNS\",\n",
       "      \"feats\": \"Number=Plur\",\n",
       "      \"head\": 59,\n",
       "      \"deprel\": \"parataxis\",\n",
       "      \"start_char\": 337,\n",
       "      \"end_char\": 341,\n",
       "      \"ner\": \"I-QUANTITY\",\n",
       "      \"multi_ner\": [\n",
       "        \"I-QUANTITY\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 67,\n",
       "      \"text\": \")\",\n",
       "      \"lemma\": \")\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \"-RRB-\",\n",
       "      \"head\": 66,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 341,\n",
       "      \"end_char\": 342,\n",
       "      \"ner\": \"E-QUANTITY\",\n",
       "      \"multi_ner\": [\n",
       "        \"E-QUANTITY\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 68,\n",
       "      \"text\": \"brk\",\n",
       "      \"lemma\": \"brk\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 72,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 343,\n",
       "      \"end_char\": 346,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 69,\n",
       "      \"text\": \"2\",\n",
       "      \"lemma\": \"2\",\n",
       "      \"upos\": \"NUM\",\n",
       "      \"xpos\": \"CD\",\n",
       "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
       "      \"head\": 70,\n",
       "      \"deprel\": \"nummod\",\n",
       "      \"start_char\": 347,\n",
       "      \"end_char\": 348,\n",
       "      \"ner\": \"S-CARDINAL\",\n",
       "      \"multi_ner\": [\n",
       "        \"S-CARDINAL\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 70,\n",
       "      \"text\": \"cups\",\n",
       "      \"lemma\": \"cup\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NNS\",\n",
       "      \"feats\": \"Number=Plur\",\n",
       "      \"head\": 71,\n",
       "      \"deprel\": \"nmod:npmod\",\n",
       "      \"start_char\": 349,\n",
       "      \"end_char\": 353,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 71,\n",
       "      \"text\": \"buttermilk\",\n",
       "      \"lemma\": \"buttermilk\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 72,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 354,\n",
       "      \"end_char\": 364,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 72,\n",
       "      \"text\": \"brk\",\n",
       "      \"lemma\": \"brk\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 5,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 365,\n",
       "      \"end_char\": 368,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 73,\n",
       "      \"text\": \"2\",\n",
       "      \"lemma\": \"2\",\n",
       "      \"upos\": \"NUM\",\n",
       "      \"xpos\": \"CD\",\n",
       "      \"feats\": \"NumForm=Digit|NumType=Card\",\n",
       "      \"head\": 74,\n",
       "      \"deprel\": \"nummod\",\n",
       "      \"start_char\": 369,\n",
       "      \"end_char\": 370,\n",
       "      \"ner\": \"S-CARDINAL\",\n",
       "      \"multi_ner\": [\n",
       "        \"S-CARDINAL\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 74,\n",
       "      \"text\": \"cups\",\n",
       "      \"lemma\": \"cup\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NNS\",\n",
       "      \"feats\": \"Number=Plur\",\n",
       "      \"head\": 72,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 371,\n",
       "      \"end_char\": 375,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 75,\n",
       "      \"text\": \"all\",\n",
       "      \"lemma\": \"all\",\n",
       "      \"upos\": \"DET\",\n",
       "      \"xpos\": \"DT\",\n",
       "      \"head\": 77,\n",
       "      \"deprel\": \"det\",\n",
       "      \"start_char\": 376,\n",
       "      \"end_char\": 379,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 76,\n",
       "      \"text\": \"-\",\n",
       "      \"lemma\": \"-\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \"HYPH\",\n",
       "      \"head\": 75,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 379,\n",
       "      \"end_char\": 380,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 77,\n",
       "      \"text\": \"purpose\",\n",
       "      \"lemma\": \"purpose\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 78,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 380,\n",
       "      \"end_char\": 387,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 78,\n",
       "      \"text\": \"flour\",\n",
       "      \"lemma\": \"flour\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 5,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 388,\n",
       "      \"end_char\": 393,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 79,\n",
       "      \"text\": \"brk\",\n",
       "      \"lemma\": \"brk\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 81,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 394,\n",
       "      \"end_char\": 397,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 80,\n",
       "      \"text\": \"kosher\",\n",
       "      \"lemma\": \"kosher\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 81,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 398,\n",
       "      \"end_char\": 404,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 81,\n",
       "      \"text\": \"salt\",\n",
       "      \"lemma\": \"salt\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 83,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 405,\n",
       "      \"end_char\": 409,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 82,\n",
       "      \"text\": \"brk\",\n",
       "      \"lemma\": \"brk\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 83,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 410,\n",
       "      \"end_char\": 413,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 83,\n",
       "      \"text\": \"honey\",\n",
       "      \"lemma\": \"honey\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 78,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 414,\n",
       "      \"end_char\": 419,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 84,\n",
       "      \"text\": \",\",\n",
       "      \"lemma\": \",\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \",\",\n",
       "      \"head\": 87,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 419,\n",
       "      \"end_char\": 420,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 85,\n",
       "      \"text\": \"flaky\",\n",
       "      \"lemma\": \"flaky\",\n",
       "      \"upos\": \"ADJ\",\n",
       "      \"xpos\": \"JJ\",\n",
       "      \"feats\": \"Degree=Pos\",\n",
       "      \"head\": 87,\n",
       "      \"deprel\": \"amod\",\n",
       "      \"start_char\": 421,\n",
       "      \"end_char\": 426,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 86,\n",
       "      \"text\": \"sea\",\n",
       "      \"lemma\": \"sea\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 87,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 427,\n",
       "      \"end_char\": 430,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 87,\n",
       "      \"text\": \"salt\",\n",
       "      \"lemma\": \"salt\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 78,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 431,\n",
       "      \"end_char\": 435,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 88,\n",
       "      \"text\": \"(\",\n",
       "      \"lemma\": \"(\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \"-LRB-\",\n",
       "      \"head\": 91,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 436,\n",
       "      \"end_char\": 437,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 89,\n",
       "      \"text\": \"such\",\n",
       "      \"lemma\": \"such\",\n",
       "      \"upos\": \"ADJ\",\n",
       "      \"xpos\": \"JJ\",\n",
       "      \"feats\": \"Degree=Pos\",\n",
       "      \"head\": 91,\n",
       "      \"deprel\": \"case\",\n",
       "      \"start_char\": 437,\n",
       "      \"end_char\": 441,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 90,\n",
       "      \"text\": \"as\",\n",
       "      \"lemma\": \"as\",\n",
       "      \"upos\": \"ADP\",\n",
       "      \"xpos\": \"IN\",\n",
       "      \"head\": 89,\n",
       "      \"deprel\": \"fixed\",\n",
       "      \"start_char\": 442,\n",
       "      \"end_char\": 444,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 91,\n",
       "      \"text\": \"maldon\",\n",
       "      \"lemma\": \"maldon\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 87,\n",
       "      \"deprel\": \"nmod\",\n",
       "      \"start_char\": 445,\n",
       "      \"end_char\": 451,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 92,\n",
       "      \"text\": \")\",\n",
       "      \"lemma\": \")\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \"-RRB-\",\n",
       "      \"head\": 91,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 451,\n",
       "      \"end_char\": 452,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 93,\n",
       "      \"text\": \",\",\n",
       "      \"lemma\": \",\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \",\",\n",
       "      \"head\": 95,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 452,\n",
       "      \"end_char\": 453,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 94,\n",
       "      \"text\": \"toasted\",\n",
       "      \"lemma\": \"toast\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VBN\",\n",
       "      \"feats\": \"Tense=Past|VerbForm=Part\",\n",
       "      \"head\": 95,\n",
       "      \"deprel\": \"amod\",\n",
       "      \"start_char\": 454,\n",
       "      \"end_char\": 461,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 95,\n",
       "      \"text\": \"benne\",\n",
       "      \"lemma\": \"benne\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 11,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 462,\n",
       "      \"end_char\": 467,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 96,\n",
       "      \"text\": \"or\",\n",
       "      \"lemma\": \"or\",\n",
       "      \"upos\": \"CCONJ\",\n",
       "      \"xpos\": \"CC\",\n",
       "      \"head\": 98,\n",
       "      \"deprel\": \"cc\",\n",
       "      \"start_char\": 468,\n",
       "      \"end_char\": 470,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 97,\n",
       "      \"text\": \"sesame\",\n",
       "      \"lemma\": \"sesame\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 98,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 471,\n",
       "      \"end_char\": 477,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 98,\n",
       "      \"text\": \"seeds\",\n",
       "      \"lemma\": \"seed\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NNS\",\n",
       "      \"feats\": \"Number=Plur\",\n",
       "      \"head\": 95,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 478,\n",
       "      \"end_char\": 483,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 99,\n",
       "      \"text\": \",\",\n",
       "      \"lemma\": \",\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \",\",\n",
       "      \"head\": 23,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 483,\n",
       "      \"end_char\": 484,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 100,\n",
       "      \"text\": \"hot\",\n",
       "      \"lemma\": \"hot\",\n",
       "      \"upos\": \"ADJ\",\n",
       "      \"xpos\": \"JJ\",\n",
       "      \"feats\": \"Degree=Pos\",\n",
       "      \"head\": 101,\n",
       "      \"deprel\": \"amod\",\n",
       "      \"start_char\": 485,\n",
       "      \"end_char\": 488,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 101,\n",
       "      \"text\": \"sauce\",\n",
       "      \"lemma\": \"sauce\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 5,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 489,\n",
       "      \"end_char\": 494,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 102,\n",
       "      \"text\": \"(\",\n",
       "      \"lemma\": \"(\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \"-LRB-\",\n",
       "      \"head\": 104,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 495,\n",
       "      \"end_char\": 496,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 103,\n",
       "      \"text\": \"for\",\n",
       "      \"lemma\": \"for\",\n",
       "      \"upos\": \"ADP\",\n",
       "      \"xpos\": \"IN\",\n",
       "      \"head\": 104,\n",
       "      \"deprel\": \"case\",\n",
       "      \"start_char\": 496,\n",
       "      \"end_char\": 499,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 104,\n",
       "      \"text\": \"serving\",\n",
       "      \"lemma\": \"serving\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 106,\n",
       "      \"deprel\": \"obl\",\n",
       "      \"start_char\": 500,\n",
       "      \"end_char\": 507,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 105,\n",
       "      \"text\": \")\",\n",
       "      \"lemma\": \")\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \"-RRB-\",\n",
       "      \"head\": 104,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 507,\n",
       "      \"end_char\": 508,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 106,\n",
       "      \"text\": \"brk\",\n",
       "      \"lemma\": \"brk\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VB\",\n",
       "      \"feats\": \"VerbForm=Fin\",\n",
       "      \"head\": 11,\n",
       "      \"deprel\": \"parataxis\",\n",
       "      \"start_char\": 509,\n",
       "      \"end_char\": 512,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 107,\n",
       "      \"text\": \"a\",\n",
       "      \"lemma\": \"a\",\n",
       "      \"upos\": \"DET\",\n",
       "      \"xpos\": \"DT\",\n",
       "      \"feats\": \"Definite=Ind|PronType=Art\",\n",
       "      \"head\": 111,\n",
       "      \"deprel\": \"det\",\n",
       "      \"start_char\": 513,\n",
       "      \"end_char\": 514,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 108,\n",
       "      \"text\": \"deep\",\n",
       "      \"lemma\": \"deep\",\n",
       "      \"upos\": \"ADJ\",\n",
       "      \"xpos\": \"JJ\",\n",
       "      \"feats\": \"Degree=Pos\",\n",
       "      \"head\": 110,\n",
       "      \"deprel\": \"amod\",\n",
       "      \"start_char\": 515,\n",
       "      \"end_char\": 519,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 109,\n",
       "      \"text\": \"-\",\n",
       "      \"lemma\": \"-\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \"HYPH\",\n",
       "      \"head\": 108,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 519,\n",
       "      \"end_char\": 520,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 110,\n",
       "      \"text\": \"fry\",\n",
       "      \"lemma\": \"fry\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 111,\n",
       "      \"deprel\": \"compound\",\n",
       "      \"start_char\": 520,\n",
       "      \"end_char\": 523,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"id\": 111,\n",
       "      \"text\": \"thermometer\",\n",
       "      \"lemma\": \"thermometer\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 106,\n",
       "      \"deprel\": \"obj\",\n",
       "      \"start_char\": 524,\n",
       "      \"end_char\": 535,\n",
       "      \"ner\": \"O\",\n",
       "      \"multi_ner\": [\n",
       "        \"O\"\n",
       "      ]\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc = stanza_preprocessor(nlp, full_df['ingredients'][0:5][0])\n",
    "preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tablespoon yellow mustard seed brk tablespoon brown mustard seed brk teaspoon coriander seed brk cup apple cider vinegar brk cup kosher salt brk cup sugar brk cup chop fresh dill brk skinless , boneless chicken thigh ( pound ) , halve , quarter large brk vegetable oil ( frying ; cup ) brk cup buttermilk brk cup - purpose flour brk kosher salt brk honey , flaky sea salt ( such maldon ) , toast benne sesame seed , hot sauce ( serving ) brk deep - fry thermometer'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmad = stanza_lemmatizer(preproc)\n",
    "lemmad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['prepSteps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['prepSteps'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_steps = full_df['prepSteps'].apply(\" \".join).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-calculating sentence mebeddings\n",
    "# embedding_model_params = {'embedding_model': 'all-MiniLM-L6-v2'}\n",
    "# embedding_model = SentenceTransformer(embedding_model_params['embedding_model'])\n",
    "# embeddings = embedding_model.encode(recipe_steps, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify UMAP dimensionality reductions\n",
    "# umap_model_params = {'n_neighbors':15, 'n_components':10, 'random_state':200}\n",
    "# umap_model = UMAP(**umap_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster with HDBSCAN\n",
    "# hdbscan_model_params = {'min_cluster_size':200, 'prediction_data':True}\n",
    "# hdbscan_model = HDBSCAN(**hdbscan_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['alternatives', 'bibb', 'boston', 'chacheres', 'chobani', 'franks', 'grands', 'hass', 'hidden', 'hunts', 'japanese', 'kc', 'lakes', 'laughing', 'masterpiece', 'pillsburyTM', 'progressoTM', 'sauce', 'secrets', 'smokies', 'tony', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m cv_params \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mstrip_accents\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39municode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmin_df\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m10\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m vectorizer_model \u001b[39m=\u001b[39m CountVectorizer(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcv_params)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m test_transform \u001b[39m=\u001b[39m vectorizer_model\u001b[39m.\u001b[39;49mfit_transform(full_df[\u001b[39m'\u001b[39;49m\u001b[39mingredients\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m:\u001b[39m5\u001b[39;49m])\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1277\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m---> 68\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# adding custom count vectorization\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    # 'preprocessor':custom_preprocessor,\n",
    "    # 'tokenizer':custom_lemmatizer, # out of memory \n",
    "    'stop_words':flushtrated_list,\n",
    "    'token_pattern':r\"(?u)\\b[a-zA-Z]{2,}\\b\",\n",
    "    'ngram_range':(1, 4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "vectorizer_model = CountVectorizer(**cv_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with open('../data/processed/bertopic_params.joblib', 'w') as fp:\n",
    "# pipeline_params = {\n",
    "#     'embedding':{'pretrained_sentence_embeddings': embedding_model_params},\n",
    "#     'dimension_reduction': {'UMAP': umap_model_params},\n",
    "#     'clustering': {'HDBSCAN': hdbscan_model_params},\n",
    "#     'vectorizer': {'sklearn_countvectorizer': cv_params},\n",
    "# }\n",
    "# joblib.dump(pipeline_params, '../data/processed/bertopic_params.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id=get_experiment_id(\"initial_explicit_spec_run_3\")):\n",
    "    pipeline_params = {\n",
    "        'language':'english',\n",
    "        'top_n_words':20,\n",
    "        'n_gram_range':(1, 4),\n",
    "        'min_topic_size':500,\n",
    "        'nr_topics':'auto',\n",
    "        'verbose':True,\n",
    "        'low_memory':True,\n",
    "        'calculate_probabilities':True\n",
    "    }\n",
    "    mlflow.log_params(pipeline_params)\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        **pipeline_params,\n",
    "        vectorizer_model=vectorizer_model\n",
    "    )\n",
    "    # TOKENIZERS_PARALLELISM=False\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(recipe_steps)\n",
    "\n",
    "    topic_model.get_topic_info().to_json('../data/processed/topic_model_df.json')\n",
    "\n",
    "    # mlflow.log_artifact('../data/processed/bertopic_params.joblib')\n",
    "    mlflow.log_artifact('../data/processed/topic_model_df.json')\n",
    "\n",
    "    print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_full(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()['Representation']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | export\n",
    "def foo():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# import nbdev\n",
    "\n",
    "# nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
