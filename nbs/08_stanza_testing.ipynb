{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Stanza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d4176199ab4b5fb834d7544daffe42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 22:24:55 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-11-22 22:24:57 INFO: File exists: /home/awchen/stanza_resources/en/default.zip\n",
      "2023-11-22 22:25:03 INFO: Finished downloading models and saved to /home/awchen/stanza_resources.\n",
      "2023-11-22 22:25:03 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e19149306448afa87c49eddead4426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 22:25:05 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2023-11-22 22:25:05 INFO: Using device: cuda\n",
      "2023-11-22 22:25:05 INFO: Loading: tokenize\n",
      "2023-11-22 22:25:22 INFO: Loading: pos\n",
      "2023-11-22 22:25:23 INFO: Loading: lemma\n",
      "2023-11-22 22:25:24 INFO: Loading: constituency\n",
      "2023-11-22 22:25:25 INFO: Loading: depparse\n",
      "2023-11-22 22:25:25 INFO: Loading: sentiment\n",
      "2023-11-22 22:25:26 INFO: Loading: ner\n",
      "2023-11-22 22:25:28 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *\n",
    "import project_path\n",
    "\n",
    "# from bertopic import BERTopic\n",
    "import dagshub\n",
    "dagshub.init(repo_name='MeaLeon', repo_owner='AaronWChen')\n",
    "# from datetime import datetime\n",
    "# from hdbscan import HDBSCAN\n",
    "import dill as pickle\n",
    "from itertools import tee, islice\n",
    "import joblib \n",
    "import json\n",
    "# import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "# from sklearn.base import TransformerMixin\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfTransformer,\n",
    "    TfidfVectorizer,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# import spacy\n",
    "# import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# from spacy.lemmatizer import Lemmatizer\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "# from umap import UMAP\n",
    "\n",
    "# importing local scripts\n",
    "# import src.custom_stanza_mlflow as stz_wrp\n",
    "from src.custom_stanza_mlflow import StanzaWrapper\n",
    "# import src.nlp_processor as nlpp\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@markdown Enter the username of your DAGsHub account:\n",
    "DAGSHUB_USER_NAME = \"AaronWChen\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the email for your DAGsHub account:\n",
    "DAGSHUB_EMAIL = \"awc33@cornell.edu\" #@param {type:\"string\"}\n",
    "\n",
    "DAGSHUB_REPO_NAME=\"MeaLeon\"\n",
    "BRANCH=\"STANZA-1/refactor-nltk-stanza\"\n",
    "dagshub.init(repo_name='MeaLeon', repo_owner='AaronWChen')\n",
    "\n",
    "mlflow.set_tracking_uri(f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_id(name):\n",
    "    exp = mlflow.get_experiment_by_name(name)\n",
    "    if exp is None:\n",
    "      exp_id = mlflow.create_experiment(name)\n",
    "      return exp_id\n",
    "    return exp.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_preprocessor(stanza_pipeline, ingredients_list):\n",
    "    # This function takes in a Stanza pipeline and a recipe's ingredients in list form and returns a Stanza transformed document to be used in the lemmatizer\n",
    "    lowered = \" brk \".join(ingredients_list).lower()\n",
    "    # lowered = ingredients_list.apply(\" brk \".join).str.lower()\n",
    "    print(type(stanza_pipeline(lowered)))\n",
    "    print(stanza_pipeline(lowered))\n",
    "    return stanza_pipeline(lowered)\n",
    "\n",
    "# print(*[f'word: {word.text+\" \"}\\t \\t lemma: {word.lemma}, \\t \\t upos: {word.upos}' for sent in doc2.sentences for word in sent.words], sep='\\n')\n",
    "\n",
    "# This will be the tokenizer?\n",
    "# lemma_test_recipe_2 = \" \".join([word.lemma for sent in doc2.sentences for word in sent.words if (\n",
    "#     word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\"]\n",
    "# )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_lemmatizer(stanza_preprocessed):\n",
    "    # This function takes in the preprocessed Stanza document from preprocessor and performs lemmatization and filtering\n",
    "    \n",
    "    return \" \".join([word.lemma \n",
    "                      for sent in stanza_preprocessed.sentences \n",
    "                      for word in sent.words if (\n",
    "                          word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\"]\n",
    "                          )\n",
    "                    ])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom ngram analyzer function, matching only ngrams that belong to the same line\n",
    "def gen_analyzer(minNgramLength, maxNgramLength):\n",
    "    def ngrams_per_line(doc):\n",
    "\n",
    "        # analyze each line of the input string seperately\n",
    "        for ln in doc.split('brk'):\n",
    "\n",
    "            # tokenize the input string (customize the regex as desired)\n",
    "            terms = re.findall(u'(?u)\\\\b\\\\w+\\\\b', ln)\n",
    "\n",
    "            # loop ngram creation for every number between min and max ngram length\n",
    "            for ngramLength in range(minNgramLength, maxNgramLength+1):\n",
    "\n",
    "                # find and return all ngrams\n",
    "                # for ngram in zip(*[terms[i:] for i in range(3)]): <-- solution without a generator (works the same but has higher memory usage)\n",
    "                for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]): # <-- solution using a generator\n",
    "                    ngram = ' '.join(ngram)\n",
    "                    yield ngram\n",
    "    return ngrams_per_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom ngram analyzer function, matching only ngrams that belong to the same line\n",
    "def gen_analyzer_2(stanza_pipeline, minNgramLength, maxNgramLength):\n",
    "    def ngrams_per_line(ingredients_list):\n",
    "\n",
    "        lowered = \" brk \".join(map(str, [ingred for ingred in ingredients_list if ingred is not None])).lower()\n",
    "        \n",
    "        if lowered is None:\n",
    "            lowered = \"Missing ingredients\"\n",
    "        \n",
    "        # print(\"Lower case ingredients: \\n\")\n",
    "        # print(lowered)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        preproc = stanza_pipeline(lowered)\n",
    "        # print(\"Preprocessed ingredients: \\n\")\n",
    "        # print(preproc)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        \n",
    "        lemmad = \" \".join(map(str,\n",
    "                              [word.lemma \n",
    "                               for sent in preproc.sentences \n",
    "                               for word in sent.words if (\n",
    "                                   word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\"]\n",
    "                                #    and word not in STOP_WORDS\n",
    "                                and word is not None\n",
    "                            )]\n",
    "                          )\n",
    "                    )\n",
    "        \n",
    "        # print(\"Lemmatized ingredients: \\n\")\n",
    "        # print(lemmad)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        # analyze each line of the input string seperately\n",
    "        for ln in lemmad.split(' brk '):\n",
    "            \n",
    "            # tokenize the input string (customize the regex as desired)\n",
    "            # terms = ln.split()\n",
    "            terms = re.split(\"(?u)\\b[a-zA-Z]{2,}\\b\", ln)\n",
    "\n",
    "            # loop ngram creation for every number between min and max ngram length\n",
    "            for ngramLength in range(minNgramLength, maxNgramLength+1):\n",
    "\n",
    "                # find and return all ngrams\n",
    "                # for ngram in zip(*[terms[i:] for i in range(3)]): <-- solution without a generator (works the same but has higher memory usage)\n",
    "                # for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]): # <-- solution using a generator\n",
    "                for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]): # <-- solution using a generator\n",
    "                    # print(\"ngram: \\n\")\n",
    "                    # print(ngram)\n",
    "                    # print(\"\\n\")\n",
    "                    \n",
    "                    ngram = ' '.join(map(str, ngram))\n",
    "                    yield ngram\n",
    "                    \n",
    "    return ngrams_per_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = '../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "food_stopwords_path = \"../food_stopwords.csv\"\n",
    "\n",
    "joblib_basepath = '../joblib/2022.08.23/'\n",
    "\n",
    "cv_path = joblib_basepath + 'countvec.joblib'\n",
    "tfidf_path = joblib_basepath + 'tfidf.joblib'\n",
    "full_df_path = joblib_basepath + 'recipes_with_cv.joblib'\n",
    "reduced_df_path = joblib_basepath + 'reduced_df.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a redeem for variable naming mixed with a free pun-ish me daddy, flushtrated will be the list of all stopword to exclude so named because we're throwing these words down the drain\n",
    "\n",
    "flushtrated = {x for x in pd.read_csv(food_stopwords_path)}\n",
    "additional_to_exclude = {\n",
    "    \"red\",\n",
    "    \"green\",\n",
    "    \"black\",\n",
    "    \"yellow\",\n",
    "    \"white\",\n",
    "    \"inch\",\n",
    "    \"mince\",\n",
    "    \"chop\",\n",
    "    \"fry\",\n",
    "    \"trim\",\n",
    "    \"flat\",\n",
    "    \"beat\",\n",
    "    \"brown\",\n",
    "    \"golden\",\n",
    "    \"balsamic\",\n",
    "    \"halve\",\n",
    "    \"blue\",\n",
    "    \"divide\",\n",
    "    \"trim\",\n",
    "    \"unbleache\",\n",
    "    \"granulate\",\n",
    "    \"Frank\",\n",
    "    \"alternative\",\n",
    "    \"american\",\n",
    "    \"annie\",\n",
    "    \"asian\",\n",
    "    \"balance\",\n",
    "    \"band\",\n",
    "    \"barrel\",\n",
    "    \"bay\",\n",
    "    \"bayou\",\n",
    "    \"beam\",\n",
    "    \"beard\",\n",
    "    \"bell\",\n",
    "    \"betty\",\n",
    "    \"bird\",\n",
    "    \"blast\",\n",
    "    \"bob\",\n",
    "    \"bone\",\n",
    "    \"breyers\",\n",
    "    \"calore\",\n",
    "    \"carb\",\n",
    "    \"card\",\n",
    "    \"chachere\",\n",
    "    \"change\",\n",
    "    \"circle\",\n",
    "    \"coffee\",\n",
    "    \"coil\",\n",
    "    \"country\",\n",
    "    \"cow\",\n",
    "    \"crack\",\n",
    "    \"cracker\",\n",
    "    \"crocker\",\n",
    "    \"crystal\",\n",
    "    \"dean\",\n",
    "    \"degree\",\n",
    "    \"deluxe\",\n",
    "    \"direction\",\n",
    "    \"duncan\",\n",
    "    \"earth\",\n",
    "    \"eggland\",\n",
    "    \"ener\",\n",
    "    \"envelope\",\n",
    "    \"eye\",\n",
    "    \"fantastic\",\n",
    "    \"far\",\n",
    "    \"fat\",\n",
    "    \"feather\",\n",
    "    \"flake\",\n",
    "    \"foot\",\n",
    "    \"fourth\",\n",
    "    \"frank\",\n",
    "    \"french\",\n",
    "    \"fusion\",\n",
    "    \"genoa\",\n",
    "    \"genovese\",\n",
    "    \"germain\",\n",
    "    \"giada\",\n",
    "    \"gold\",\n",
    "    \"granule\",\n",
    "    \"greek\",\n",
    "    \"hamburger\",\n",
    "    \"helper\",\n",
    "    \"herbe\",\n",
    "    \"hines\",\n",
    "    \"hodgson\",\n",
    "    \"hunt\",\n",
    "    \"instruction\",\n",
    "    \"interval\",\n",
    "    \"italianstyle\",\n",
    "    \"jim\",\n",
    "    \"jimmy\",\n",
    "    \"kellogg\",\n",
    "    \"lagrille\",\n",
    "    \"lake\",\n",
    "    \"land\",\n",
    "    \"laurentiis\",\n",
    "    \"lawry\",\n",
    "    \"lipton\",\n",
    "    \"litre\",\n",
    "    \"ll\",\n",
    "    \"maid\",\n",
    "    \"malt\",\n",
    "    \"mate\",\n",
    "    \"mayer\",\n",
    "    \"meal\",\n",
    "    \"medal\",\n",
    "    \"medallion\",\n",
    "    \"member\",\n",
    "    \"mexicanstyle\",\n",
    "    \"monte\",\n",
    "    \"mori\",\n",
    "    \"nest\",\n",
    "    \"nu\",\n",
    "    \"oounce\",\n",
    "    \"oscar\",\n",
    "    \"ox\",\n",
    "    \"paso\",\n",
    "    \"pasta\",\n",
    "    \"patty\",\n",
    "    \"petal\",\n",
    "    \"pinche\",\n",
    "    \"preserve\",\n",
    "    \"quartere\",\n",
    "    \"ranch\",\n",
    "    \"ranchstyle\",\n",
    "    \"rasher\",\n",
    "    \"redhot\",\n",
    "    \"resemble\",\n",
    "    \"rice\",\n",
    "    \"ro\",\n",
    "    \"roni\",\n",
    "    \"scissor\",\n",
    "    \"scrap\",\n",
    "    \"secret\",\n",
    "    \"semicircle\",\n",
    "    \"shard\",\n",
    "    \"shear\",\n",
    "    \"sixth\",\n",
    "    \"sliver\",\n",
    "    \"smucker\",\n",
    "    \"snicker\",\n",
    "    \"source\",\n",
    "    \"spot\",\n",
    "    \"state\",\n",
    "    \"strand\",\n",
    "    \"sun\",\n",
    "    \"supreme\",\n",
    "    \"tablepoon\",\n",
    "    \"tail\",\n",
    "    \"target\",\n",
    "    \"tm\",\n",
    "    \"tong\",\n",
    "    \"toothpick\",\n",
    "    \"triangle\",\n",
    "    \"trimming\",\n",
    "    \"tweezer\",\n",
    "    \"valley\",\n",
    "    \"vay\",\n",
    "    \"wise\",\n",
    "    \"wishbone\",\n",
    "    \"wrapper\",\n",
    "    \"yoplait\",\n",
    "    \"ziploc\",\n",
    "}\n",
    "\n",
    "flushtrated = flushtrated.union(STOP_WORDS)\n",
    "flushtrated = flushtrated.union(additional_to_exclude)\n",
    "flushtrated_list = list(flushtrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dek</th>\n",
       "      <th>hed</th>\n",
       "      <th>aggregateRating</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>prepSteps</th>\n",
       "      <th>reviewsCount</th>\n",
       "      <th>willMakeAgainPct</th>\n",
       "      <th>cuisine_name</th>\n",
       "      <th>photo_filename</th>\n",
       "      <th>...</th>\n",
       "      <th>zest pith</th>\n",
       "      <th>zest vegetable</th>\n",
       "      <th>zinfandel</th>\n",
       "      <th>ziti</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zucchini blossom</th>\n",
       "      <th>zucchini crookneck</th>\n",
       "      <th>zucchini squash</th>\n",
       "      <th>árbol</th>\n",
       "      <th>árbol pepper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54a2b6b019925f464b373351</td>\n",
       "      <td>How does fried chicken achieve No. 1 status? B...</td>\n",
       "      <td>Pickle-Brined Fried Chicken</td>\n",
       "      <td>3.11</td>\n",
       "      <td>[1 tablespoons yellow mustard seeds, 1 tablesp...</td>\n",
       "      <td>[Toast mustard and coriander seeds in a dry me...</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>51247610_fried-chicken_1x1.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54a408a019925f464b3733bc</td>\n",
       "      <td>Spinaci all'Ebraica</td>\n",
       "      <td>Spinach Jewish Style</td>\n",
       "      <td>3.22</td>\n",
       "      <td>[3 pounds small-leaved bulk spinach, Salt, 1/2...</td>\n",
       "      <td>[Remove the stems and roots from the spinach. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>Italian</td>\n",
       "      <td>EP_12162015_placeholders_rustic.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54a408a26529d92b2c003631</td>\n",
       "      <td>This majestic, moist, and richly spiced honey ...</td>\n",
       "      <td>New Year’s Honey Cake</td>\n",
       "      <td>3.62</td>\n",
       "      <td>[3 1/2 cups all-purpose flour, 1 tablespoon ba...</td>\n",
       "      <td>[I like this cake best baked in a 9-inch angel...</td>\n",
       "      <td>105</td>\n",
       "      <td>88</td>\n",
       "      <td>Kosher</td>\n",
       "      <td>EP_09022015_honeycake-2.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54a408a66529d92b2c003638</td>\n",
       "      <td>The idea for this sandwich came to me when my ...</td>\n",
       "      <td>The B.L.A.Bagel with Lox and Avocado</td>\n",
       "      <td>4.00</td>\n",
       "      <td>[1 small ripe avocado, preferably Hass (see No...</td>\n",
       "      <td>[A short time before serving, mash avocado and...</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>Kosher</td>\n",
       "      <td>EP_12162015_placeholders_casual.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54a408a719925f464b3733cc</td>\n",
       "      <td>In 1930, Simon Agranat, the chief justice of t...</td>\n",
       "      <td>Shakshuka a la Doktor Shakshuka</td>\n",
       "      <td>2.71</td>\n",
       "      <td>[2 pounds fresh tomatoes, unpeeled and cut in ...</td>\n",
       "      <td>[1. Place the tomatoes, garlic, salt, paprika,...</td>\n",
       "      <td>7</td>\n",
       "      <td>83</td>\n",
       "      <td>Kosher</td>\n",
       "      <td>EP_12162015_placeholders_formal.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34751</th>\n",
       "      <td>59541a31bff3052847ae2107</td>\n",
       "      <td>Buttering the bread before you waffle it ensur...</td>\n",
       "      <td>Waffled Ham and Cheese Melt with Maple Butter</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[1 tablespoon unsalted butter, at room tempera...</td>\n",
       "      <td>[Preheat the waffle iron on low., Spread a thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>waffle-ham-and-cheese-melt-062817.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34752</th>\n",
       "      <td>5954233ad52ca90dc28200e7</td>\n",
       "      <td>Spread this easy compound butter on waffles, p...</td>\n",
       "      <td>Maple Butter</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[8 tablespoons (1 stick) salted butter, at roo...</td>\n",
       "      <td>[Combine the ingredients in a medium-size bowl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>EP_12162015_placeholders_bright.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34753</th>\n",
       "      <td>595424c2109c972493636f83</td>\n",
       "      <td>Leftover mac and cheese is not exactly one of ...</td>\n",
       "      <td>Waffled Macaroni and Cheese</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[3 tablespoons unsalted butter, plus more for ...</td>\n",
       "      <td>[Preheat the oven to 375°F. Butter a 9x5-inch ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>waffle-mac-n-cheese-062816.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34754</th>\n",
       "      <td>5956638625dc3d1d829b7166</td>\n",
       "      <td>A classic Mexican beer cocktail you can sip al...</td>\n",
       "      <td>Classic Michelada</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[Coarse salt, 2 lime wedges, 2 ounces tomato j...</td>\n",
       "      <td>[Place about 1/4 cup salt on a small plate. Ru...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>Classic Michelada 07292017.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34755</th>\n",
       "      <td>59566daa25dc3d1d829b7169</td>\n",
       "      <td>A grapefruit beer that's one of the most refre...</td>\n",
       "      <td>So Radler</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[1 bottle (375 ml) sour beer, such as Almanac ...</td>\n",
       "      <td>[Combine the water, honey, rosemary, and grape...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>EP_12162015_placeholders_bright.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34656 rows × 3365 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  \\\n",
       "0      54a2b6b019925f464b373351   \n",
       "1      54a408a019925f464b3733bc   \n",
       "2      54a408a26529d92b2c003631   \n",
       "3      54a408a66529d92b2c003638   \n",
       "4      54a408a719925f464b3733cc   \n",
       "...                         ...   \n",
       "34751  59541a31bff3052847ae2107   \n",
       "34752  5954233ad52ca90dc28200e7   \n",
       "34753  595424c2109c972493636f83   \n",
       "34754  5956638625dc3d1d829b7166   \n",
       "34755  59566daa25dc3d1d829b7169   \n",
       "\n",
       "                                                     dek  \\\n",
       "0      How does fried chicken achieve No. 1 status? B...   \n",
       "1                                    Spinaci all'Ebraica   \n",
       "2      This majestic, moist, and richly spiced honey ...   \n",
       "3      The idea for this sandwich came to me when my ...   \n",
       "4      In 1930, Simon Agranat, the chief justice of t...   \n",
       "...                                                  ...   \n",
       "34751  Buttering the bread before you waffle it ensur...   \n",
       "34752  Spread this easy compound butter on waffles, p...   \n",
       "34753  Leftover mac and cheese is not exactly one of ...   \n",
       "34754  A classic Mexican beer cocktail you can sip al...   \n",
       "34755  A grapefruit beer that's one of the most refre...   \n",
       "\n",
       "                                                 hed  aggregateRating  \\\n",
       "0                        Pickle-Brined Fried Chicken             3.11   \n",
       "1                               Spinach Jewish Style             3.22   \n",
       "2                              New Year’s Honey Cake             3.62   \n",
       "3              The B.L.A.Bagel with Lox and Avocado             4.00   \n",
       "4                    Shakshuka a la Doktor Shakshuka             2.71   \n",
       "...                                              ...              ...   \n",
       "34751  Waffled Ham and Cheese Melt with Maple Butter             0.00   \n",
       "34752                                   Maple Butter             0.00   \n",
       "34753                    Waffled Macaroni and Cheese             0.00   \n",
       "34754                              Classic Michelada             0.00   \n",
       "34755                                      So Radler             0.00   \n",
       "\n",
       "                                             ingredients  \\\n",
       "0      [1 tablespoons yellow mustard seeds, 1 tablesp...   \n",
       "1      [3 pounds small-leaved bulk spinach, Salt, 1/2...   \n",
       "2      [3 1/2 cups all-purpose flour, 1 tablespoon ba...   \n",
       "3      [1 small ripe avocado, preferably Hass (see No...   \n",
       "4      [2 pounds fresh tomatoes, unpeeled and cut in ...   \n",
       "...                                                  ...   \n",
       "34751  [1 tablespoon unsalted butter, at room tempera...   \n",
       "34752  [8 tablespoons (1 stick) salted butter, at roo...   \n",
       "34753  [3 tablespoons unsalted butter, plus more for ...   \n",
       "34754  [Coarse salt, 2 lime wedges, 2 ounces tomato j...   \n",
       "34755  [1 bottle (375 ml) sour beer, such as Almanac ...   \n",
       "\n",
       "                                               prepSteps  reviewsCount  \\\n",
       "0      [Toast mustard and coriander seeds in a dry me...             7   \n",
       "1      [Remove the stems and roots from the spinach. ...             5   \n",
       "2      [I like this cake best baked in a 9-inch angel...           105   \n",
       "3      [A short time before serving, mash avocado and...             7   \n",
       "4      [1. Place the tomatoes, garlic, salt, paprika,...             7   \n",
       "...                                                  ...           ...   \n",
       "34751  [Preheat the waffle iron on low., Spread a thi...             0   \n",
       "34752  [Combine the ingredients in a medium-size bowl...             0   \n",
       "34753  [Preheat the oven to 375°F. Butter a 9x5-inch ...             0   \n",
       "34754  [Place about 1/4 cup salt on a small plate. Ru...             0   \n",
       "34755  [Combine the water, honey, rosemary, and grape...             0   \n",
       "\n",
       "       willMakeAgainPct     cuisine_name  \\\n",
       "0                   100  Missing Cuisine   \n",
       "1                    80          Italian   \n",
       "2                    88           Kosher   \n",
       "3                   100           Kosher   \n",
       "4                    83           Kosher   \n",
       "...                 ...              ...   \n",
       "34751                 0  Missing Cuisine   \n",
       "34752                 0  Missing Cuisine   \n",
       "34753                 0  Missing Cuisine   \n",
       "34754                 0  Missing Cuisine   \n",
       "34755                 0  Missing Cuisine   \n",
       "\n",
       "                              photo_filename  ... zest pith zest vegetable  \\\n",
       "0             51247610_fried-chicken_1x1.jpg  ...       0.0            0.0   \n",
       "1        EP_12162015_placeholders_rustic.jpg  ...       0.0            0.0   \n",
       "2                EP_09022015_honeycake-2.jpg  ...       0.0            0.0   \n",
       "3        EP_12162015_placeholders_casual.jpg  ...       0.0            0.0   \n",
       "4        EP_12162015_placeholders_formal.jpg  ...       0.0            0.0   \n",
       "...                                      ...  ...       ...            ...   \n",
       "34751  waffle-ham-and-cheese-melt-062817.jpg  ...       0.0            0.0   \n",
       "34752    EP_12162015_placeholders_bright.jpg  ...       0.0            0.0   \n",
       "34753         waffle-mac-n-cheese-062816.jpg  ...       0.0            0.0   \n",
       "34754         Classic Michelada 07292017.jpg  ...       0.0            0.0   \n",
       "34755    EP_12162015_placeholders_bright.jpg  ...       0.0            0.0   \n",
       "\n",
       "      zinfandel ziti  zucchini  zucchini blossom  zucchini crookneck  \\\n",
       "0           0.0  0.0       0.0               0.0                 0.0   \n",
       "1           0.0  0.0       0.0               0.0                 0.0   \n",
       "2           0.0  0.0       0.0               0.0                 0.0   \n",
       "3           0.0  0.0       0.0               0.0                 0.0   \n",
       "4           0.0  0.0       0.0               0.0                 0.0   \n",
       "...         ...  ...       ...               ...                 ...   \n",
       "34751       0.0  0.0       0.0               0.0                 0.0   \n",
       "34752       0.0  0.0       0.0               0.0                 0.0   \n",
       "34753       0.0  0.0       0.0               0.0                 0.0   \n",
       "34754       0.0  0.0       0.0               0.0                 0.0   \n",
       "34755       0.0  0.0       0.0               0.0                 0.0   \n",
       "\n",
       "       zucchini squash  árbol  árbol pepper  \n",
       "0                  0.0    0.0           0.0  \n",
       "1                  0.0    0.0           0.0  \n",
       "2                  0.0    0.0           0.0  \n",
       "3                  0.0    0.0           0.0  \n",
       "4                  0.0    0.0           0.0  \n",
       "...                ...    ...           ...  \n",
       "34751              0.0    0.0           0.0  \n",
       "34752              0.0    0.0           0.0  \n",
       "34753              0.0    0.0           0.0  \n",
       "34754              0.0    0.0           0.0  \n",
       "34755              0.0    0.0           0.0  \n",
       "\n",
       "[34656 rows x 3365 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = joblib.load(full_df_path)\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['ingredients'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipe = \". \".join(full_df['ingredients'][0]).lower()\n",
    "test_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(test_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([sentence.text for sentence in doc.sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*[f'word: {word.text+\" \"}\\t \\t lemma: {word.lemma}, \\t \\t upos: {word.upos}' for sent in doc.sentences for word in sent.words], sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*[f'lemma: {word.lemma}' for sent in doc.sentences for word in sent.words if (\n",
    "    word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"PUNCT\", \"SCONJ\"]\n",
    ")], sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word.lemma for sent in doc.sentences for word in sent.words if (\n",
    "    word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"PUNCT\", \"SCONJ\"]\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = []\n",
    "for sent in doc.sentences:\n",
    "    ingredients = []\n",
    "    for word in sent.words:\n",
    "        if word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"PUNCT\", \"SCONJ\"]:\n",
    "            ingredients.append(word.lemma)\n",
    "        else:\n",
    "            pass\n",
    "    recipe.append(ingredients)\n",
    "\n",
    "recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found [this resource](https://stackoverflow.com/questions/26907309/create-ngrams-only-for-words-on-the-same-line-disregarding-line-breaks-with-sc), trying custom analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is probably going to be the preprocessor\n",
    "test_recipe_2 = \" brk \".join(full_df['ingredients'][0]).lower()\n",
    "doc2 = nlp(test_recipe_2)\n",
    "\n",
    "# print(*[f'word: {word.text+\" \"}\\t \\t lemma: {word.lemma}, \\t \\t upos: {word.upos}' for sent in doc2.sentences for word in sent.words], sep='\\n')\n",
    "\n",
    "# This will be the tokenizer?\n",
    "# lemma_test_recipe_2 = \" \".join([word.lemma for sent in doc2.sentences for word in sent.words if (\n",
    "#     word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\"]\n",
    "# )])\n",
    "lemma_test_recipe_2 = [\" \".join([word.lemma for sent in doc2.sentences for word in sent.words if (\n",
    "    word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\"]\n",
    ")])]\n",
    "lemma_test_recipe_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer=gen_analyzer(1, 4))\n",
    "cv.fit(lemma_test_recipe_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(lemma_test_recipe_2)\n",
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipe_pipeline = full_df['ingredients'][0]\n",
    "\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'preprocessor':stanza_preprocessor,\n",
    "    'tokenizer':stanza_lemmatizer, # out of memory \n",
    "    # 'stop_words':flushtrated_list,\n",
    "    'analyzer': gen_analyzer(1, 4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "vectorizer_model = CountVectorizer(**cv_params)\n",
    "\n",
    "test_transform = vectorizer_model.fit_transform(test_recipe_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipe_pipeline = full_df['ingredients'][0]\n",
    "\n",
    "test_recipe_preproc = stanza_preprocessor(nlp, test_recipe_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipe_lemma = stanza_lemmatizer(test_recipe_preproc)\n",
    "test_recipe_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ln in test_recipe_lemma.split('brk'):\n",
    "\n",
    "    # tokenize the input string (customize the regex as desired)\n",
    "    terms = re.findall(u'(?u)\\\\b\\\\w+\\\\b', ln)\n",
    "\n",
    "    # loop ngram creation for every number between min and max ngram length\n",
    "    for ngramLength in range(1, 5):\n",
    "\n",
    "        # find and return all ngrams\n",
    "        for ngram in zip(*[terms[i:] for i in range(3)]): #<-- solution without a generator (works the same but has higher memory usage)\n",
    "        # for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]): # <-- solution using a generator\n",
    "            ngram = ' '.join(ngram)\n",
    "            # yield ngram\n",
    "ngram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipe_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipe_lemma.split(' brk ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_analyzer_tester(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = full_df[\"ingredients\"][0:50].apply(\"|\".join)\n",
    "\n",
    "\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'preprocessor':stanza_preprocessor,\n",
    "    'tokenizer':stanza_lemmatizer, # out of memory \n",
    "    # 'stop_words':flushtrated_list,\n",
    "    'analyzer': gen_analyzer(1, 4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "vectorizer_model = CountVectorizer(**cv_params)\n",
    "\n",
    "test_transform = vectorizer_model.fit_transform(temp)#:5])\n",
    "vectorizer_model.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the warnings (preprocessor not used since analyzer is callable and tokenizer not used since analyzer is not word), may need to move preprocessing into analyzer. And based on [this](https://stackoverflow.com/questions/63185843/scikit-learn-countvectorizer-customizing-preprocessor-tokenizer-and-analyzer), will need to incorporate stopwords into the analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = full_df[\"ingredients\"][0:500]#.apply(\"|\".join)\n",
    "\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': gen_analyzer_2(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "vectorizer_model = CountVectorizer(**cv_params)\n",
    "\n",
    "test_transform = vectorizer_model.fit_transform(tqdm(full_df[\"ingredients\"]))\n",
    "# test_transform = vectorizer_model.fit_transform(temp)\n",
    "vectorizer_model.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model.transform(full_df[\"ingredients\"][1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': gen_analyzer_2(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "tfidf_vectorizer_model = TfidfVectorizer(**cv_params)\n",
    "\n",
    "test_tfidf_transform = tfidf_vectorizer_model.fit_transform(tqdm(full_df[\"ingredients\"]))\n",
    "# test_transform = vectorizer_model.fit_transform(temp)\n",
    "# tfidf_vectorizer_model.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_matrix = pd.DataFrame(\n",
    "        test_tfidf_transform.toarray(), columns=tfidf_vectorizer_model.get_feature_names_out(), index=full_df.index\n",
    "    )\n",
    "\n",
    "with open(\"../joblib/tfidf_transformer.joblib\", \"wb\") as fo:\n",
    "          joblib.dump(tfidf_vectorizer_model, fo, compress=True)\n",
    "\n",
    "with open(\"../joblib/database_word_matrix.joblib\", \"wb\") as fo:\n",
    "          joblib.dump(word_matrix, fo, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34656/34656 [6:35:04<00:00,  1.46it/s]   \n"
     ]
    }
   ],
   "source": [
    "# stz_wrpper = stz_wrp.StanzaWrapper()\n",
    "\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "tfidf_vectorizer_model = TfidfVectorizer(**cv_params)\n",
    "\n",
    "test_tfidf_transform = tfidf_vectorizer_model.fit_transform(tqdm(full_df[\"ingredients\"]))\n",
    "\n",
    "word_matrix = pd.DataFrame(\n",
    "        test_tfidf_transform.toarray(), columns=tfidf_vectorizer_model.get_feature_names_out(), index=full_df[\"ingredients\"].index\n",
    "    )\n",
    "\n",
    "with open(\"../joblib/tfidf_transformer.pkl\", \"wb\") as fo:\n",
    "          pickle.dump(tfidf_vectorizer_model, fo)\n",
    "\n",
    "with open(\"../joblib/database_word_matrix.pkl\", \"wb\") as fo:\n",
    "          pickle.dump(word_matrix, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:43<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(experiment_id=get_experiment_id(\"stanza_quadgrams_small_set_v1\")):\n",
    "    cv_params = {\n",
    "        'strip_accents':\"unicode\",\n",
    "        'lowercase':True,\n",
    "        'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "        'min_df':10,\n",
    "    }\n",
    "\n",
    "    pipeline_params = {\n",
    "        'stanza_model': 'en',\n",
    "        'language': 'english',\n",
    "        'sklearn-transformer': 'TfidfVectorizer'\n",
    "    }\n",
    "\n",
    "    pipeline_params.update(cv_params)\n",
    "\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    tfidf_vectorizer_model = TfidfVectorizer(**cv_params)\n",
    "\n",
    "    test_tfidf_transform = tfidf_vectorizer_model.fit_transform(tqdm(full_df[\"ingredients\"][0:50]))\n",
    "\n",
    "    word_matrix = pd.DataFrame(\n",
    "            test_tfidf_transform.toarray(), columns=tfidf_vectorizer_model.get_feature_names_out(), index=full_df[\"ingredients\"][0:50].index\n",
    "        )\n",
    "\n",
    "    with open(\"../joblib/tfidf_transformer_small_test.pkl\", \"wb\") as fo:\n",
    "            pickle.dump(tfidf_vectorizer_model, fo)\n",
    "            # mlflow.log_artifact(\"../joblib/database_word_matrix.pkl\")\n",
    "\n",
    "    with open(\"../joblib/database_word_matrix_small_test.pkl\", \"wb\") as fo:\n",
    "            pickle.dump(word_matrix, fo)\n",
    "            # mlflow.log_artifact(\"../joblib/database_word_matrix.pkl\")\n",
    "\n",
    "    mlflow.log_artifacts(\"../joblib/\", artifact_path=\"sklearn_dill_pkls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id=get_experiment_id(\"stanza_quadgrams_v1\")):\n",
    "    cv_params = {\n",
    "        'strip_accents':\"unicode\",\n",
    "        'lowercase':True,\n",
    "        'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "        'min_df':10,\n",
    "    }\n",
    "\n",
    "    pipeline_params = {\n",
    "        'stanza_model': 'en',\n",
    "        'language': 'english',\n",
    "        'sklearn-transformer': 'TfidfVectorizer'\n",
    "    }\n",
    "\n",
    "    pipeline_params.update(cv_params)\n",
    "\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # tfidf_vectorizer_model = TfidfVectorizer(**cv_params)\n",
    "\n",
    "    # test_tfidf_transform = tfidf_vectorizer_model.fit_transform(tqdm(full_df[\"ingredients\"]))\n",
    "\n",
    "    # word_matrix = pd.DataFrame(\n",
    "    #         test_tfidf_transform.toarray(), columns=tfidf_vectorizer_model.get_feature_names_out(), index=full_df[\"ingredients\"].index\n",
    "    #     )\n",
    "\n",
    "    # with open(\"../joblib/tfidf_transformer.pkl\", \"wb\") as fo:\n",
    "    #         pickle.dump(tfidf_vectorizer_model, fo)\n",
    "    #         # mlflow.log_artifact(\"../joblib/database_word_matrix.pkl\")\n",
    "\n",
    "    # with open(\"../joblib/database_word_matrix.pkl\", \"wb\") as fo:\n",
    "    #         pickle.dump(word_matrix, fo)\n",
    "    #         # mlflow.log_artifact(\"../joblib/database_word_matrix.pkl\")\n",
    "\n",
    "    mlflow.log_artifacts(\"../joblib/\", artifact_path=\"sklearn_dill_pkls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# was getting an error (TypeError: sequence item 37: expected str instance, NoneType found) saying that some recipes have NoneType, and wondered if there were recipes with no ingredients for some reason\n",
    "full_df[full_df[\"ingredients\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"ingredients\"][34:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = full_df['ingredients'][0:5].apply(\" brk \".join).str.lower()\n",
    "subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preproc = full_df['ingredients'][0:5].apply(stanza_preprocessor, args=(nlp,))\n",
    "test_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = stanza_preprocessor(nlp, full_df['ingredients'][0:5][0])\n",
    "preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmad = stanza_lemmatizer(preproc)\n",
    "lemmad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['prepSteps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['prepSteps'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_steps = full_df['prepSteps'].apply(\" \".join).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-calculating sentence mebeddings\n",
    "# embedding_model_params = {'embedding_model': 'all-MiniLM-L6-v2'}\n",
    "# embedding_model = SentenceTransformer(embedding_model_params['embedding_model'])\n",
    "# embeddings = embedding_model.encode(recipe_steps, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify UMAP dimensionality reductions\n",
    "# umap_model_params = {'n_neighbors':15, 'n_components':10, 'random_state':200}\n",
    "# umap_model = UMAP(**umap_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster with HDBSCAN\n",
    "# hdbscan_model_params = {'min_cluster_size':200, 'prediction_data':True}\n",
    "# hdbscan_model = HDBSCAN(**hdbscan_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding custom count vectorization\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    # 'preprocessor':custom_preprocessor,\n",
    "    # 'tokenizer':custom_lemmatizer, # out of memory \n",
    "    'stop_words':flushtrated_list,\n",
    "    'token_pattern':r\"(?u)\\b[a-zA-Z]{2,}\\b\",\n",
    "    'ngram_range':(1, 4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "vectorizer_model = CountVectorizer(**cv_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with open('../data/processed/bertopic_params.joblib', 'w') as fp:\n",
    "# pipeline_params = {\n",
    "#     'embedding':{'pretrained_sentence_embeddings': embedding_model_params},\n",
    "#     'dimension_reduction': {'UMAP': umap_model_params},\n",
    "#     'clustering': {'HDBSCAN': hdbscan_model_params},\n",
    "#     'vectorizer': {'sklearn_countvectorizer': cv_params},\n",
    "# }\n",
    "# joblib.dump(pipeline_params, '../data/processed/bertopic_params.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id=get_experiment_id(\"initial_explicit_spec_run_3\")):\n",
    "    pipeline_params = {\n",
    "        'language':'english',\n",
    "        'top_n_words':20,\n",
    "        'n_gram_range':(1, 4),\n",
    "        'min_topic_size':500,\n",
    "        'nr_topics':'auto',\n",
    "        'verbose':True,\n",
    "        'low_memory':True,\n",
    "        'calculate_probabilities':True\n",
    "    }\n",
    "    mlflow.log_params(pipeline_params)\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        **pipeline_params,\n",
    "        vectorizer_model=vectorizer_model\n",
    "    )\n",
    "    # TOKENIZERS_PARALLELISM=False\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(recipe_steps)\n",
    "\n",
    "    topic_model.get_topic_info().to_json('../data/processed/topic_model_df.json')\n",
    "\n",
    "    # mlflow.log_artifact('../data/processed/bertopic_params.joblib')\n",
    "    mlflow.log_artifact('../data/processed/topic_model_df.json')\n",
    "\n",
    "    print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_full(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()['Representation']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | export\n",
    "def foo():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# import nbdev\n",
    "\n",
    "# nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
