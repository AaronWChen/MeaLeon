{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Stanza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ee85da024347019ce35b218515ff98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 22:31:21 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-11-06 22:31:22 INFO: File exists: /home/awchen/stanza_resources/en/default.zip\n",
      "2023-11-06 22:31:25 INFO: Finished downloading models and saved to /home/awchen/stanza_resources.\n",
      "2023-11-06 22:31:25 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cf687184e44923bd18754040580f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 22:31:25 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2023-11-06 22:31:25 INFO: Using device: cuda\n",
      "2023-11-06 22:31:25 INFO: Loading: tokenize\n",
      "2023-11-06 22:31:27 INFO: Loading: pos\n",
      "2023-11-06 22:31:28 INFO: Loading: lemma\n",
      "2023-11-06 22:31:28 INFO: Loading: constituency\n",
      "2023-11-06 22:31:28 INFO: Loading: depparse\n",
      "2023-11-06 22:31:28 INFO: Loading: sentiment\n",
      "2023-11-06 22:31:28 INFO: Loading: ner\n",
      "2023-11-06 22:31:29 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *\n",
    "import project_path\n",
    "\n",
    "# from bertopic import BERTopic\n",
    "import dagshub\n",
    "dagshub.init(repo_name='MeaLeon', repo_owner='AaronWChen')\n",
    "# from datetime import datetime\n",
    "# from hdbscan import HDBSCAN\n",
    "from itertools import tee, islice\n",
    "import joblib \n",
    "import json\n",
    "# import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "# from sklearn.base import TransformerMixin\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfTransformer,\n",
    "    TfidfVectorizer,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# import spacy\n",
    "# import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# from spacy.lemmatizer import Lemmatizer\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "# from umap import UMAP\n",
    "\n",
    "# import local scripts\n",
    "# import src.nlp_processor as nlpp\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@markdown Enter the username of your DAGsHub account:\n",
    "DAGSHUB_USER_NAME = \"AaronWChen\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the email for your DAGsHub account:\n",
    "DAGSHUB_EMAIL = \"awc33@cornell.edu\" #@param {type:\"string\"}\n",
    "\n",
    "DAGSHUB_REPO_NAME=\"MeaLeon\"\n",
    "BRANCH=\"STANZA-1/refactor-nltk-stanza\"\n",
    "dagshub.init(repo_name='MeaLeon', repo_owner='AaronWChen')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_id(name):\n",
    "    exp = mlflow.get_experiment_by_name(name)\n",
    "    if exp is None:\n",
    "      exp_id = mlflow.create_experiment(name)\n",
    "      return exp_id\n",
    "    return exp.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = '../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "food_stopwords_path = \"../food_stopwords.csv\"\n",
    "\n",
    "joblib_basepath = '../joblib/2022.08.23/'\n",
    "\n",
    "cv_path = joblib_basepath + 'countvec.joblib'\n",
    "tfidf_path = joblib_basepath + 'tfidf.joblib'\n",
    "full_df_path = joblib_basepath + 'recipes_with_cv.joblib'\n",
    "reduced_df_path = joblib_basepath + 'reduced_df.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is a redeem for variable naming mixed with a free pun-ish me daddy, flushtrated will be the list of all stopword to exclude so named because we're throwing these words down the drain\n",
    "\n",
    "flushtrated = {x for x in pd.read_csv(food_stopwords_path)}\n",
    "additional_to_exclude = {\n",
    "    \"red\",\n",
    "    \"green\",\n",
    "    \"black\",\n",
    "    \"yellow\",\n",
    "    \"white\",\n",
    "    \"inch\",\n",
    "    \"mince\",\n",
    "    \"chop\",\n",
    "    \"fry\",\n",
    "    \"trim\",\n",
    "    \"flat\",\n",
    "    \"beat\",\n",
    "    \"brown\",\n",
    "    \"golden\",\n",
    "    \"balsamic\",\n",
    "    \"halve\",\n",
    "    \"blue\",\n",
    "    \"divide\",\n",
    "    \"trim\",\n",
    "    \"unbleache\",\n",
    "    \"granulate\",\n",
    "    \"Frank\",\n",
    "    \"alternative\",\n",
    "    \"american\",\n",
    "    \"annie\",\n",
    "    \"asian\",\n",
    "    \"balance\",\n",
    "    \"band\",\n",
    "    \"barrel\",\n",
    "    \"bay\",\n",
    "    \"bayou\",\n",
    "    \"beam\",\n",
    "    \"beard\",\n",
    "    \"bell\",\n",
    "    \"betty\",\n",
    "    \"bird\",\n",
    "    \"blast\",\n",
    "    \"bob\",\n",
    "    \"bone\",\n",
    "    \"breyers\",\n",
    "    \"calore\",\n",
    "    \"carb\",\n",
    "    \"card\",\n",
    "    \"chachere\",\n",
    "    \"change\",\n",
    "    \"circle\",\n",
    "    \"coffee\",\n",
    "    \"coil\",\n",
    "    \"country\",\n",
    "    \"cow\",\n",
    "    \"crack\",\n",
    "    \"cracker\",\n",
    "    \"crocker\",\n",
    "    \"crystal\",\n",
    "    \"dean\",\n",
    "    \"degree\",\n",
    "    \"deluxe\",\n",
    "    \"direction\",\n",
    "    \"duncan\",\n",
    "    \"earth\",\n",
    "    \"eggland\",\n",
    "    \"ener\",\n",
    "    \"envelope\",\n",
    "    \"eye\",\n",
    "    \"fantastic\",\n",
    "    \"far\",\n",
    "    \"fat\",\n",
    "    \"feather\",\n",
    "    \"flake\",\n",
    "    \"foot\",\n",
    "    \"fourth\",\n",
    "    \"frank\",\n",
    "    \"french\",\n",
    "    \"fusion\",\n",
    "    \"genoa\",\n",
    "    \"genovese\",\n",
    "    \"germain\",\n",
    "    \"giada\",\n",
    "    \"gold\",\n",
    "    \"granule\",\n",
    "    \"greek\",\n",
    "    \"hamburger\",\n",
    "    \"helper\",\n",
    "    \"herbe\",\n",
    "    \"hines\",\n",
    "    \"hodgson\",\n",
    "    \"hunt\",\n",
    "    \"instruction\",\n",
    "    \"interval\",\n",
    "    \"italianstyle\",\n",
    "    \"jim\",\n",
    "    \"jimmy\",\n",
    "    \"kellogg\",\n",
    "    \"lagrille\",\n",
    "    \"lake\",\n",
    "    \"land\",\n",
    "    \"laurentiis\",\n",
    "    \"lawry\",\n",
    "    \"lipton\",\n",
    "    \"litre\",\n",
    "    \"ll\",\n",
    "    \"maid\",\n",
    "    \"malt\",\n",
    "    \"mate\",\n",
    "    \"mayer\",\n",
    "    \"meal\",\n",
    "    \"medal\",\n",
    "    \"medallion\",\n",
    "    \"member\",\n",
    "    \"mexicanstyle\",\n",
    "    \"monte\",\n",
    "    \"mori\",\n",
    "    \"nest\",\n",
    "    \"nu\",\n",
    "    \"oounce\",\n",
    "    \"oscar\",\n",
    "    \"ox\",\n",
    "    \"paso\",\n",
    "    \"pasta\",\n",
    "    \"patty\",\n",
    "    \"petal\",\n",
    "    \"pinche\",\n",
    "    \"preserve\",\n",
    "    \"quartere\",\n",
    "    \"ranch\",\n",
    "    \"ranchstyle\",\n",
    "    \"rasher\",\n",
    "    \"redhot\",\n",
    "    \"resemble\",\n",
    "    \"rice\",\n",
    "    \"ro\",\n",
    "    \"roni\",\n",
    "    \"scissor\",\n",
    "    \"scrap\",\n",
    "    \"secret\",\n",
    "    \"semicircle\",\n",
    "    \"shard\",\n",
    "    \"shear\",\n",
    "    \"sixth\",\n",
    "    \"sliver\",\n",
    "    \"smucker\",\n",
    "    \"snicker\",\n",
    "    \"source\",\n",
    "    \"spot\",\n",
    "    \"state\",\n",
    "    \"strand\",\n",
    "    \"sun\",\n",
    "    \"supreme\",\n",
    "    \"tablepoon\",\n",
    "    \"tail\",\n",
    "    \"target\",\n",
    "    \"tm\",\n",
    "    \"tong\",\n",
    "    \"toothpick\",\n",
    "    \"triangle\",\n",
    "    \"trimming\",\n",
    "    \"tweezer\",\n",
    "    \"valley\",\n",
    "    \"vay\",\n",
    "    \"wise\",\n",
    "    \"wishbone\",\n",
    "    \"wrapper\",\n",
    "    \"yoplait\",\n",
    "    \"ziploc\",\n",
    "}\n",
    "\n",
    "flushtrated = flushtrated.union(STOP_WORDS)\n",
    "flushtrated = flushtrated.union(additional_to_exclude)\n",
    "flushtrated_list = list(flushtrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dek</th>\n",
       "      <th>hed</th>\n",
       "      <th>aggregateRating</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>prepSteps</th>\n",
       "      <th>reviewsCount</th>\n",
       "      <th>willMakeAgainPct</th>\n",
       "      <th>cuisine_name</th>\n",
       "      <th>photo_filename</th>\n",
       "      <th>...</th>\n",
       "      <th>zest pith</th>\n",
       "      <th>zest vegetable</th>\n",
       "      <th>zinfandel</th>\n",
       "      <th>ziti</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zucchini blossom</th>\n",
       "      <th>zucchini crookneck</th>\n",
       "      <th>zucchini squash</th>\n",
       "      <th>árbol</th>\n",
       "      <th>árbol pepper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54a2b6b019925f464b373351</td>\n",
       "      <td>How does fried chicken achieve No. 1 status? B...</td>\n",
       "      <td>Pickle-Brined Fried Chicken</td>\n",
       "      <td>3.11</td>\n",
       "      <td>[1 tablespoons yellow mustard seeds, 1 tablesp...</td>\n",
       "      <td>[Toast mustard and coriander seeds in a dry me...</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>51247610_fried-chicken_1x1.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54a408a019925f464b3733bc</td>\n",
       "      <td>Spinaci all'Ebraica</td>\n",
       "      <td>Spinach Jewish Style</td>\n",
       "      <td>3.22</td>\n",
       "      <td>[3 pounds small-leaved bulk spinach, Salt, 1/2...</td>\n",
       "      <td>[Remove the stems and roots from the spinach. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>Italian</td>\n",
       "      <td>EP_12162015_placeholders_rustic.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54a408a26529d92b2c003631</td>\n",
       "      <td>This majestic, moist, and richly spiced honey ...</td>\n",
       "      <td>New Year’s Honey Cake</td>\n",
       "      <td>3.62</td>\n",
       "      <td>[3 1/2 cups all-purpose flour, 1 tablespoon ba...</td>\n",
       "      <td>[I like this cake best baked in a 9-inch angel...</td>\n",
       "      <td>105</td>\n",
       "      <td>88</td>\n",
       "      <td>Kosher</td>\n",
       "      <td>EP_09022015_honeycake-2.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54a408a66529d92b2c003638</td>\n",
       "      <td>The idea for this sandwich came to me when my ...</td>\n",
       "      <td>The B.L.A.Bagel with Lox and Avocado</td>\n",
       "      <td>4.00</td>\n",
       "      <td>[1 small ripe avocado, preferably Hass (see No...</td>\n",
       "      <td>[A short time before serving, mash avocado and...</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>Kosher</td>\n",
       "      <td>EP_12162015_placeholders_casual.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54a408a719925f464b3733cc</td>\n",
       "      <td>In 1930, Simon Agranat, the chief justice of t...</td>\n",
       "      <td>Shakshuka a la Doktor Shakshuka</td>\n",
       "      <td>2.71</td>\n",
       "      <td>[2 pounds fresh tomatoes, unpeeled and cut in ...</td>\n",
       "      <td>[1. Place the tomatoes, garlic, salt, paprika,...</td>\n",
       "      <td>7</td>\n",
       "      <td>83</td>\n",
       "      <td>Kosher</td>\n",
       "      <td>EP_12162015_placeholders_formal.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34751</th>\n",
       "      <td>59541a31bff3052847ae2107</td>\n",
       "      <td>Buttering the bread before you waffle it ensur...</td>\n",
       "      <td>Waffled Ham and Cheese Melt with Maple Butter</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[1 tablespoon unsalted butter, at room tempera...</td>\n",
       "      <td>[Preheat the waffle iron on low., Spread a thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>waffle-ham-and-cheese-melt-062817.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34752</th>\n",
       "      <td>5954233ad52ca90dc28200e7</td>\n",
       "      <td>Spread this easy compound butter on waffles, p...</td>\n",
       "      <td>Maple Butter</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[8 tablespoons (1 stick) salted butter, at roo...</td>\n",
       "      <td>[Combine the ingredients in a medium-size bowl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>EP_12162015_placeholders_bright.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34753</th>\n",
       "      <td>595424c2109c972493636f83</td>\n",
       "      <td>Leftover mac and cheese is not exactly one of ...</td>\n",
       "      <td>Waffled Macaroni and Cheese</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[3 tablespoons unsalted butter, plus more for ...</td>\n",
       "      <td>[Preheat the oven to 375°F. Butter a 9x5-inch ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>waffle-mac-n-cheese-062816.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34754</th>\n",
       "      <td>5956638625dc3d1d829b7166</td>\n",
       "      <td>A classic Mexican beer cocktail you can sip al...</td>\n",
       "      <td>Classic Michelada</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[Coarse salt, 2 lime wedges, 2 ounces tomato j...</td>\n",
       "      <td>[Place about 1/4 cup salt on a small plate. Ru...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>Classic Michelada 07292017.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34755</th>\n",
       "      <td>59566daa25dc3d1d829b7169</td>\n",
       "      <td>A grapefruit beer that's one of the most refre...</td>\n",
       "      <td>So Radler</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[1 bottle (375 ml) sour beer, such as Almanac ...</td>\n",
       "      <td>[Combine the water, honey, rosemary, and grape...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Cuisine</td>\n",
       "      <td>EP_12162015_placeholders_bright.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34656 rows × 3365 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  \\\n",
       "0      54a2b6b019925f464b373351   \n",
       "1      54a408a019925f464b3733bc   \n",
       "2      54a408a26529d92b2c003631   \n",
       "3      54a408a66529d92b2c003638   \n",
       "4      54a408a719925f464b3733cc   \n",
       "...                         ...   \n",
       "34751  59541a31bff3052847ae2107   \n",
       "34752  5954233ad52ca90dc28200e7   \n",
       "34753  595424c2109c972493636f83   \n",
       "34754  5956638625dc3d1d829b7166   \n",
       "34755  59566daa25dc3d1d829b7169   \n",
       "\n",
       "                                                     dek  \\\n",
       "0      How does fried chicken achieve No. 1 status? B...   \n",
       "1                                    Spinaci all'Ebraica   \n",
       "2      This majestic, moist, and richly spiced honey ...   \n",
       "3      The idea for this sandwich came to me when my ...   \n",
       "4      In 1930, Simon Agranat, the chief justice of t...   \n",
       "...                                                  ...   \n",
       "34751  Buttering the bread before you waffle it ensur...   \n",
       "34752  Spread this easy compound butter on waffles, p...   \n",
       "34753  Leftover mac and cheese is not exactly one of ...   \n",
       "34754  A classic Mexican beer cocktail you can sip al...   \n",
       "34755  A grapefruit beer that's one of the most refre...   \n",
       "\n",
       "                                                 hed  aggregateRating  \\\n",
       "0                        Pickle-Brined Fried Chicken             3.11   \n",
       "1                               Spinach Jewish Style             3.22   \n",
       "2                              New Year’s Honey Cake             3.62   \n",
       "3              The B.L.A.Bagel with Lox and Avocado             4.00   \n",
       "4                    Shakshuka a la Doktor Shakshuka             2.71   \n",
       "...                                              ...              ...   \n",
       "34751  Waffled Ham and Cheese Melt with Maple Butter             0.00   \n",
       "34752                                   Maple Butter             0.00   \n",
       "34753                    Waffled Macaroni and Cheese             0.00   \n",
       "34754                              Classic Michelada             0.00   \n",
       "34755                                      So Radler             0.00   \n",
       "\n",
       "                                             ingredients  \\\n",
       "0      [1 tablespoons yellow mustard seeds, 1 tablesp...   \n",
       "1      [3 pounds small-leaved bulk spinach, Salt, 1/2...   \n",
       "2      [3 1/2 cups all-purpose flour, 1 tablespoon ba...   \n",
       "3      [1 small ripe avocado, preferably Hass (see No...   \n",
       "4      [2 pounds fresh tomatoes, unpeeled and cut in ...   \n",
       "...                                                  ...   \n",
       "34751  [1 tablespoon unsalted butter, at room tempera...   \n",
       "34752  [8 tablespoons (1 stick) salted butter, at roo...   \n",
       "34753  [3 tablespoons unsalted butter, plus more for ...   \n",
       "34754  [Coarse salt, 2 lime wedges, 2 ounces tomato j...   \n",
       "34755  [1 bottle (375 ml) sour beer, such as Almanac ...   \n",
       "\n",
       "                                               prepSteps  reviewsCount  \\\n",
       "0      [Toast mustard and coriander seeds in a dry me...             7   \n",
       "1      [Remove the stems and roots from the spinach. ...             5   \n",
       "2      [I like this cake best baked in a 9-inch angel...           105   \n",
       "3      [A short time before serving, mash avocado and...             7   \n",
       "4      [1. Place the tomatoes, garlic, salt, paprika,...             7   \n",
       "...                                                  ...           ...   \n",
       "34751  [Preheat the waffle iron on low., Spread a thi...             0   \n",
       "34752  [Combine the ingredients in a medium-size bowl...             0   \n",
       "34753  [Preheat the oven to 375°F. Butter a 9x5-inch ...             0   \n",
       "34754  [Place about 1/4 cup salt on a small plate. Ru...             0   \n",
       "34755  [Combine the water, honey, rosemary, and grape...             0   \n",
       "\n",
       "       willMakeAgainPct     cuisine_name  \\\n",
       "0                   100  Missing Cuisine   \n",
       "1                    80          Italian   \n",
       "2                    88           Kosher   \n",
       "3                   100           Kosher   \n",
       "4                    83           Kosher   \n",
       "...                 ...              ...   \n",
       "34751                 0  Missing Cuisine   \n",
       "34752                 0  Missing Cuisine   \n",
       "34753                 0  Missing Cuisine   \n",
       "34754                 0  Missing Cuisine   \n",
       "34755                 0  Missing Cuisine   \n",
       "\n",
       "                              photo_filename  ... zest pith zest vegetable  \\\n",
       "0             51247610_fried-chicken_1x1.jpg  ...       0.0            0.0   \n",
       "1        EP_12162015_placeholders_rustic.jpg  ...       0.0            0.0   \n",
       "2                EP_09022015_honeycake-2.jpg  ...       0.0            0.0   \n",
       "3        EP_12162015_placeholders_casual.jpg  ...       0.0            0.0   \n",
       "4        EP_12162015_placeholders_formal.jpg  ...       0.0            0.0   \n",
       "...                                      ...  ...       ...            ...   \n",
       "34751  waffle-ham-and-cheese-melt-062817.jpg  ...       0.0            0.0   \n",
       "34752    EP_12162015_placeholders_bright.jpg  ...       0.0            0.0   \n",
       "34753         waffle-mac-n-cheese-062816.jpg  ...       0.0            0.0   \n",
       "34754         Classic Michelada 07292017.jpg  ...       0.0            0.0   \n",
       "34755    EP_12162015_placeholders_bright.jpg  ...       0.0            0.0   \n",
       "\n",
       "      zinfandel ziti  zucchini  zucchini blossom  zucchini crookneck  \\\n",
       "0           0.0  0.0       0.0               0.0                 0.0   \n",
       "1           0.0  0.0       0.0               0.0                 0.0   \n",
       "2           0.0  0.0       0.0               0.0                 0.0   \n",
       "3           0.0  0.0       0.0               0.0                 0.0   \n",
       "4           0.0  0.0       0.0               0.0                 0.0   \n",
       "...         ...  ...       ...               ...                 ...   \n",
       "34751       0.0  0.0       0.0               0.0                 0.0   \n",
       "34752       0.0  0.0       0.0               0.0                 0.0   \n",
       "34753       0.0  0.0       0.0               0.0                 0.0   \n",
       "34754       0.0  0.0       0.0               0.0                 0.0   \n",
       "34755       0.0  0.0       0.0               0.0                 0.0   \n",
       "\n",
       "       zucchini squash  árbol  árbol pepper  \n",
       "0                  0.0    0.0           0.0  \n",
       "1                  0.0    0.0           0.0  \n",
       "2                  0.0    0.0           0.0  \n",
       "3                  0.0    0.0           0.0  \n",
       "4                  0.0    0.0           0.0  \n",
       "...                ...    ...           ...  \n",
       "34751              0.0    0.0           0.0  \n",
       "34752              0.0    0.0           0.0  \n",
       "34753              0.0    0.0           0.0  \n",
       "34754              0.0    0.0           0.0  \n",
       "34755              0.0    0.0           0.0  \n",
       "\n",
       "[34656 rows x 3365 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = joblib.load(full_df_path)\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 tablespoons yellow mustard seeds',\n",
       " '1 tablespoons brown mustard seeds',\n",
       " '1 1/2 teaspoons coriander seeds',\n",
       " '1 cup apple cider vinegar',\n",
       " '2/3 cup kosher salt',\n",
       " '1/3 cup sugar',\n",
       " '1/4 cup chopped fresh dill',\n",
       " '8 skinless, boneless chicken thighs (about 3 pounds), halved, quartered if large',\n",
       " 'Vegetable oil (for frying; about 10 cups)',\n",
       " '2 cups buttermilk',\n",
       " '2 cups all-purpose flour',\n",
       " 'Kosher salt',\n",
       " 'Honey, flaky sea salt (such as Maldon), toasted benne or sesame seeds, hot sauce (for serving)',\n",
       " 'A deep-fry thermometer']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['ingredients'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 tablespoons yellow mustard seeds. 1 tablespoons brown mustard seeds. 1 1/2 teaspoons coriander seeds. 1 cup apple cider vinegar. 2/3 cup kosher salt. 1/3 cup sugar. 1/4 cup chopped fresh dill. 8 skinless, boneless chicken thighs (about 3 pounds), halved, quartered if large. vegetable oil (for frying; about 10 cups). 2 cups buttermilk. 2 cups all-purpose flour. kosher salt. honey, flaky sea salt (such as maldon), toasted benne or sesame seeds, hot sauce (for serving). a deep-fry thermometer'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_recipe = \". \".join(full_df['ingredients'][0]).lower()\n",
    "test_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(test_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: 1\n",
      "id: (2,)\ttext: tablespoons\n",
      "id: (3,)\ttext: yellow\n",
      "id: (4,)\ttext: mustard\n",
      "id: (5,)\ttext: seeds\n",
      "id: (6,)\ttext: .\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: 1\n",
      "id: (2,)\ttext: tablespoons\n",
      "id: (3,)\ttext: brown\n",
      "id: (4,)\ttext: mustard\n",
      "id: (5,)\ttext: seeds\n",
      "id: (6,)\ttext: .\n",
      "====== Sentence 3 tokens =======\n",
      "id: (1,)\ttext: 1\n",
      "id: (2,)\ttext: 1/2\n",
      "id: (3,)\ttext: teaspoons\n",
      "id: (4,)\ttext: coriander\n",
      "id: (5,)\ttext: seeds\n",
      "id: (6,)\ttext: .\n",
      "====== Sentence 4 tokens =======\n",
      "id: (1,)\ttext: 1\n",
      "id: (2,)\ttext: cup\n",
      "id: (3,)\ttext: apple\n",
      "id: (4,)\ttext: cider\n",
      "id: (5,)\ttext: vinegar\n",
      "id: (6,)\ttext: .\n",
      "====== Sentence 5 tokens =======\n",
      "id: (1,)\ttext: 2/3\n",
      "id: (2,)\ttext: cup\n",
      "id: (3,)\ttext: kosher\n",
      "id: (4,)\ttext: salt\n",
      "id: (5,)\ttext: .\n",
      "====== Sentence 6 tokens =======\n",
      "id: (1,)\ttext: 1/3\n",
      "id: (2,)\ttext: cup\n",
      "id: (3,)\ttext: sugar\n",
      "id: (4,)\ttext: .\n",
      "====== Sentence 7 tokens =======\n",
      "id: (1,)\ttext: 1/4\n",
      "id: (2,)\ttext: cup\n",
      "id: (3,)\ttext: chopped\n",
      "id: (4,)\ttext: fresh\n",
      "id: (5,)\ttext: dill\n",
      "id: (6,)\ttext: .\n",
      "====== Sentence 8 tokens =======\n",
      "id: (1,)\ttext: 8\n",
      "id: (2,)\ttext: skinless\n",
      "id: (3,)\ttext: ,\n",
      "id: (4,)\ttext: boneless\n",
      "id: (5,)\ttext: chicken\n",
      "id: (6,)\ttext: thighs\n",
      "id: (7,)\ttext: (\n",
      "id: (8,)\ttext: about\n",
      "id: (9,)\ttext: 3\n",
      "id: (10,)\ttext: pounds\n",
      "id: (11,)\ttext: )\n",
      "id: (12,)\ttext: ,\n",
      "id: (13,)\ttext: halved\n",
      "id: (14,)\ttext: ,\n",
      "id: (15,)\ttext: quartered\n",
      "id: (16,)\ttext: if\n",
      "id: (17,)\ttext: large\n",
      "id: (18,)\ttext: .\n",
      "====== Sentence 9 tokens =======\n",
      "id: (1,)\ttext: vegetable\n",
      "id: (2,)\ttext: oil\n",
      "id: (3,)\ttext: (\n",
      "id: (4,)\ttext: for\n",
      "id: (5,)\ttext: frying\n",
      "id: (6,)\ttext: ;\n",
      "id: (7,)\ttext: about\n",
      "id: (8,)\ttext: 10\n",
      "id: (9,)\ttext: cups\n",
      "id: (10,)\ttext: )\n",
      "id: (11,)\ttext: .\n",
      "====== Sentence 10 tokens =======\n",
      "id: (1,)\ttext: 2\n",
      "id: (2,)\ttext: cups\n",
      "id: (3,)\ttext: buttermilk\n",
      "id: (4,)\ttext: .\n",
      "====== Sentence 11 tokens =======\n",
      "id: (1,)\ttext: 2\n",
      "id: (2,)\ttext: cups\n",
      "id: (3,)\ttext: all\n",
      "id: (4,)\ttext: -\n",
      "id: (5,)\ttext: purpose\n",
      "id: (6,)\ttext: flour\n",
      "id: (7,)\ttext: .\n",
      "====== Sentence 12 tokens =======\n",
      "id: (1,)\ttext: kosher\n",
      "id: (2,)\ttext: salt\n",
      "id: (3,)\ttext: .\n",
      "====== Sentence 13 tokens =======\n",
      "id: (1,)\ttext: honey\n",
      "id: (2,)\ttext: ,\n",
      "id: (3,)\ttext: flaky\n",
      "id: (4,)\ttext: sea\n",
      "id: (5,)\ttext: salt\n",
      "id: (6,)\ttext: (\n",
      "id: (7,)\ttext: such\n",
      "id: (8,)\ttext: as\n",
      "id: (9,)\ttext: maldon\n",
      "id: (10,)\ttext: )\n",
      "id: (11,)\ttext: ,\n",
      "id: (12,)\ttext: toasted\n",
      "id: (13,)\ttext: benne\n",
      "id: (14,)\ttext: or\n",
      "id: (15,)\ttext: sesame\n",
      "id: (16,)\ttext: seeds\n",
      "id: (17,)\ttext: ,\n",
      "id: (18,)\ttext: hot\n",
      "id: (19,)\ttext: sauce\n",
      "id: (20,)\ttext: (\n",
      "id: (21,)\ttext: for\n",
      "id: (22,)\ttext: serving\n",
      "id: (23,)\ttext: )\n",
      "id: (24,)\ttext: .\n",
      "====== Sentence 14 tokens =======\n",
      "id: (1,)\ttext: a\n",
      "id: (2,)\ttext: deep\n",
      "id: (3,)\ttext: -\n",
      "id: (4,)\ttext: fry\n",
      "id: (5,)\ttext: thermometer\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 tablespoons yellow mustard seeds.', '1 tablespoons brown mustard seeds.', '1 1/2 teaspoons coriander seeds.', '1 cup apple cider vinegar.', '2/3 cup kosher salt.', '1/3 cup sugar.', '1/4 cup chopped fresh dill.', '8 skinless, boneless chicken thighs (about 3 pounds), halved, quartered if large.', 'vegetable oil (for frying; about 10 cups).', '2 cups buttermilk.', '2 cups all-purpose flour.', 'kosher salt.', 'honey, flaky sea salt (such as maldon), toasted benne or sesame seeds, hot sauce (for serving).', 'a deep-fry thermometer']\n"
     ]
    }
   ],
   "source": [
    "print([sentence.text for sentence in doc.sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: 1 \t \t lemma: 1, \t \t upos: NUM\n",
      "word: tablespoons \t \t lemma: tablespoon, \t \t upos: NOUN\n",
      "word: yellow \t \t lemma: yellow, \t \t upos: ADJ\n",
      "word: mustard \t \t lemma: mustard, \t \t upos: NOUN\n",
      "word: seeds \t \t lemma: seed, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 1 \t \t lemma: 1, \t \t upos: NUM\n",
      "word: tablespoons \t \t lemma: tablespoon, \t \t upos: NOUN\n",
      "word: brown \t \t lemma: brown, \t \t upos: ADJ\n",
      "word: mustard \t \t lemma: mustard, \t \t upos: NOUN\n",
      "word: seeds \t \t lemma: seed, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 1 \t \t lemma: 1, \t \t upos: NUM\n",
      "word: 1/2 \t \t lemma: 1/2, \t \t upos: NUM\n",
      "word: teaspoons \t \t lemma: teaspoon, \t \t upos: NOUN\n",
      "word: coriander \t \t lemma: coriander, \t \t upos: NOUN\n",
      "word: seeds \t \t lemma: seed, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 1 \t \t lemma: 1, \t \t upos: NUM\n",
      "word: cup \t \t lemma: cup, \t \t upos: NOUN\n",
      "word: apple \t \t lemma: apple, \t \t upos: NOUN\n",
      "word: cider \t \t lemma: cider, \t \t upos: NOUN\n",
      "word: vinegar \t \t lemma: vinegar, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 2/3 \t \t lemma: 2/3, \t \t upos: NUM\n",
      "word: cup \t \t lemma: cup, \t \t upos: NOUN\n",
      "word: kosher \t \t lemma: kosher, \t \t upos: NOUN\n",
      "word: salt \t \t lemma: salt, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 1/3 \t \t lemma: 1/3, \t \t upos: NUM\n",
      "word: cup \t \t lemma: cup, \t \t upos: NOUN\n",
      "word: sugar \t \t lemma: sugar, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 1/4 \t \t lemma: 1/4, \t \t upos: NUM\n",
      "word: cup \t \t lemma: cup, \t \t upos: NOUN\n",
      "word: chopped \t \t lemma: chop, \t \t upos: VERB\n",
      "word: fresh \t \t lemma: fresh, \t \t upos: ADJ\n",
      "word: dill \t \t lemma: dill, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 8 \t \t lemma: 8, \t \t upos: NUM\n",
      "word: skinless \t \t lemma: skinless, \t \t upos: NOUN\n",
      "word: , \t \t lemma: ,, \t \t upos: PUNCT\n",
      "word: boneless \t \t lemma: boneless, \t \t upos: ADJ\n",
      "word: chicken \t \t lemma: chicken, \t \t upos: NOUN\n",
      "word: thighs \t \t lemma: thigh, \t \t upos: NOUN\n",
      "word: ( \t \t lemma: (, \t \t upos: PUNCT\n",
      "word: about \t \t lemma: about, \t \t upos: ADV\n",
      "word: 3 \t \t lemma: 3, \t \t upos: NUM\n",
      "word: pounds \t \t lemma: pound, \t \t upos: NOUN\n",
      "word: ) \t \t lemma: ), \t \t upos: PUNCT\n",
      "word: , \t \t lemma: ,, \t \t upos: PUNCT\n",
      "word: halved \t \t lemma: halve, \t \t upos: VERB\n",
      "word: , \t \t lemma: ,, \t \t upos: PUNCT\n",
      "word: quartered \t \t lemma: quarter, \t \t upos: VERB\n",
      "word: if \t \t lemma: if, \t \t upos: SCONJ\n",
      "word: large \t \t lemma: large, \t \t upos: ADJ\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: vegetable \t \t lemma: vegetable, \t \t upos: NOUN\n",
      "word: oil \t \t lemma: oil, \t \t upos: NOUN\n",
      "word: ( \t \t lemma: (, \t \t upos: PUNCT\n",
      "word: for \t \t lemma: for, \t \t upos: ADP\n",
      "word: frying \t \t lemma: frying, \t \t upos: NOUN\n",
      "word: ; \t \t lemma: ;, \t \t upos: PUNCT\n",
      "word: about \t \t lemma: about, \t \t upos: ADV\n",
      "word: 10 \t \t lemma: 10, \t \t upos: NUM\n",
      "word: cups \t \t lemma: cup, \t \t upos: NOUN\n",
      "word: ) \t \t lemma: ), \t \t upos: PUNCT\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 2 \t \t lemma: 2, \t \t upos: NUM\n",
      "word: cups \t \t lemma: cup, \t \t upos: NOUN\n",
      "word: buttermilk \t \t lemma: buttermilk, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: 2 \t \t lemma: 2, \t \t upos: NUM\n",
      "word: cups \t \t lemma: cup, \t \t upos: NOUN\n",
      "word: all \t \t lemma: all, \t \t upos: DET\n",
      "word: - \t \t lemma: -, \t \t upos: PUNCT\n",
      "word: purpose \t \t lemma: purpose, \t \t upos: NOUN\n",
      "word: flour \t \t lemma: flour, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: kosher \t \t lemma: kosher, \t \t upos: NOUN\n",
      "word: salt \t \t lemma: salt, \t \t upos: NOUN\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: honey \t \t lemma: honey, \t \t upos: NOUN\n",
      "word: , \t \t lemma: ,, \t \t upos: PUNCT\n",
      "word: flaky \t \t lemma: flaky, \t \t upos: ADJ\n",
      "word: sea \t \t lemma: sea, \t \t upos: NOUN\n",
      "word: salt \t \t lemma: salt, \t \t upos: NOUN\n",
      "word: ( \t \t lemma: (, \t \t upos: PUNCT\n",
      "word: such \t \t lemma: such, \t \t upos: ADJ\n",
      "word: as \t \t lemma: as, \t \t upos: ADP\n",
      "word: maldon \t \t lemma: maldon, \t \t upos: NOUN\n",
      "word: ) \t \t lemma: ), \t \t upos: PUNCT\n",
      "word: , \t \t lemma: ,, \t \t upos: PUNCT\n",
      "word: toasted \t \t lemma: toast, \t \t upos: VERB\n",
      "word: benne \t \t lemma: benne, \t \t upos: NOUN\n",
      "word: or \t \t lemma: or, \t \t upos: CCONJ\n",
      "word: sesame \t \t lemma: sesame, \t \t upos: NOUN\n",
      "word: seeds \t \t lemma: seed, \t \t upos: NOUN\n",
      "word: , \t \t lemma: ,, \t \t upos: PUNCT\n",
      "word: hot \t \t lemma: hot, \t \t upos: ADJ\n",
      "word: sauce \t \t lemma: sauce, \t \t upos: NOUN\n",
      "word: ( \t \t lemma: (, \t \t upos: PUNCT\n",
      "word: for \t \t lemma: for, \t \t upos: ADP\n",
      "word: serving \t \t lemma: serving, \t \t upos: NOUN\n",
      "word: ) \t \t lemma: ), \t \t upos: PUNCT\n",
      "word: . \t \t lemma: ., \t \t upos: PUNCT\n",
      "word: a \t \t lemma: a, \t \t upos: DET\n",
      "word: deep \t \t lemma: deep, \t \t upos: ADJ\n",
      "word: - \t \t lemma: -, \t \t upos: PUNCT\n",
      "word: fry \t \t lemma: fry, \t \t upos: NOUN\n",
      "word: thermometer \t \t lemma: thermometer, \t \t upos: NOUN\n"
     ]
    }
   ],
   "source": [
    "print(*[f'word: {word.text+\" \"}\\t \\t lemma: {word.lemma}, \\t \\t upos: {word.upos}' for sent in doc.sentences for word in sent.words], sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma: tablespoon\n",
      "lemma: yellow\n",
      "lemma: mustard\n",
      "lemma: seed\n",
      "lemma: tablespoon\n",
      "lemma: brown\n",
      "lemma: mustard\n",
      "lemma: seed\n",
      "lemma: teaspoon\n",
      "lemma: coriander\n",
      "lemma: seed\n",
      "lemma: cup\n",
      "lemma: apple\n",
      "lemma: cider\n",
      "lemma: vinegar\n",
      "lemma: cup\n",
      "lemma: kosher\n",
      "lemma: salt\n",
      "lemma: cup\n",
      "lemma: sugar\n",
      "lemma: cup\n",
      "lemma: chop\n",
      "lemma: fresh\n",
      "lemma: dill\n",
      "lemma: skinless\n",
      "lemma: boneless\n",
      "lemma: chicken\n",
      "lemma: thigh\n",
      "lemma: pound\n",
      "lemma: halve\n",
      "lemma: quarter\n",
      "lemma: large\n",
      "lemma: vegetable\n",
      "lemma: oil\n",
      "lemma: frying\n",
      "lemma: cup\n",
      "lemma: cup\n",
      "lemma: buttermilk\n",
      "lemma: cup\n",
      "lemma: purpose\n",
      "lemma: flour\n",
      "lemma: kosher\n",
      "lemma: salt\n",
      "lemma: honey\n",
      "lemma: flaky\n",
      "lemma: sea\n",
      "lemma: salt\n",
      "lemma: such\n",
      "lemma: maldon\n",
      "lemma: toast\n",
      "lemma: benne\n",
      "lemma: sesame\n",
      "lemma: seed\n",
      "lemma: hot\n",
      "lemma: sauce\n",
      "lemma: serving\n",
      "lemma: deep\n",
      "lemma: fry\n",
      "lemma: thermometer\n"
     ]
    }
   ],
   "source": [
    "print(*[f'lemma: {word.lemma}' for sent in doc.sentences for word in sent.words if (\n",
    "    word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"PUNCT\", \"SCONJ\"]\n",
    ")], sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tablespoon',\n",
       " 'yellow',\n",
       " 'mustard',\n",
       " 'seed',\n",
       " 'tablespoon',\n",
       " 'brown',\n",
       " 'mustard',\n",
       " 'seed',\n",
       " 'teaspoon',\n",
       " 'coriander',\n",
       " 'seed',\n",
       " 'cup',\n",
       " 'apple',\n",
       " 'cider',\n",
       " 'vinegar',\n",
       " 'cup',\n",
       " 'kosher',\n",
       " 'salt',\n",
       " 'cup',\n",
       " 'sugar',\n",
       " 'cup',\n",
       " 'chop',\n",
       " 'fresh',\n",
       " 'dill',\n",
       " 'skinless',\n",
       " 'boneless',\n",
       " 'chicken',\n",
       " 'thigh',\n",
       " 'pound',\n",
       " 'halve',\n",
       " 'quarter',\n",
       " 'large',\n",
       " 'vegetable',\n",
       " 'oil',\n",
       " 'frying',\n",
       " 'cup',\n",
       " 'cup',\n",
       " 'buttermilk',\n",
       " 'cup',\n",
       " 'purpose',\n",
       " 'flour',\n",
       " 'kosher',\n",
       " 'salt',\n",
       " 'honey',\n",
       " 'flaky',\n",
       " 'sea',\n",
       " 'salt',\n",
       " 'such',\n",
       " 'maldon',\n",
       " 'toast',\n",
       " 'benne',\n",
       " 'sesame',\n",
       " 'seed',\n",
       " 'hot',\n",
       " 'sauce',\n",
       " 'serving',\n",
       " 'deep',\n",
       " 'fry',\n",
       " 'thermometer']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.lemma for sent in doc.sentences for word in sent.words if (\n",
    "    word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"PUNCT\", \"SCONJ\"]\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tablespoon', 'yellow', 'mustard', 'seed'],\n",
       " ['tablespoon', 'brown', 'mustard', 'seed'],\n",
       " ['teaspoon', 'coriander', 'seed'],\n",
       " ['cup', 'apple', 'cider', 'vinegar'],\n",
       " ['cup', 'kosher', 'salt'],\n",
       " ['cup', 'sugar'],\n",
       " ['cup', 'chop', 'fresh', 'dill'],\n",
       " ['skinless',\n",
       "  'boneless',\n",
       "  'chicken',\n",
       "  'thigh',\n",
       "  'pound',\n",
       "  'halve',\n",
       "  'quarter',\n",
       "  'large'],\n",
       " ['vegetable', 'oil', 'frying', 'cup'],\n",
       " ['cup', 'buttermilk'],\n",
       " ['cup', 'purpose', 'flour'],\n",
       " ['kosher', 'salt'],\n",
       " ['honey',\n",
       "  'flaky',\n",
       "  'sea',\n",
       "  'salt',\n",
       "  'such',\n",
       "  'maldon',\n",
       "  'toast',\n",
       "  'benne',\n",
       "  'sesame',\n",
       "  'seed',\n",
       "  'hot',\n",
       "  'sauce',\n",
       "  'serving'],\n",
       " ['deep', 'fry', 'thermometer']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe = []\n",
    "for sent in doc.sentences:\n",
    "    ingredients = []\n",
    "    for word in sent.words:\n",
    "        if word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"PUNCT\", \"SCONJ\"]:\n",
    "            ingredients.append(word.lemma)\n",
    "        else:\n",
    "            pass\n",
    "    recipe.append(ingredients)\n",
    "\n",
    "recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found [this resource](https://stackoverflow.com/questions/26907309/create-ngrams-only-for-words-on-the-same-line-disregarding-line-breaks-with-sc), trying custom analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tablespoon yellow mustard seed brk tablespoon brown mustard seed brk teaspoon coriander seed brk cup apple cider vinegar brk cup kosher salt brk cup sugar brk cup chop fresh dill brk skinless , boneless chicken thigh ( pound ) , halve , quarter large brk vegetable oil ( frying ; cup ) brk cup buttermilk brk cup - purpose flour brk kosher salt brk honey , flaky sea salt ( such maldon ) , toast benne sesame seed , hot sauce ( serving ) brk deep - fry thermometer']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is probably going to be the preprocessor\n",
    "test_recipe_2 = \" brk \".join(full_df['ingredients'][0]).lower()\n",
    "doc2 = nlp(test_recipe_2)\n",
    "\n",
    "# print(*[f'word: {word.text+\" \"}\\t \\t lemma: {word.lemma}, \\t \\t upos: {word.upos}' for sent in doc2.sentences for word in sent.words], sep='\\n')\n",
    "\n",
    "# This will be the tokenizer?\n",
    "# lemma_test_recipe_2 = \" \".join([word.lemma for sent in doc2.sentences for word in sent.words if (\n",
    "#     word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\"]\n",
    "# )])\n",
    "lemma_test_recipe_2 = [\" \".join([word.lemma for sent in doc2.sentences for word in sent.words if (\n",
    "    word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\"]\n",
    ")])]\n",
    "lemma_test_recipe_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom ngram analyzer function, matching only ngrams that belong to the same line\n",
    "def gen_analyzer(minNgramLength, maxNgramLength):\n",
    "    def ngrams_per_line(doc):\n",
    "\n",
    "        # analyze each line of the input string seperately\n",
    "        for ln in doc.split('brk'):\n",
    "\n",
    "            # tokenize the input string (customize the regex as desired)\n",
    "            terms = re.findall(u'(?u)\\\\b\\\\w+\\\\b', ln)\n",
    "\n",
    "            # loop ngram creation for every number between min and max ngram length\n",
    "            for ngramLength in range(minNgramLength, maxNgramLength+1):\n",
    "\n",
    "                # find and return all ngrams\n",
    "                # for ngram in zip(*[terms[i:] for i in range(3)]): <-- solution without a generator (works the same but has higher memory usage)\n",
    "                for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]): # <-- solution using a generator\n",
    "                    ngram = ' '.join(ngram)\n",
    "                    yield ngram\n",
    "    return ngrams_per_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(analyzer=&lt;function gen_analyzer.&lt;locals&gt;.ngrams_per_line at 0x7fd92b20c670&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&lt;function gen_analyzer.&lt;locals&gt;.ngrams_per_line at 0x7fd92b20c670&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(analyzer=<function gen_analyzer.<locals>.ngrams_per_line>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(analyzer=gen_analyzer(1, 4))\n",
    "cv.fit(lemma_test_recipe_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['apple', 'apple cider', 'apple cider vinegar', 'benne',\n",
       "       'benne sesame', 'benne sesame seed', 'benne sesame seed hot',\n",
       "       'boneless', 'boneless chicken', 'boneless chicken thigh',\n",
       "       'boneless chicken thigh pound', 'brown', 'brown mustard',\n",
       "       'brown mustard seed', 'buttermilk', 'chicken', 'chicken thigh',\n",
       "       'chicken thigh pound', 'chicken thigh pound halve', 'chop',\n",
       "       'chop fresh', 'chop fresh dill', 'cider', 'cider vinegar',\n",
       "       'coriander', 'coriander seed', 'cup', 'cup apple',\n",
       "       'cup apple cider', 'cup apple cider vinegar', 'cup buttermilk',\n",
       "       'cup chop', 'cup chop fresh', 'cup chop fresh dill', 'cup kosher',\n",
       "       'cup kosher salt', 'cup purpose', 'cup purpose flour', 'cup sugar',\n",
       "       'deep', 'deep fry', 'deep fry thermometer', 'dill', 'flaky',\n",
       "       'flaky sea', 'flaky sea salt', 'flaky sea salt such', 'flour',\n",
       "       'fresh', 'fresh dill', 'fry', 'fry thermometer', 'frying',\n",
       "       'frying cup', 'halve', 'halve quarter', 'halve quarter large',\n",
       "       'honey', 'honey flaky', 'honey flaky sea', 'honey flaky sea salt',\n",
       "       'hot', 'hot sauce', 'hot sauce serving', 'kosher', 'kosher salt',\n",
       "       'large', 'maldon', 'maldon toast', 'maldon toast benne',\n",
       "       'maldon toast benne sesame', 'mustard', 'mustard seed', 'oil',\n",
       "       'oil frying', 'oil frying cup', 'pound', 'pound halve',\n",
       "       'pound halve quarter', 'pound halve quarter large', 'purpose',\n",
       "       'purpose flour', 'quarter', 'quarter large', 'salt', 'salt such',\n",
       "       'salt such maldon', 'salt such maldon toast', 'sauce',\n",
       "       'sauce serving', 'sea', 'sea salt', 'sea salt such',\n",
       "       'sea salt such maldon', 'seed', 'seed hot', 'seed hot sauce',\n",
       "       'seed hot sauce serving', 'serving', 'sesame', 'sesame seed',\n",
       "       'sesame seed hot', 'sesame seed hot sauce', 'skinless',\n",
       "       'skinless boneless', 'skinless boneless chicken',\n",
       "       'skinless boneless chicken thigh', 'such', 'such maldon',\n",
       "       'such maldon toast', 'such maldon toast benne', 'sugar',\n",
       "       'tablespoon', 'tablespoon brown', 'tablespoon brown mustard',\n",
       "       'tablespoon brown mustard seed', 'tablespoon yellow',\n",
       "       'tablespoon yellow mustard', 'tablespoon yellow mustard seed',\n",
       "       'teaspoon', 'teaspoon coriander', 'teaspoon coriander seed',\n",
       "       'thermometer', 'thigh', 'thigh pound', 'thigh pound halve',\n",
       "       'thigh pound halve quarter', 'toast', 'toast benne',\n",
       "       'toast benne sesame', 'toast benne sesame seed', 'vegetable',\n",
       "       'vegetable oil', 'vegetable oil frying',\n",
       "       'vegetable oil frying cup', 'vinegar', 'yellow', 'yellow mustard',\n",
       "       'yellow mustard seed'], dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = cv.fit_transform(lemma_test_recipe_2)\n",
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_preprocessor(stanza_pipeline, ingredients_list):\n",
    "    # This function takes in a Stanza pipeline and a recipe's ingredients in list form and returns a Stanza transformed document to be used in the lemmatizer\n",
    "    # lowered = \" brk \".join(ingredients_list).lower()\n",
    "    lowered = ingredients_list.apply(\" brk \".join).str.lower()\n",
    "    return stanza_pipeline(lowered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_lemmatizer(stanza_preprocessed):\n",
    "    # This function takes in the preprocessed Stanza document from preprocessor and performs lemmatization and filtering\n",
    "    return [\" \".join([word.lemma for sent in stanza_preprocessed.sentences for word in sent.words if (\n",
    "    word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\"]\n",
    ")])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:531: UserWarning: The parameter 'preprocessor' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\n",
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:560: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m cv_params \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mstrip_accents\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39municode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmin_df\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m10\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m vectorizer_model \u001b[39m=\u001b[39m CountVectorizer(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcv_params)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m test_transform \u001b[39m=\u001b[39m vectorizer_model\u001b[39m.\u001b[39;49mfit_transform(full_df[\u001b[39m'\u001b[39;49m\u001b[39mingredients\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m:\u001b[39m5\u001b[39;49m])\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1277\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "\u001b[1;32m/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb Cell 26\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mngrams_per_line\u001b[39m(doc):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# analyze each line of the input string seperately\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfor\u001b[39;00m ln \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39m\u001b[39mbrk\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m# tokenize the input string (customize the regex as desired)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         terms \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mfindall(\u001b[39mu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(?u)\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mb\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mw+\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m, ln)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m# loop ngram creation for every number between min and max ngram length\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'preprocessor':stanza_preprocessor,\n",
    "    'tokenizer':stanza_lemmatizer, # out of memory \n",
    "    # 'stop_words':flushtrated_list,\n",
    "    'analyzer': gen_analyzer(1, 4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "vectorizer_model = CountVectorizer(**cv_params)\n",
    "\n",
    "test_transform = vectorizer_model.fit_transform(full_df['ingredients'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1 tablespoons yellow mustard seeds brk 1 table...\n",
       "1    3 pounds small-leaved bulk spinach brk salt br...\n",
       "2    3 1/2 cups all-purpose flour brk 1 tablespoon ...\n",
       "3    1 small ripe avocado, preferably hass (see not...\n",
       "4    2 pounds fresh tomatoes, unpeeled and cut in q...\n",
       "Name: ingredients, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['ingredients'][0:5].apply(\" brk \".join).str.lower()\n",
    "# test_preproc = stanza_preprocessor(nlp, full_df['ingredients'][0:5])\n",
    "# test_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['prepSteps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['prepSteps'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_steps = full_df['prepSteps'].apply(\" \".join).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-calculating sentence mebeddings\n",
    "# embedding_model_params = {'embedding_model': 'all-MiniLM-L6-v2'}\n",
    "# embedding_model = SentenceTransformer(embedding_model_params['embedding_model'])\n",
    "# embeddings = embedding_model.encode(recipe_steps, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify UMAP dimensionality reductions\n",
    "# umap_model_params = {'n_neighbors':15, 'n_components':10, 'random_state':200}\n",
    "# umap_model = UMAP(**umap_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster with HDBSCAN\n",
    "# hdbscan_model_params = {'min_cluster_size':200, 'prediction_data':True}\n",
    "# hdbscan_model = HDBSCAN(**hdbscan_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['alternatives', 'bibb', 'boston', 'chacheres', 'chobani', 'franks', 'grands', 'hass', 'hidden', 'hunts', 'japanese', 'kc', 'lakes', 'laughing', 'masterpiece', 'pillsburyTM', 'progressoTM', 'sauce', 'secrets', 'smokies', 'tony', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m cv_params \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mstrip_accents\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39municode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmin_df\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m10\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m vectorizer_model \u001b[39m=\u001b[39m CountVectorizer(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcv_params)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/awchen/Repos/Projects/MeaLeon/nbs/08_stanza_testing.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m test_transform \u001b[39m=\u001b[39m vectorizer_model\u001b[39m.\u001b[39;49mfit_transform(full_df[\u001b[39m'\u001b[39;49m\u001b[39mingredients\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m:\u001b[39m5\u001b[39;49m])\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1277\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m---> 68\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# adding custom count vectorization\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    # 'preprocessor':custom_preprocessor,\n",
    "    # 'tokenizer':custom_lemmatizer, # out of memory \n",
    "    'stop_words':flushtrated_list,\n",
    "    'token_pattern':r\"(?u)\\b[a-zA-Z]{2,}\\b\",\n",
    "    'ngram_range':(1, 4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "vectorizer_model = CountVectorizer(**cv_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with open('../data/processed/bertopic_params.joblib', 'w') as fp:\n",
    "# pipeline_params = {\n",
    "#     'embedding':{'pretrained_sentence_embeddings': embedding_model_params},\n",
    "#     'dimension_reduction': {'UMAP': umap_model_params},\n",
    "#     'clustering': {'HDBSCAN': hdbscan_model_params},\n",
    "#     'vectorizer': {'sklearn_countvectorizer': cv_params},\n",
    "# }\n",
    "# joblib.dump(pipeline_params, '../data/processed/bertopic_params.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id=get_experiment_id(\"initial_explicit_spec_run_3\")):\n",
    "    pipeline_params = {\n",
    "        'language':'english',\n",
    "        'top_n_words':20,\n",
    "        'n_gram_range':(1, 4),\n",
    "        'min_topic_size':500,\n",
    "        'nr_topics':'auto',\n",
    "        'verbose':True,\n",
    "        'low_memory':True,\n",
    "        'calculate_probabilities':True\n",
    "    }\n",
    "    mlflow.log_params(pipeline_params)\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        **pipeline_params,\n",
    "        vectorizer_model=vectorizer_model\n",
    "    )\n",
    "    # TOKENIZERS_PARALLELISM=False\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(recipe_steps)\n",
    "\n",
    "    topic_model.get_topic_info().to_json('../data/processed/topic_model_df.json')\n",
    "\n",
    "    # mlflow.log_artifact('../data/processed/bertopic_params.joblib')\n",
    "    mlflow.log_artifact('../data/processed/topic_model_df.json')\n",
    "\n",
    "    print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_full(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()['Representation']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | export\n",
    "def foo():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# import nbdev\n",
    "\n",
    "# nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
