{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis of Epicurious Scrape in a JSON file\n",
    "\n",
    "This is an idealized workflow for Aaron Chen in looking at data science problems. It likely isn't the best path, nor has he rigidly applied or stuck to this ideal, but he wishes that he worked this way more frequently.\n",
    "\n",
    "## Purpose: Work through some exploratory data analysis of the Epicurious scrape on stream. Try to write some functions to help process the data.\n",
    "\n",
    "### Author: Aaron Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If needed, run shell commands here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Resources\n",
    "\n",
    "List out references or documentation that has helped you with this notebook\n",
    "\n",
    "### Code\n",
    "Regex Checker: https://regex101.com/\n",
    "\n",
    "#### Scikit-learn\n",
    "1. https://scikit-learn.org/stable/modules/decomposition.html#latent-dirichlet-allocation-lda\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "For this notebook, the data is stored in the repo base folder/data/raw\n",
    "\n",
    "### Process\n",
    "\n",
    "Are there steps or tutorials you are following? Those are things I try to list in Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "import project_path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "import unicodedata\n",
    "\n",
    "# import local scripts\n",
    "import src.dataframe_preprocessor as dfpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions\n",
    "\n",
    "My workflow is to try things with code cells, then when the code cells get messy and repetitive, to convert into helper functions that can be called.\n",
    "\n",
    "When the helper functions are getting used a lot, it is usually better to convert them to scripts or classes that can be called/instantiated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"This function takes in a pandas DataFrame from pd.read_json and performs some preprocessing by unpacking the nested dictionaries and creating new columns with the simplified structures. It will then drop the original columns that would no longer be needed.\n",
    "\n",
    "    Args:\n",
    "        pd.DataFrame\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    def null_filler(to_check: Dict[str, str], key_target: str) -> str:\n",
    "        \"\"\"This function takes in a dictionary that is currently fed in with a lambda function and then performs column specific preprocessing.\n",
    "\n",
    "        Args:\n",
    "            to_check: dict\n",
    "            key_target: str\n",
    "\n",
    "        Returns:\n",
    "            str\n",
    "        \"\"\"\n",
    "\n",
    "        # Only look in the following keys, if the input isn't one of these, it should be recognized as an improper key\n",
    "        valid_keys = [\"name\", \"filename\", \"credit\"]\n",
    "\n",
    "        # This dictionary converts the input keys into substrings that can be used in f-strings to fill in missing values in the record\n",
    "        translation_keys = {\n",
    "            \"name\": \"Cuisine\",\n",
    "            \"filename\": \"Photo\",\n",
    "            \"credit\": \"Photo Credit\",\n",
    "        }\n",
    "\n",
    "        if key_target not in valid_keys:\n",
    "            # this logic makes sure we are only looking at valid keys\n",
    "            return (\n",
    "                \"Improper key target: can only pick from 'name', 'filename', 'credit'.\"\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            if pd.isna(to_check):\n",
    "                # this logic checks to see if the dictionary exists at all. if so, return Missing\n",
    "                return f\"Missing {translation_keys[key_target]}\"\n",
    "            else:\n",
    "                if key_target == \"name\" and (to_check[\"category\"] != \"cuisine\"):\n",
    "                    # This logic checks for the cuisine, if the cuisine is not there (and instead has 'ingredient', 'type', 'item', 'equipment', 'meal'), mark as missing\n",
    "                    return f\"Missing {translation_keys[key_target]}\"\n",
    "                else:\n",
    "                    # Otherwise, there should be no issue with returning\n",
    "                    return to_check[key_target]\n",
    "\n",
    "    # Dive into the tag column and extract the cuisine label. Put into new column or fills with \"missing data\"\n",
    "    df[\"cuisine_name\"] = df[\"tag\"].apply(\n",
    "        lambda x: null_filler(to_check=x, key_target=\"name\")\n",
    "    )\n",
    "    # df[\"cuisine_name\"] = df[\"tag\"].apply(lambda x: x['name'] if not pd.isna(x) and x['category'] == 'cuisine' else 'Cuisine Missing')\n",
    "\n",
    "    # this lambda function goes into the photo data column and extracts just the filename from the dictionary\n",
    "    df[\"photo_filename\"] = df[\"photoData\"].apply(\n",
    "        lambda x: null_filler(to_check=x, key_target=\"filename\")\n",
    "    )\n",
    "    # df[\"photo_filename\"] = df['photoData'].apply(lambda x: x['filename'] if not pd.isna(x) else 'Missing photo')\n",
    "\n",
    "    # This lambda function goes into the photo data column and extracts just the photo credit from the dictionary\n",
    "    df[\"photo_credit\"] = df[\"photoData\"].apply(\n",
    "        lambda x: null_filler(to_check=x, key_target=\"credit\")\n",
    "    )\n",
    "    # df[\"photo_credit\"] = df['photoData'].apply(lambda x: x['credit'] if not pd.isna(x) else 'Missing credit')\n",
    "\n",
    "    # for the above, maybe they can be refactored to one function where the arguments are a column name, dictionary key name, the substring return\n",
    "\n",
    "    # this lambda funciton goes into the author column and extract the author name or fills iwth \"missing data\"\n",
    "    df[\"author_name\"] = df[\"author\"].apply(\n",
    "        lambda x: x[0][\"name\"] if x else \"Missing Author Name\"\n",
    "    )\n",
    "\n",
    "    # This function takes in the given pubDate column and creates a new column with the pubDate values converted to datetime objects\n",
    "    df[\"date_published\"] = pd.to_datetime(df[\"pubDate\"], infer_datetime_format=True)\n",
    "\n",
    "    # drop some original columns to clean up the dataframe\n",
    "    df.drop(\n",
    "        labels=[\"tag\", \"photoData\", \"author\", \"type\", \"dateCrawled\", \"pubDate\"],\n",
    "        axis=1,\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables \n",
    "\n",
    "**Remember to refactor these out, not ideal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "data_path = \"../data/recipes-en-201706/epicurious-recipes_m2.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Commentary\n",
    "\n",
    "1. I used numbered lists to keep track of things I noticed\n",
    "\n",
    "### To Do\n",
    "\n",
    "1. Try to determine consistency of nested data structures\n",
    "   1. Is the photoData or number of things inside photoData the same from record to record\n",
    "   2. What about for tag?\n",
    "\n",
    "Data wasn't fully consistent but logic in helper function helped handle nulls\n",
    "\n",
    "2. How to handle nulls?\n",
    "   1. Author      Filled in with \"Missing Author\"\n",
    "   2. Tag         Filled in with \"Missing Cuisine\"\n",
    "3. ~~Convert pubDate to actual timestamp~~  \n",
    "4. ~~Convert ScrapeDate to actual timestamp~~\n",
    "   1. This was ignored as the datestamp was not useful (generally within minutes of the origin of UNIX time)\n",
    "   \n",
    "**5. Append new columns for relevant nested structures and unfold them**\n",
    "\n",
    "6. Determine actual types of `ingredients` and `prepSteps`\n",
    "7. Continue working through test example of single recipe to feed into spaCy and then sklearn.feature_extraction.text stack\n",
    "8. Will need to remove numbers, punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and viewing the data as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/recipes-en-201706/epicurious-recipes_m2.json\"\n",
    "\n",
    "epic_dataframe = pd.read_json(data_path, typ=\"frame\")\n",
    "\n",
    "dfpp.preprocess_dataframe(df=epic_dataframe)\n",
    "epic_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throw this into CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1 tablespoons yellow mustard seeds 1 tablespoo...\n",
       "1        3 pounds small-leaved bulk spinach Salt 1/2 cu...\n",
       "2        3 1/2 cups all-purpose flour 1 tablespoon baki...\n",
       "3        1 small ripe avocado, preferably Hass (see Not...\n",
       "4        2 pounds fresh tomatoes, unpeeled and cut in q...\n",
       "                               ...                        \n",
       "34751    1 tablespoon unsalted butter, at room temperat...\n",
       "34752    8 tablespoons (1 stick) salted butter, at room...\n",
       "34753    3 tablespoons unsalted butter, plus more for g...\n",
       "34754    Coarse salt 2 lime wedges 2 ounces tomato juic...\n",
       "34755    1 bottle (375 ml) sour beer, such as Almanac C...\n",
       "Name: ingredients, Length: 34756, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_recipes_list = epic_dataframe[\"ingredients\"].str.join(\" \")\n",
    "# .apply(\" \".join).str.lower()\n",
    "# print(type(all_recipes_list))\n",
    "all_recipes_list\n",
    "# for i in range(0,5):\n",
    "#     print((i, all_recipes_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.8/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tablespoons yellow mustard seeds 1 tablespoons brown mustard seeds 1 1/2 teaspoons coriander seeds 1 cup apple cider vinegar 2/3 cup kosher salt 1/3 cup sugar 1/4 cup chopped fresh dill 8 skinless, boneless chicken thighs (about 3 pounds), halved, quartered if large Vegetable oil (for frying; about 10 cups) 2 cups buttermilk 2 cups all-purpose flour Kosher salt Honey, flaky sea salt (such as Maldon), toasted benne or sesame seeds, hot sauce (for serving) A deep-fry thermometer\n"
     ]
    }
   ],
   "source": [
    "first_rec = all_recipes_list[0]\n",
    "print(first_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tablespoons NOUN  tablespoon False\n",
      "yellow ADJ  yellow False\n",
      "mustard NOUN  mustard False\n",
      "seeds NOUN  seed False\n",
      "tablespoons NOUN  tablespoon False\n",
      "brown ADJ  brown False\n",
      "mustard NOUN  mustard False\n",
      "seeds NOUN  seed False\n",
      "teaspoons NOUN  teaspoon False\n",
      "coriander NOUN  coriander False\n",
      "seeds NOUN  seed False\n",
      "cup NOUN QUANTITY cup False\n",
      "apple NOUN  apple False\n",
      "cider NOUN  cider False\n",
      "vinegar NOUN  vinegar False\n",
      "cup NOUN  cup False\n",
      "kosher ADJ  kosher False\n",
      "salt NOUN  salt False\n",
      "cup NOUN  cup False\n",
      "sugar NOUN  sugar False\n",
      "cup NOUN  cup False\n",
      "chopped VERB  chop False\n",
      "fresh ADJ  fresh False\n",
      "dill NOUN  dill False\n",
      "skinless NOUN  skinless False\n",
      ", PUNCT  , False\n",
      "boneless NOUN  boneless False\n",
      "chicken NOUN  chicken False\n",
      "thighs NOUN  thigh False\n",
      "( PUNCT  ( False\n",
      "about ADV QUANTITY about False\n",
      "pounds NOUN QUANTITY pound False\n",
      ") PUNCT  ) False\n",
      ", PUNCT  , False\n",
      "halved VERB  halve False\n",
      ", PUNCT  , False\n",
      "quartered VERB  quarter False\n",
      "if SCONJ  if False\n",
      "large ADJ  large False\n",
      "Vegetable ADJ  vegetable False\n",
      "oil NOUN  oil False\n",
      "( PUNCT  ( False\n",
      "for ADP  for False\n",
      "frying VERB  fry False\n",
      "; PUNCT  ; False\n",
      "about ADV CARDINAL about False\n",
      "cups NOUN  cup False\n",
      ") PUNCT  ) False\n",
      "cups NOUN  cup False\n",
      "buttermilk NOUN  buttermilk False\n",
      "cups NOUN  cup False\n",
      "all DET  all False\n",
      "- PUNCT  - False\n",
      "purpose NOUN  purpose False\n",
      "flour NOUN  flour False\n",
      "Kosher PROPN PERSON Kosher False\n",
      "salt NOUN  salt False\n",
      "Honey PROPN ORG Honey False\n",
      ", PUNCT  , False\n",
      "flaky ADJ  flaky False\n",
      "sea NOUN  sea False\n",
      "salt NOUN  salt False\n",
      "( PUNCT  ( False\n",
      "such ADJ  such False\n",
      "as ADP  as False\n",
      "Maldon PROPN ORG Maldon False\n",
      ") PUNCT  ) False\n",
      ", PUNCT  , False\n",
      "toasted VERB  toast False\n",
      "benne PROPN  benne False\n",
      "or CCONJ  or False\n",
      "sesame ADJ  sesame False\n",
      "seeds NOUN  seed False\n",
      ", PUNCT  , False\n",
      "hot ADJ  hot False\n",
      "sauce NOUN  sauce False\n",
      "( PUNCT  ( False\n",
      "for ADP  for False\n",
      "serving VERB  serve False\n",
      ") PUNCT  ) False\n",
      "A DET  a False\n",
      "deep ADJ  deep False\n",
      "- PUNCT  - False\n",
      "fry NOUN  fry False\n",
      "thermometer NOUN  thermometer False\n"
     ]
    }
   ],
   "source": [
    "doc_first_rec = nlp(first_rec)\n",
    "for token in doc_first_rec:\n",
    "    if token.like_num == False:\n",
    "        print(token.text, token.pos_, token.ent_type_, token.lemma_, token.is_digit)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_dataframe[\"recipe_url\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is looking for accented characters inside text, which may not be necessary\n",
    "# for word in test_rec_list:\n",
    "#     print(unicodedata.normalize(\"NFKD\", word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
