{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: test\n",
    "output-file: template.html\n",
    "title: Template\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# from bertopic import BERTopic\n",
    "# from bertopic.vectorizers import OnlineCountVectorizer\n",
    "import dagshub\n",
    "from datetime import datetime\n",
    "import dill as pickle\n",
    "import dvc.api\n",
    "# from hdbscan import HDBSCAN\n",
    "from itertools import tee, islice, product\n",
    "import joblib\n",
    "# import mlflow\n",
    "# from mlflow.models import infer_signature\n",
    "import nbdev\n",
    "from nbdev.showdoc import *\n",
    "import pandas as pd\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer\n",
    "    , TfidfTransformer\n",
    "    , TfidfVectorizer\n",
    "    , \n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from src.custom_sklearn_text_transformer_mlflow import CustomSKLearnAnalyzer\n",
    "# from src.custom_stanza_mlflow import CustomSKLearnWrapper\n",
    "import src.dataframe_preprocessor as dfpp\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "# from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate stanza pipeline\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', \n",
    "                    depparse_batch_size=50, \n",
    "                    depparse_min_length_to_batch_separately=50,\n",
    "                    verbose=True,\n",
    "                    use_gpu=True, # set to true when on cloud/not on streaming computer\n",
    "                    batch_size=100\n",
    "                    )\n",
    "\n",
    "# load raw data and preprocess/clean\n",
    "data = dvc.api.read(\n",
    "    path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "    , mode='r')\n",
    "raw_df = pd.read_json(data)\n",
    "print('\\n')\n",
    "print('--------------')\n",
    "print('Raw Dataframe:', end='\\n')\n",
    "print(raw_df.head())\n",
    "print(raw_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take sample and train/test split \n",
    "subset_df = raw_df.sample(n=100, random_state=45)\n",
    "train_df, test_df = train_test_split(subset_df,test_size=0.5, random_state=45)\n",
    "\n",
    "# pre_proc_df is cleaned dataframe\n",
    "to_nlp_df = dfpp.preprocess_dataframe(train_df)\n",
    "print('\\n')\n",
    "print('--------------')\n",
    "print('Preprocessed Dataframe:', end='\\n')\n",
    "print(to_nlp_df.head())\n",
    "print(to_nlp_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_transformer_params = {\n",
    "    'analyzer': 'word',\n",
    "    'ngram_range': (1,4),\n",
    "    'min_df':3,\n",
    "    'binary':False\n",
    "}\n",
    "\n",
    "sklearn_transformer = TfidfVectorizer(**sklearn_transformer_params)\n",
    "\n",
    "# print('\\n')\n",
    "# print('-' * 80)\n",
    "# print('sklearn fit transform on ingredients:', end='\\n')\n",
    "\n",
    "model_input = to_nlp_df['ingredients'].apply(\" \".join).str.lower()\n",
    "\n",
    "print(\"sklearn fit transform start: \" + str(datetime.now()))\n",
    "\n",
    "# Do fit transform on data\n",
    "response = sklearn_transformer.fit_transform(tqdm(model_input)) \n",
    "\n",
    "print(\"sklearn fit transform end: \" + str(datetime.now()))\n",
    "\n",
    "transformed_recipe = pd.DataFrame(\n",
    "        response.toarray(),\n",
    "        columns=sklearn_transformer.get_feature_names_out(),\n",
    "        index=model_input.index\n",
    ")\n",
    "\n",
    "print(transformed_recipe)\n",
    "print(transformed_recipe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_transformer_params = {    \n",
    "    'analyzer': CustomSKLearnAnalyzer.ngrams_maker(\n",
    "        min_ngram_length=1,\n",
    "        max_ngram_length=4\n",
    "        ),\n",
    "    'min_df':3,\n",
    "    'binary':False\n",
    "}\n",
    "\n",
    "sklearn_transformer = TfidfVectorizer(**sklearn_transformer_params)\n",
    "\n",
    "model_input = to_nlp_df['ingredients_lemmafied']\n",
    "\n",
    "# Do fit transform on data\n",
    "print(\"fit_transform start: \" + str(datetime.now()))\n",
    "response = sklearn_transformer.fit_transform(tqdm(model_input)) \n",
    "print(\"fit_transform end: \" + str(datetime.now()))\n",
    "\n",
    "transformed_recipe = pd.DataFrame(\n",
    "        response.toarray(),\n",
    "        columns=sklearn_transformer.get_feature_names_out(),\n",
    "        index=model_input.index\n",
    ")\n",
    "\n",
    "print(transformed_recipe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_transformer_params = {\n",
    "    'analyzer': CustomSKLearnAnalyzer.ngrams_maker(\n",
    "        min_ngram_length=1,\n",
    "        max_ngram_length=4\n",
    "        ),\n",
    "    'min_df':3,\n",
    "    'binary':True\n",
    "}\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'Tf'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_transformer_params)\n",
    "# pipeline_params.update(bertopic_params)\n",
    "\n",
    "# signature = infer_signature(model_input=to_nlp_df['ingredients'],\n",
    "#                             )\n",
    "\n",
    "with mlflow.start_run(experiment_id=mlflow_exp_id):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    # Will be useful in STAGING/Evaluation\n",
    "    \n",
    "    # LOG MODEL\n",
    "    # Instantiate sklearn OneHotEncoder\n",
    "    sklearn_transformer = CountVectorizer(**sklearn_transformer_params)\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('sklearn fit transform on ingredients:', end='\\n')\n",
    "\n",
    "    model_input = to_nlp_df['ingredients']\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('Input Data: ', end='\\n')\n",
    "    print(model_input)\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('Input Data Shape: ', end='\\n')\n",
    "    print(model_input.shape)\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('Random 3 Records from Input Data: ', end='\\n')\n",
    "    print(model_input.sample(3, random_state=200))\n",
    "\n",
    "    # Do fit transform on data\n",
    "    response = sklearn_transformer.fit_transform(tqdm(model_input)) \n",
    "    \n",
    "    transformed_recipe = pd.DataFrame(\n",
    "            response.toarray(),\n",
    "            columns=sklearn_transformer.get_feature_names_out(),\n",
    "            index=model_input.index\n",
    "    )\n",
    "\n",
    "    signature = infer_signature(model_input=model_input,\n",
    "                                model_output=transformed_recipe\n",
    "                                )\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('Transformed Data:', end='\\n')\n",
    "    print(transformed_recipe)\n",
    "    \n",
    "    # mlflow.pyfunc.save_model(\n",
    "    #     path=model_directory,\n",
    "    #     code_path=[\"../src/\"],\n",
    "    #     python_model=CustomSKLearnWrapper(),\n",
    "    #     input_example=to_nlp_df['ingredients'][0],    \n",
    "    #     artifacts=artifacts\n",
    "    # )\n",
    "\n",
    "     # joblib.dump(sklearn_transformer, sklearn_transformer_path)\n",
    "    with open(sklearn_transformer_path, \"wb\") as fo:\n",
    "        pickle.dump(sklearn_transformer, fo)\n",
    "        # mlflow.log_artifact(sklearn_transformer_path,\n",
    "        #                     artifact_path='sklearn_transformer')\n",
    "\n",
    "    # joblib.dump(transformed_recipe, transformed_recipes_path)\n",
    "    with open(transformed_recipes_path, \"wb\") as fo:\n",
    "        pickle.dump(transformed_recipe, fo)\n",
    "        # mlflow.log_artifact(transformed_recipes_path,\n",
    "        #                     artifact_path='transformed_recipes')\n",
    "\n",
    "\n",
    "    model_info = mlflow.pyfunc.log_model( \n",
    "        code_path=[\"../src/\"],\n",
    "        python_model=CustomSKLearnWrapper(),\n",
    "        input_example=to_nlp_df['ingredients'][0],\n",
    "        signature=signature,        \n",
    "        artifact_path=\"sklearn_model\",\n",
    "        artifacts=artifacts\n",
    "        ) \n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictor = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_proc_df is cleaned dataframe\n",
    "pre_proc_test_df = dfpp.preprocess_dataframe(test_df)\n",
    "print('\\n')\n",
    "print('--------------')\n",
    "print('Preprocessed Dataframe: ', end='\\n')\n",
    "print(pre_proc_test_df.head())\n",
    "print(pre_proc_test_df.shape)\n",
    "\n",
    "# create subset for dev purposes\n",
    "# to_nlp_test_df = pre_proc_test_df\n",
    "# print('\\n')\n",
    "# print('-' * 80)\n",
    "# print('Subset Dataframe:', end='\\n')\n",
    "# print(to_nlp_test_df.head())\n",
    "# print(to_nlp_test_df.shape)\n",
    "\n",
    "test_model_input = pre_proc_test_df['ingredients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_input.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info.signature.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictor.predict(test_model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n')\n",
    "print('-' * 80)\n",
    "print('Input Data: ', end='\\n')\n",
    "print(test_model_input)\n",
    "\n",
    "print('\\n')\n",
    "print('-' * 80)\n",
    "print('Input Data Shape: ', end='\\n')\n",
    "print(test_model_input.shape)\n",
    "\n",
    "print('\\n')\n",
    "print('-' * 80)\n",
    "print('Random 3 Records from Input Data: ', end='\\n')\n",
    "print(test_model_input.sample(3, random_state=200))\n",
    "\n",
    "# test_response = sklearn_transformer.transform(tqdm(test_model_input)) \n",
    "test_response = sklearn_transformer.transform(test_model_input)\n",
    "    \n",
    "    \n",
    "test_transformed_recipe = pd.DataFrame(\n",
    "            test_response.toarray(),\n",
    "            columns=sklearn_transformer.get_feature_names_out(),\n",
    "            index=test_model_input.index\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformed_recipe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
