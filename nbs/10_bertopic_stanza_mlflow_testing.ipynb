{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: test\n",
    "output-file: template.html\n",
    "title: Template\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import OnlineCountVectorizer\n",
    "import dagshub\n",
    "from datetime import datetime\n",
    "import dill as pickle\n",
    "import dvc.api\n",
    "from hdbscan import HDBSCAN\n",
    "from itertools import tee, islice\n",
    "import mlflow\n",
    "import nbdev\n",
    "from nbdev.showdoc import *\n",
    "import pandas as pd\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer\n",
    "    , TfidfTransformer\n",
    "    , TfidfVectorizer\n",
    "    ,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from src.custom_stanza_mlflow import StanzaWrapper\n",
    "import src.dataframe_preprocessor as dfpp\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | export\n",
    "def foo():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# this function allows us to get the experiment ID from an experiment name\n",
    "def get_experiment_id(name):\n",
    "    exp = mlflow.get_experiment_by_name(name)\n",
    "    if exp is None:\n",
    "      exp_id = mlflow.create_experiment(name)\n",
    "      return exp_id\n",
    "    return exp.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_analyzer(step_list, stanza_pipeline, minNgramLength, maxNgramLength, lemmatize=True):\n",
    "    lowered = \" brk \".join(map(str, [step for step in step_list if step is not None])).lower()\n",
    "\n",
    "    preproc = stanza_pipeline(lowered)\n",
    "    \n",
    "    if lemmatize:\n",
    "        lemmad = \" \".join(map(str,\n",
    "                            [word.lemma\n",
    "                            for sent in preproc.sentences \n",
    "                            for word in sent.words if (\n",
    "                                word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\", \"PUNCT\"]\n",
    "                                and word is not None\n",
    "                            )]\n",
    "                        )\n",
    "                    )\n",
    "    else:\n",
    "        lemmad = \" \".join(map(str,\n",
    "                            [word.text\n",
    "                            for sent in preproc.sentences \n",
    "                            for word in sent.words if (\n",
    "                                word is not None\n",
    "                            )]\n",
    "                        )\n",
    "                    )\n",
    "    # analyze each line of the input string seperately\n",
    "    for ln in lemmad.split(' brk '):\n",
    "        # tokenize the input string (customize the regex as desired)\n",
    "        at_least_two_english_characters_whole_words = \"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "        terms = re.split(at_least_two_english_characters_whole_words, ln)\n",
    "\n",
    "        # loop ngram creation for every number between min and max ngram length\n",
    "        for ngramLength in range(minNgramLength, maxNgramLength+1):\n",
    "\n",
    "            # find and return all ngrams\n",
    "            # for ngram in zip(*[terms[i:] for i in range(3)]): \n",
    "                # <-- solution without a generator (works the same but has higher memory usage)\n",
    "            for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]):   # <-- solution using a generator\n",
    "                \n",
    "                ngram = ' '.join(map(str, ngram))\n",
    "                # yield ngram\n",
    "                return str(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | Below this are blocks to use DagsHub with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@markdown Enter the username of your DAGsHub account:\n",
    "DAGSHUB_USER_NAME = \"AaronWChen\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the email for your DAGsHub account:\n",
    "DAGSHUB_EMAIL = \"awc33@cornell.edu\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the repo name \n",
    "DAGSHUB_REPO_NAME=\"MeaLeon\"\n",
    "\n",
    "#@markdown Enter the name of the branch you are working on \n",
    "BRANCH=\"STANZA-2/investigate_bertopic_compatibility\"\n",
    "dagshub.init(repo_name=DAGSHUB_REPO_NAME\n",
    "             , repo_owner=DAGSHUB_USER_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4861aee40e642b3800b453cd6805102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 19:10:46 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-12-18 19:10:47 INFO: File exists: /home/awchen/stanza_resources/en/default.zip\n",
      "2023-12-18 19:10:50 INFO: Finished downloading models and saved to /home/awchen/stanza_resources.\n",
      "2023-12-18 19:10:50 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2c7736969446a8be387c82f57787a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 19:10:51 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2023-12-18 19:10:51 INFO: Using device: cpu\n",
      "2023-12-18 19:10:51 INFO: Loading: tokenize\n",
      "2023-12-18 19:10:51 INFO: Loading: pos\n",
      "2023-12-18 19:10:51 INFO: Loading: lemma\n",
      "2023-12-18 19:10:51 INFO: Loading: constituency\n",
      "2023-12-18 19:10:52 INFO: Loading: depparse\n",
      "2023-12-18 19:10:52 INFO: Loading: sentiment\n",
      "2023-12-18 19:10:52 INFO: Loading: ner\n",
      "2023-12-18 19:10:52 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# instantiate stanza pipeline\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', \n",
    "                      depparse_batch_size=50, \n",
    "                      depparse_min_length_to_batch_separately=50,\n",
    "                      verbose=True,\n",
    "                      use_gpu=False,\n",
    "                    #   batch_size=100\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# starter idea for making an experiment name can be the git branch, but need more specificity\n",
    "experiment_name = f\"{DAGSHUB_EMAIL}/bertopic_stanza_small_set_v1\"\n",
    "mlflow_exp_id = get_experiment_id(experiment_name)\n",
    "\n",
    "# run_id that has the logged info needed\n",
    "mlflow_run_id = 'c6fbcf396af34ee3aade5503ee01c2bb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# load dataframes from artifacts\n",
    "# mlflow.artifacts.download_artifacts(\n",
    "#     run_id=mlflow_run_id\n",
    "# )\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':3,\n",
    "}\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':5,\n",
    "    'nr_topics':'auto',\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(cv_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=mlflow_exp_id):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Raw Dataframe:', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    to_nlp_df = pre_proc_df[0:50]\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('Subset Dataframe:', end='\\n')\n",
    "    print(to_nlp_df.head())\n",
    "    print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params,\n",
    "    )\n",
    "\n",
    "    def custom_analyzer(step_list, stanza_pipeline, minNgramLength, maxNgramLength):\n",
    "            lowered = \" brk \".join(map(str, [step for step in step_list if step is not None])).lower()\n",
    "\n",
    "            preproc = stanza_pipeline(lowered)\n",
    "            \n",
    "            lemmad = \" \".join(map(str,\n",
    "                                [word.text\n",
    "                                for sent in preproc.sentences \n",
    "                                for word in sent.words if (\n",
    "                                    word is not None\n",
    "                                )]\n",
    "                            )\n",
    "                        )\n",
    "            \n",
    "            # analyze each line of the input string seperately\n",
    "            for ln in lemmad.split(' brk '):\n",
    "                \n",
    "                # tokenize the input string (customize the regex as desired)\n",
    "                at_least_two_english_characters_whole_words = \"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "                terms = re.split(at_least_two_english_characters_whole_words, ln)\n",
    "\n",
    "                # loop ngram creation for every number between min and max ngram length\n",
    "                for ngramLength in range(minNgramLength, maxNgramLength+1):\n",
    "\n",
    "                    # find and return all ngrams\n",
    "                    # for ngram in zip(*[terms[i:] for i in range(3)]): \n",
    "                        # <-- solution without a generator (works the same but has higher memory usage)\n",
    "                    for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]):   # <-- solution using a generator\n",
    "                        \n",
    "                        ngram = ' '.join(map(str, ngram))\n",
    "                        # yield ngram\n",
    "                        return str(ngram)\n",
    "\n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4}\n",
    "    \n",
    "    recipe_steps = to_nlp_df[\"prepSteps\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('Recipe steps:', end='\\n')\n",
    "    print(recipe_steps)\n",
    "\n",
    "    # train on the recipes' steps\n",
    "    topics, probs = topic_model.fit_transform(recipe_steps)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    # steps_vectorizer_model = CountVectorizer(**cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    steps_vectorizer_model = OnlineCountVectorizer(**cv_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_steps\n",
    "        , vectorizer_model=steps_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_small_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_small_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# load dataframes from artifacts\n",
    "# mlflow.artifacts.download_artifacts(\n",
    "#     run_id=mlflow_run_id\n",
    "# )\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':'auto',\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(cv_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_stanza_ingreds_full_set_v1\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Raw Dataframe:', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    # to_nlp_df = pre_proc_df[0:50]\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Subset Dataframe:', end='\\n')\n",
    "    # print(to_nlp_df.head())\n",
    "    # print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params,\n",
    "    )\n",
    "\n",
    "    def custom_analyzer(step_list, stanza_pipeline, minNgramLength, maxNgramLength):\n",
    "            lowered = \" brk \".join(map(str, [step for step in step_list if step is not None])).lower()\n",
    "\n",
    "            preproc = stanza_pipeline(lowered)\n",
    "            \n",
    "            lemmad = \" \".join(map(str,\n",
    "                                [word.text\n",
    "                                for sent in preproc.sentences \n",
    "                                for word in sent.words if (\n",
    "                                    word is not None\n",
    "                                )]\n",
    "                            )\n",
    "                        )\n",
    "            \n",
    "            # analyze each line of the input string seperately\n",
    "            for ln in lemmad.split(' brk '):\n",
    "                \n",
    "                # tokenize the input string (customize the regex as desired)\n",
    "                at_least_two_english_characters_whole_words = \"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "                terms = re.split(at_least_two_english_characters_whole_words, ln)\n",
    "\n",
    "                # loop ngram creation for every number between min and max ngram length\n",
    "                for ngramLength in range(minNgramLength, maxNgramLength+1):\n",
    "\n",
    "                    # find and return all ngrams\n",
    "                    # for ngram in zip(*[terms[i:] for i in range(3)]): \n",
    "                        # <-- solution without a generator (works the same but has higher memory usage)\n",
    "                    for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]):   # <-- solution using a generator\n",
    "                        \n",
    "                        ngram = ' '.join(map(str, ngram))\n",
    "                        # yield ngram\n",
    "                        return str(ngram)\n",
    "\n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4}\n",
    "    \n",
    "    recipe_ingreds = pre_proc_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('Recipe ingredients:', end='\\n')\n",
    "    print(recipe_ingreds)\n",
    "\n",
    "    # train on the recipes' steps\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    # steps_vectorizer_model = CountVectorizer(**cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    steps_vectorizer_model = OnlineCountVectorizer(**cv_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=steps_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_full_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_full_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt run with lighter weight configuration\n",
    "#### This attempt will still use Stanza processing on the ingredients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_nlp_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# create sklearn pipeline as in BERTopic lightweight configuration\n",
    "# pipe = make_pipeline(\n",
    "#     TfidfVectorizer(**sklearn_nlp_params),\n",
    "#     TruncatedSVD(100)\n",
    "# )\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    # 'embedding_model': TfidfVectorizer(**sklearn_nlp_params),\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':50,\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True,\n",
    "    # 'min_cluster_size': 10 # Possibly only works if modifying individual HDBSCAN component of BERTopic\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_nlp_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_lightweight_stanza_ingreds_small_set_v1\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Raw Dataframe: ', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    to_nlp_df = pre_proc_df[0:100]\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Subset Dataframe:', end='\\n')\n",
    "    print(to_nlp_df.head())\n",
    "    print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params\n",
    "    )\n",
    "    \n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4}\n",
    "    \n",
    "    recipe_ingreds = to_nlp_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(**sklearn_nlp_params)\n",
    "    embeddings = vectorizer.fit_transform(recipe_ingreds)\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    print(recipe_ingreds)\n",
    "\n",
    "    # train on the recipes' ingredientss\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds, embeddings)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    sklearn_cv_params = {\n",
    "        'strip_accents':\"unicode\",\n",
    "        'lowercase':True,\n",
    "        'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "        # 'min_df':10,\n",
    "    }\n",
    "    steps_vectorizer_model = CountVectorizer(**sklearn_cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    # steps_vectorizer_model = OnlineCountVectorizer(**sklearn_nlp_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=steps_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representative Docs:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_full_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_full_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()['Representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()['Representation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_nlp_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# create sklearn pipeline as in BERTopic lightweight configuration\n",
    "# pipe = make_pipeline(\n",
    "#     TfidfVectorizer(**sklearn_nlp_params),\n",
    "#     TruncatedSVD(100)\n",
    "# )\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    # 'embedding_model': TfidfVectorizer(**sklearn_nlp_params),\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':50,\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True,\n",
    "    # 'min_cluster_size': 10 # Possibly only works if modifying individual HDBSCAN component of BERTopic\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_nlp_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_lightweight_stanza_ingreds_small_set_v1\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Raw Dataframe: ', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    to_nlp_df = pre_proc_df[0:100]\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Subset Dataframe:', end='\\n')\n",
    "    print(to_nlp_df.head())\n",
    "    print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params\n",
    "    )\n",
    "    \n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4}\n",
    "    \n",
    "    recipe_ingreds = to_nlp_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(**sklearn_nlp_params)\n",
    "    embeddings = vectorizer.fit_transform(recipe_ingreds)\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    print(recipe_ingreds)\n",
    "\n",
    "    # train on the recipes' ingredientss\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds, embeddings)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    sklearn_cv_params = {\n",
    "        'strip_accents':\"unicode\",\n",
    "        'lowercase':True,\n",
    "        # 'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "        # 'min_df':10,\n",
    "    }\n",
    "    steps_vectorizer_model = CountVectorizer(**sklearn_cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    # steps_vectorizer_model = OnlineCountVectorizer(**sklearn_nlp_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=steps_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representative Docs:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_full_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_full_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------\n",
      "2023-12-18 18:13:41.917123, Raw Dataframe: \n",
      "                         id  \\\n",
      "0  54a2b6b019925f464b373351   \n",
      "1  54a408a019925f464b3733bc   \n",
      "2  54a408a26529d92b2c003631   \n",
      "3  54a408a66529d92b2c003638   \n",
      "4  54a408a719925f464b3733cc   \n",
      "\n",
      "                                                 dek  \\\n",
      "0  How does fried chicken achieve No. 1 status? B...   \n",
      "1                                Spinaci all'Ebraica   \n",
      "2  This majestic, moist, and richly spiced honey ...   \n",
      "3  The idea for this sandwich came to me when my ...   \n",
      "4  In 1930, Simon Agranat, the chief justice of t...   \n",
      "\n",
      "                                     hed                   pubDate  \\\n",
      "0            Pickle-Brined Fried Chicken  2014-08-19T04:00:00.000Z   \n",
      "1                   Spinach Jewish Style  2008-09-09T04:00:00.000Z   \n",
      "2                  New Year’s Honey Cake  2008-09-10T04:00:00.000Z   \n",
      "3  The B.L.A.Bagel with Lox and Avocado  2008-09-08T04:00:00.000Z   \n",
      "4        Shakshuka a la Doktor Shakshuka  2008-09-09T04:00:00.000Z   \n",
      "\n",
      "                             author    type  \\\n",
      "0                                []  recipe   \n",
      "1  [{'name': 'Edda Servi Machlin'}]  recipe   \n",
      "2       [{'name': 'Marcy Goldman'}]  recipe   \n",
      "3           [{'name': 'Faye Levy'}]  recipe   \n",
      "4         [{'name': 'Joan Nathan'}]  recipe   \n",
      "\n",
      "                                                 url  \\\n",
      "0  /recipes/food/views/pickle-brined-fried-chicke...   \n",
      "1    /recipes/food/views/spinach-jewish-style-350152   \n",
      "2  /recipes/food/views/majestic-and-moist-new-yea...   \n",
      "3  /recipes/food/views/the-b-l-a-bagel-with-lox-a...   \n",
      "4  /recipes/food/views/shakshuka-a-la-doktor-shak...   \n",
      "\n",
      "                                           photoData  \\\n",
      "0  {'id': '54a2b64a6529d92b2c003409', 'filename':...   \n",
      "1  {'id': '56746182accb4c9831e45e0a', 'filename':...   \n",
      "2  {'id': '55e85ba4cf90d6663f728014', 'filename':...   \n",
      "3  {'id': '5674617e47d1a28026045e4f', 'filename':...   \n",
      "4  {'id': '56746183b47c050a284a4e15', 'filename':...   \n",
      "\n",
      "                                                 tag  aggregateRating  \\\n",
      "0  {'category': 'ingredient', 'name': 'Chicken', ...             3.11   \n",
      "1  {'category': 'cuisine', 'name': 'Italian', 'ur...             3.22   \n",
      "2  {'category': 'cuisine', 'name': 'Jewish', 'url...             3.62   \n",
      "3  {'category': 'cuisine', 'name': 'Jewish', 'url...             4.00   \n",
      "4  {'category': 'cuisine', 'name': 'Jewish', 'url...             2.71   \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  [1 tablespoons yellow mustard seeds, 1 tablesp...   \n",
      "1  [3 pounds small-leaved bulk spinach, Salt, 1/2...   \n",
      "2  [3 1/2 cups all-purpose flour, 1 tablespoon ba...   \n",
      "3  [1 small ripe avocado, preferably Hass (see No...   \n",
      "4  [2 pounds fresh tomatoes, unpeeled and cut in ...   \n",
      "\n",
      "                                           prepSteps  reviewsCount  \\\n",
      "0  [Toast mustard and coriander seeds in a dry me...             7   \n",
      "1  [Remove the stems and roots from the spinach. ...             5   \n",
      "2  [I like this cake best baked in a 9-inch angel...           105   \n",
      "3  [A short time before serving, mash avocado and...             7   \n",
      "4  [1. Place the tomatoes, garlic, salt, paprika,...             7   \n",
      "\n",
      "   willMakeAgainPct  dateCrawled  \n",
      "0               100   1498547035  \n",
      "1                80   1498547740  \n",
      "2                88   1498547738  \n",
      "3               100   1498547740  \n",
      "4                83   1498547740  \n",
      "(34756, 15)\n",
      "\n",
      "\n",
      "--------------\n",
      "2023-12-18 18:13:42.360889, Preprocessed Dataframe:\n",
      "                                                                        dek  \\\n",
      "id                                                                            \n",
      "54a2b6b019925f464b373351  How does fried chicken achieve No. 1 status? B...   \n",
      "54a408a019925f464b3733bc                                Spinaci all'Ebraica   \n",
      "54a408a26529d92b2c003631  This majestic, moist, and richly spiced honey ...   \n",
      "54a408a66529d92b2c003638  The idea for this sandwich came to me when my ...   \n",
      "54a408a719925f464b3733cc  In 1930, Simon Agranat, the chief justice of t...   \n",
      "\n",
      "                                                            hed  \\\n",
      "id                                                                \n",
      "54a2b6b019925f464b373351            Pickle-Brined Fried Chicken   \n",
      "54a408a019925f464b3733bc                   Spinach Jewish Style   \n",
      "54a408a26529d92b2c003631                  New Year’s Honey Cake   \n",
      "54a408a66529d92b2c003638  The B.L.A.Bagel with Lox and Avocado   \n",
      "54a408a719925f464b3733cc        Shakshuka a la Doktor Shakshuka   \n",
      "\n",
      "                          aggregateRating  \\\n",
      "id                                          \n",
      "54a2b6b019925f464b373351             3.11   \n",
      "54a408a019925f464b3733bc             3.22   \n",
      "54a408a26529d92b2c003631             3.62   \n",
      "54a408a66529d92b2c003638             4.00   \n",
      "54a408a719925f464b3733cc             2.71   \n",
      "\n",
      "                                                                ingredients  \\\n",
      "id                                                                            \n",
      "54a2b6b019925f464b373351  [1 tablespoons yellow mustard seeds, 1 tablesp...   \n",
      "54a408a019925f464b3733bc  [3 pounds small-leaved bulk spinach, Salt, 1/2...   \n",
      "54a408a26529d92b2c003631  [3 1/2 cups all-purpose flour, 1 tablespoon ba...   \n",
      "54a408a66529d92b2c003638  [1 small ripe avocado, preferably Hass (see No...   \n",
      "54a408a719925f464b3733cc  [2 pounds fresh tomatoes, unpeeled and cut in ...   \n",
      "\n",
      "                                                                  prepSteps  \\\n",
      "id                                                                            \n",
      "54a2b6b019925f464b373351  [Toast mustard and coriander seeds in a dry me...   \n",
      "54a408a019925f464b3733bc  [Remove the stems and roots from the spinach. ...   \n",
      "54a408a26529d92b2c003631  [I like this cake best baked in a 9-inch angel...   \n",
      "54a408a66529d92b2c003638  [A short time before serving, mash avocado and...   \n",
      "54a408a719925f464b3733cc  [1. Place the tomatoes, garlic, salt, paprika,...   \n",
      "\n",
      "                          reviewsCount  willMakeAgainPct     cuisine_name  \\\n",
      "id                                                                          \n",
      "54a2b6b019925f464b373351             7               100  Missing Cuisine   \n",
      "54a408a019925f464b3733bc             5                80          Italian   \n",
      "54a408a26529d92b2c003631           105                88           Kosher   \n",
      "54a408a66529d92b2c003638             7               100           Kosher   \n",
      "54a408a719925f464b3733cc             7                83           Kosher   \n",
      "\n",
      "                                               photo_filename  \\\n",
      "id                                                              \n",
      "54a2b6b019925f464b373351       51247610_fried-chicken_1x1.jpg   \n",
      "54a408a019925f464b3733bc  EP_12162015_placeholders_rustic.jpg   \n",
      "54a408a26529d92b2c003631          EP_09022015_honeycake-2.jpg   \n",
      "54a408a66529d92b2c003638  EP_12162015_placeholders_casual.jpg   \n",
      "54a408a719925f464b3733cc  EP_12162015_placeholders_formal.jpg   \n",
      "\n",
      "                                                               photo_credit  \\\n",
      "id                                                                            \n",
      "54a2b6b019925f464b373351                Michael Graydon and Nikole Herriott   \n",
      "54a408a019925f464b3733bc  Photo by Chelsea Kyle, Prop Styling by Anna St...   \n",
      "54a408a26529d92b2c003631  Photo by Chelsea Kyle, Food Styling by Anna St...   \n",
      "54a408a66529d92b2c003638  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a408a719925f464b3733cc  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "\n",
      "                                  author_name            date_published  \\\n",
      "id                                                                        \n",
      "54a2b6b019925f464b373351  Missing Author Name 2014-08-19 04:00:00+00:00   \n",
      "54a408a019925f464b3733bc   Edda Servi Machlin 2008-09-09 04:00:00+00:00   \n",
      "54a408a26529d92b2c003631        Marcy Goldman 2008-09-10 04:00:00+00:00   \n",
      "54a408a66529d92b2c003638            Faye Levy 2008-09-08 04:00:00+00:00   \n",
      "54a408a719925f464b3733cc          Joan Nathan 2008-09-09 04:00:00+00:00   \n",
      "\n",
      "                                                                 recipe_url  \n",
      "id                                                                           \n",
      "54a2b6b019925f464b373351  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a408a019925f464b3733bc  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a408a26529d92b2c003631  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a408a66529d92b2c003638  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a408a719925f464b3733cc  https://www.epicurious.com/recipes/food/views/...  \n",
      "(34656, 13)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2023-12-18 18:13:42.380564, Subset Dataframe:\n",
      "                                                                        dek  \\\n",
      "id                                                                            \n",
      "54a2b6b019925f464b373351  How does fried chicken achieve No. 1 status? B...   \n",
      "54a408a019925f464b3733bc                                Spinaci all'Ebraica   \n",
      "54a408a26529d92b2c003631  This majestic, moist, and richly spiced honey ...   \n",
      "54a408a66529d92b2c003638  The idea for this sandwich came to me when my ...   \n",
      "54a408a719925f464b3733cc  In 1930, Simon Agranat, the chief justice of t...   \n",
      "\n",
      "                                                            hed  \\\n",
      "id                                                                \n",
      "54a2b6b019925f464b373351            Pickle-Brined Fried Chicken   \n",
      "54a408a019925f464b3733bc                   Spinach Jewish Style   \n",
      "54a408a26529d92b2c003631                  New Year’s Honey Cake   \n",
      "54a408a66529d92b2c003638  The B.L.A.Bagel with Lox and Avocado   \n",
      "54a408a719925f464b3733cc        Shakshuka a la Doktor Shakshuka   \n",
      "\n",
      "                          aggregateRating  \\\n",
      "id                                          \n",
      "54a2b6b019925f464b373351             3.11   \n",
      "54a408a019925f464b3733bc             3.22   \n",
      "54a408a26529d92b2c003631             3.62   \n",
      "54a408a66529d92b2c003638             4.00   \n",
      "54a408a719925f464b3733cc             2.71   \n",
      "\n",
      "                                                                ingredients  \\\n",
      "id                                                                            \n",
      "54a2b6b019925f464b373351  [1 tablespoons yellow mustard seeds, 1 tablesp...   \n",
      "54a408a019925f464b3733bc  [3 pounds small-leaved bulk spinach, Salt, 1/2...   \n",
      "54a408a26529d92b2c003631  [3 1/2 cups all-purpose flour, 1 tablespoon ba...   \n",
      "54a408a66529d92b2c003638  [1 small ripe avocado, preferably Hass (see No...   \n",
      "54a408a719925f464b3733cc  [2 pounds fresh tomatoes, unpeeled and cut in ...   \n",
      "\n",
      "                                                                  prepSteps  \\\n",
      "id                                                                            \n",
      "54a2b6b019925f464b373351  [Toast mustard and coriander seeds in a dry me...   \n",
      "54a408a019925f464b3733bc  [Remove the stems and roots from the spinach. ...   \n",
      "54a408a26529d92b2c003631  [I like this cake best baked in a 9-inch angel...   \n",
      "54a408a66529d92b2c003638  [A short time before serving, mash avocado and...   \n",
      "54a408a719925f464b3733cc  [1. Place the tomatoes, garlic, salt, paprika,...   \n",
      "\n",
      "                          reviewsCount  willMakeAgainPct     cuisine_name  \\\n",
      "id                                                                          \n",
      "54a2b6b019925f464b373351             7               100  Missing Cuisine   \n",
      "54a408a019925f464b3733bc             5                80          Italian   \n",
      "54a408a26529d92b2c003631           105                88           Kosher   \n",
      "54a408a66529d92b2c003638             7               100           Kosher   \n",
      "54a408a719925f464b3733cc             7                83           Kosher   \n",
      "\n",
      "                                               photo_filename  \\\n",
      "id                                                              \n",
      "54a2b6b019925f464b373351       51247610_fried-chicken_1x1.jpg   \n",
      "54a408a019925f464b3733bc  EP_12162015_placeholders_rustic.jpg   \n",
      "54a408a26529d92b2c003631          EP_09022015_honeycake-2.jpg   \n",
      "54a408a66529d92b2c003638  EP_12162015_placeholders_casual.jpg   \n",
      "54a408a719925f464b3733cc  EP_12162015_placeholders_formal.jpg   \n",
      "\n",
      "                                                               photo_credit  \\\n",
      "id                                                                            \n",
      "54a2b6b019925f464b373351                Michael Graydon and Nikole Herriott   \n",
      "54a408a019925f464b3733bc  Photo by Chelsea Kyle, Prop Styling by Anna St...   \n",
      "54a408a26529d92b2c003631  Photo by Chelsea Kyle, Food Styling by Anna St...   \n",
      "54a408a66529d92b2c003638  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a408a719925f464b3733cc  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "\n",
      "                                  author_name            date_published  \\\n",
      "id                                                                        \n",
      "54a2b6b019925f464b373351  Missing Author Name 2014-08-19 04:00:00+00:00   \n",
      "54a408a019925f464b3733bc   Edda Servi Machlin 2008-09-09 04:00:00+00:00   \n",
      "54a408a26529d92b2c003631        Marcy Goldman 2008-09-10 04:00:00+00:00   \n",
      "54a408a66529d92b2c003638            Faye Levy 2008-09-08 04:00:00+00:00   \n",
      "54a408a719925f464b3733cc          Joan Nathan 2008-09-09 04:00:00+00:00   \n",
      "\n",
      "                                                                 recipe_url  \n",
      "id                                                                           \n",
      "54a2b6b019925f464b373351  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a408a019925f464b3733bc  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a408a26529d92b2c003631  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a408a66529d92b2c003638  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a408a719925f464b3733cc  https://www.epicurious.com/recipes/food/views/...  \n",
      "(100, 13)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2023-12-18 18:20:03.980036, Recipe ingredients:\n",
      "['tablespoon yellow mustard seed', 'pound small leave bulk spinach', 'cup purpose flour', 'small ripe avocado hass see note', 'pound fresh tomato unpeeled cut quarter ounce can tomato', 'cup cook rice', 'tablespoon shorten', 'cup sugar', 'cup vegetable shortening', 'purchase inch angel food cake', 'shorten need', 'teaspoon fresh lemon juice divide', 'large dry guajillo chile dry new mexico chile stem seed', 'large egg', 'lemon', 'tablespoon yellow mustard seed ounce', 'cup apple juice', 'pound large unripe green tomato core cut / inch dice', 'tablespoon more ice water', 'lemon cut half', 'cup sugar divide', 'cup pure maple syrup grade benefit', 'cup purpose flour', 'teaspoon whole black peppercorn', 'dry tart cherry ounce', 'cup purpose flour', 'tablespoon butter', 'cup stick unsalt butter', 'tablespoon extra-virgin olive oil', 'pound medium parsnip peel cut diagonal inch thick slice', 'pound head cauliflower core cut inch floret', 'tablespoon butter', 'cup fresh breadcrumb crustless French bread ground processor', 'pound inch long beef short rib', 'pound plum tomato', 'cup inch cube peel seed butternut squash pound squash', 'cup hazelnut ounce toast', 'pound rhubarb stalk trim cut inch piece', 'tablespoon butter', 'ounce mix seafood such firm fish fillet salmon kingfish halibut fish steak tuna swordfish sea scallop peel devein shrimp skin bone shell remove', 'cup sugar', 'medium navel orange', 'cup purpose flour', 'tablespoon fresh lemon juice', 'large red heirloom tomato peel seed', 'cup plain yogurt', 'juice lemon cup', 'ounce package dry porcini mushroom', 'large dry ancho chile', 'nonstick vegetable oil spray', 'cup purpose flour', 'nonstick vegetable oil spray', 'cup water', 'teaspoon paprika', 'tablespoon stick butter', 'bacon slice chop', 'skinless boneless chicken breast half', 'large tomato ounce', 'nonstick vegetable oil spray', 'tablespoon grate parmesan cheese parmesan cheese shaving', 'pound red tomato peel seed chop cup divide', 'lb gram ground lamb beef veal', 'egg', 'pound baby potato new red potato unpeel', 'pound fusilli other corkscrew pasta', 'tsp saffron thread', 'tablespoon soy sauce divide', 'tablespoon olive oil divide', 'teaspoon ground cumin', 'teaspoon coriander seed', 'cup plain nonfat yogurt', 'ounce dry morel porcini mushroom', 'large garlic clove press', 'tablespoon extra-virgin olive oil', 'cup chop onion', 'pound flank steak trim', 'quart reduce sodium chicken broth', 'cup olive oil', 'cup dry white bean such great northern navy lb pick rinse', 'slice firm white sandwich bread', 'fresh mint sprig', 'inch cup capacity bundt style pan', 'stick unsalted butter soften', 'cup whole milk', 'grape cherry tomato halve', 'tablespoon olive oil divide', 'boneless skinless chicken breast ounce', 'center -cut salmon fillet oz.', 'red bell pepper', 'large egg', 'firm pear', 'tablespoon unsalted butter room temperature', '2 small zucchini yellow squash end trim', 'pound sweet hot italian sausage pork turkey', 'cup water', 'cup steam hot low fat milk', 'carrot chop', 'large egg separate room temperature minute', 'piece bubblegum double bubble cut half', 'pound medium shrimp shell']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:51<00:00,  2.31s/it]\n",
      "2023-12-18 18:23:57,887 - BERTopic - Reduced dimensionality\n",
      "2023-12-18 18:23:57,895 - BERTopic - Clustered reduced embeddings\n",
      "2023-12-18 18:23:57,908 - BERTopic - Reduced number of topics from 5 to 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2023-12-18 18:23:57.932116, BERTopic Model Dataframe:\n",
      "   Topic  Count                            Name  \\\n",
      "0     -1      6     -1_sugar_chop_spinach_leave   \n",
      "1      0     12    0_flour_purpose_cup_mushroom   \n",
      "2      1     55     1_tablespoon_large_cup_seed   \n",
      "3      2     14  2_oil_nonstick_spray_vegetable   \n",
      "4      3     13       3_fish_fillet_such_salmon   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [sugar, chop, spinach, leave, carrot, bulk, ba...   \n",
      "1  [flour, purpose, cup, mushroom, porcini, tomat...   \n",
      "2  [tablespoon, large, cup, seed, pound, butter, ...   \n",
      "3  [oil, nonstick, spray, vegetable, olive, milk,...   \n",
      "4  [fish, fillet, such, salmon, olive, tablespoon...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [cup sugar, bacon slice chop, pound small leav...  \n",
      "1  [cup purpose flour, cup purpose flour, cup pur...  \n",
      "2  [cup inch cube peel seed butternut squash poun...  \n",
      "3  [nonstick vegetable oil spray, nonstick vegeta...  \n",
      "4  [center -cut salmon fillet oz., cup dry white ...  \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2023-12-18 18:23:57.941604, BERTopic Model Representations:\n",
      "0    [sugar, chop, spinach, leave, carrot, bulk, ba...\n",
      "1    [flour, purpose, cup, mushroom, porcini, tomat...\n",
      "2    [tablespoon, large, cup, seed, pound, butter, ...\n",
      "3    [oil, nonstick, spray, vegetable, olive, milk,...\n",
      "4    [fish, fillet, such, salmon, olive, tablespoon...\n",
      "Name: Representation, dtype: object\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2023-12-18 18:23:57.952229, BERTopic Model Representative Docs:\n",
      "0    [cup sugar, bacon slice chop, pound small leav...\n",
      "1    [cup purpose flour, cup purpose flour, cup pur...\n",
      "2    [cup inch cube peel seed butternut squash poun...\n",
      "3    [nonstick vegetable oil spray, nonstick vegeta...\n",
      "4    [center -cut salmon fillet oz., cup dry white ...\n",
      "Name: Representative_Docs, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_nlp_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# create sklearn pipeline as in BERTopic lightweight configuration\n",
    "# pipe = make_pipeline(\n",
    "#     TfidfVectorizer(**sklearn_nlp_params),\n",
    "#     TruncatedSVD(100)\n",
    "# )\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    # 'embedding_model': TfidfVectorizer(**sklearn_nlp_params),\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':50,\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True,\n",
    "    # 'min_cluster_size': 10 # Possibly only works if modifying individual HDBSCAN component of BERTopic\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_nlp_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_lightweight_stanza_ingreds_small_set_v1.01\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Raw Dataframe: ', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    to_nlp_df = pre_proc_df[0:100]\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Subset Dataframe:', end='\\n')\n",
    "    print(to_nlp_df.head())\n",
    "    print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params\n",
    "    )\n",
    "    \n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4\n",
    "                       , 'lemmatize': True}\n",
    "    \n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    recipe_ingreds = to_nlp_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    print([ingred for ingred in recipe_ingreds])\n",
    "\n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(**sklearn_nlp_params)\n",
    "    embeddings = vectorizer.fit_transform(tqdm(recipe_ingreds))\n",
    "\n",
    "    # train on the recipes' ingredientss\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds, embeddings)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    sklearn_cv_params = {\n",
    "        # 'strip_accents':\"unicode\",\n",
    "        # 'lowercase':True,\n",
    "        # 'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "        # 'min_df':10,\n",
    "        'token_pattern': r\"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "    }\n",
    "    ingreds_vectorizer_model = CountVectorizer(**sklearn_cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    # steps_vectorizer_model = OnlineCountVectorizer(**sklearn_nlp_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=ingreds_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representative Docs:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_small_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_small_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 tablespoons yellow mustard seeds',\n",
       " '1 tablespoons brown mustard seeds',\n",
       " '1 1/2 teaspoons coriander seeds',\n",
       " '1 cup apple cider vinegar',\n",
       " '2/3 cup kosher salt',\n",
       " '1/3 cup sugar',\n",
       " '1/4 cup chopped fresh dill',\n",
       " '8 skinless, boneless chicken thighs (about 3 pounds), halved, quartered if large',\n",
       " 'Vegetable oil (for frying; about 10 cups)',\n",
       " '2 cups buttermilk',\n",
       " '2 cups all-purpose flour',\n",
       " 'Kosher salt',\n",
       " 'Honey, flaky sea salt (such as Maldon), toasted benne or sesame seeds, hot sauce (for serving)',\n",
       " 'A deep-fry thermometer']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_nlp_df['ingredients'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tablespoon yellow mustard seed\n",
      "pound small - leave bulk spinach\n",
      "cup - purpose flour\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_recipe_ingreds \u001b[38;5;241m=\u001b[39m \u001b[43mto_nlp_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mingredients\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_analyzer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43manalyzer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/apply.py:142\u001b[0m, in \u001b[0;36mApply.__init__.<locals>.f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m, in \u001b[0;36mcustom_analyzer\u001b[0;34m(step_list, stanza_pipeline, minNgramLength, maxNgramLength, lemmatize)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_analyzer\u001b[39m(step_list, stanza_pipeline, minNgramLength, maxNgramLength, lemmatize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     lowered \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m brk \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, [step \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m step_list \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]))\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m----> 4\u001b[0m     preproc \u001b[38;5;241m=\u001b[39m \u001b[43mstanza_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlowered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lemmatize:\n\u001b[1;32m      7\u001b[0m         lemmad \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      8\u001b[0m                             [word\u001b[38;5;241m.\u001b[39mlemma\n\u001b[1;32m      9\u001b[0m                             \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m preproc\u001b[38;5;241m.\u001b[39msentences \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m                         )\n\u001b[1;32m     15\u001b[0m                     )\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/stanza/pipeline/core.py:471\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/stanza/pipeline/core.py:422\u001b[0m, in \u001b[0;36mPipeline.process\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mget(processor_name):\n\u001b[1;32m    421\u001b[0m         process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mbulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mprocess\n\u001b[0;32m--> 422\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/stanza/pipeline/lemma_processor.py:88\u001b[0m, in \u001b[0;36mLemmaProcessor.process\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     86\u001b[0m edits \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seq2seq_batch):\n\u001b[0;32m---> 88\u001b[0m     ps, es \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ps\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m es \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/stanza/models/lemma/trainer.py:105\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, batch, beam_size)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    104\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m preds, edit_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m pred_seqs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munmap(ids) \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m preds] \u001b[38;5;66;03m# unmap to tokens\u001b[39;00m\n\u001b[1;32m    107\u001b[0m pred_seqs \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mprune_decoded_seqs(pred_seqs)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/stanza/models/common/seq2seq_model.py:257\u001b[0m, in \u001b[0;36mSeq2SeqModel.predict\u001b[0;34m(self, src, src_mask, pos, beam_size, raw)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Predict with beam search. \"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beam_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_greedy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m enc_inputs, batch_size, src_lens, src_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(src, src_mask, pos, raw)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# (1) encode source\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/stanza/models/common/seq2seq_model.py:239\u001b[0m, in \u001b[0;36mSeq2SeqModel.predict_greedy\u001b[0;34m(self, src, src_mask, pos, raw)\u001b[0m\n\u001b[1;32m    236\u001b[0m output_seqs \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)]\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m total_done \u001b[38;5;241m<\u001b[39m batch_size \u001b[38;5;129;01mand\u001b[39;00m max_len \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_dec_len:\n\u001b[0;32m--> 239\u001b[0m     log_probs, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m log_probs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput must have 1-step of output.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m     _, preds \u001b[38;5;241m=\u001b[39m log_probs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/stanza/models/common/seq2seq_model.py:145\u001b[0m, in \u001b[0;36mSeq2SeqModel.decode\u001b[0;34m(self, dec_inputs, hn, cn, ctx, ctx_mask, src)\u001b[0m\n\u001b[1;32m    143\u001b[0m decoder_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec2vocab(h_out_reshape)\n\u001b[1;32m    144\u001b[0m decoder_logits \u001b[38;5;241m=\u001b[39m decoder_logits\u001b[38;5;241m.\u001b[39mview(h_out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), h_out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 145\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy:\n\u001b[1;32m    148\u001b[0m     copy_logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_gate(h_out)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/stanza/models/common/seq2seq_model.py:212\u001b[0m, in \u001b[0;36mSeq2SeqModel.get_log_prob\u001b[0;34m(self, logits)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, logits):\n\u001b[1;32m    211\u001b[0m     logits_reshape \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size)\n\u001b[0;32m--> 212\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits_reshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logits\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m log_probs\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1932\u001b[0m, in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1930\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_softmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1934\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mlog_softmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_recipe_ingreds = to_nlp_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i think i should start leaving out units/including stopwords again since i'm not using Stanza's deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------\n",
      "2023-12-19 20:08:42.203994, Raw Dataframe: \n",
      "                         id  \\\n",
      "0  54a2b6b019925f464b373351   \n",
      "1  54a408a019925f464b3733bc   \n",
      "2  54a408a26529d92b2c003631   \n",
      "3  54a408a66529d92b2c003638   \n",
      "4  54a408a719925f464b3733cc   \n",
      "\n",
      "                                                 dek  \\\n",
      "0  How does fried chicken achieve No. 1 status? B...   \n",
      "1                                Spinaci all'Ebraica   \n",
      "2  This majestic, moist, and richly spiced honey ...   \n",
      "3  The idea for this sandwich came to me when my ...   \n",
      "4  In 1930, Simon Agranat, the chief justice of t...   \n",
      "\n",
      "                                     hed                   pubDate  \\\n",
      "0            Pickle-Brined Fried Chicken  2014-08-19T04:00:00.000Z   \n",
      "1                   Spinach Jewish Style  2008-09-09T04:00:00.000Z   \n",
      "2                  New Year’s Honey Cake  2008-09-10T04:00:00.000Z   \n",
      "3  The B.L.A.Bagel with Lox and Avocado  2008-09-08T04:00:00.000Z   \n",
      "4        Shakshuka a la Doktor Shakshuka  2008-09-09T04:00:00.000Z   \n",
      "\n",
      "                             author    type  \\\n",
      "0                                []  recipe   \n",
      "1  [{'name': 'Edda Servi Machlin'}]  recipe   \n",
      "2       [{'name': 'Marcy Goldman'}]  recipe   \n",
      "3           [{'name': 'Faye Levy'}]  recipe   \n",
      "4         [{'name': 'Joan Nathan'}]  recipe   \n",
      "\n",
      "                                                 url  \\\n",
      "0  /recipes/food/views/pickle-brined-fried-chicke...   \n",
      "1    /recipes/food/views/spinach-jewish-style-350152   \n",
      "2  /recipes/food/views/majestic-and-moist-new-yea...   \n",
      "3  /recipes/food/views/the-b-l-a-bagel-with-lox-a...   \n",
      "4  /recipes/food/views/shakshuka-a-la-doktor-shak...   \n",
      "\n",
      "                                           photoData  \\\n",
      "0  {'id': '54a2b64a6529d92b2c003409', 'filename':...   \n",
      "1  {'id': '56746182accb4c9831e45e0a', 'filename':...   \n",
      "2  {'id': '55e85ba4cf90d6663f728014', 'filename':...   \n",
      "3  {'id': '5674617e47d1a28026045e4f', 'filename':...   \n",
      "4  {'id': '56746183b47c050a284a4e15', 'filename':...   \n",
      "\n",
      "                                                 tag  aggregateRating  \\\n",
      "0  {'category': 'ingredient', 'name': 'Chicken', ...             3.11   \n",
      "1  {'category': 'cuisine', 'name': 'Italian', 'ur...             3.22   \n",
      "2  {'category': 'cuisine', 'name': 'Jewish', 'url...             3.62   \n",
      "3  {'category': 'cuisine', 'name': 'Jewish', 'url...             4.00   \n",
      "4  {'category': 'cuisine', 'name': 'Jewish', 'url...             2.71   \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  [1 tablespoons yellow mustard seeds, 1 tablesp...   \n",
      "1  [3 pounds small-leaved bulk spinach, Salt, 1/2...   \n",
      "2  [3 1/2 cups all-purpose flour, 1 tablespoon ba...   \n",
      "3  [1 small ripe avocado, preferably Hass (see No...   \n",
      "4  [2 pounds fresh tomatoes, unpeeled and cut in ...   \n",
      "\n",
      "                                           prepSteps  reviewsCount  \\\n",
      "0  [Toast mustard and coriander seeds in a dry me...             7   \n",
      "1  [Remove the stems and roots from the spinach. ...             5   \n",
      "2  [I like this cake best baked in a 9-inch angel...           105   \n",
      "3  [A short time before serving, mash avocado and...             7   \n",
      "4  [1. Place the tomatoes, garlic, salt, paprika,...             7   \n",
      "\n",
      "   willMakeAgainPct  dateCrawled  \n",
      "0               100   1498547035  \n",
      "1                80   1498547740  \n",
      "2                88   1498547738  \n",
      "3               100   1498547740  \n",
      "4                83   1498547740  \n",
      "(34756, 15)\n",
      "\n",
      "\n",
      "--------------\n",
      "2023-12-19 20:08:42.748745, Preprocessed Dataframe:\n",
      "                                                                        dek  \\\n",
      "id                                                                            \n",
      "54a2b6b019925f464b373351  How does fried chicken achieve No. 1 status? B...   \n",
      "54a408a019925f464b3733bc                                Spinaci all'Ebraica   \n",
      "54a408a26529d92b2c003631  This majestic, moist, and richly spiced honey ...   \n",
      "54a408a66529d92b2c003638  The idea for this sandwich came to me when my ...   \n",
      "54a408a719925f464b3733cc  In 1930, Simon Agranat, the chief justice of t...   \n",
      "\n",
      "                                                            hed  \\\n",
      "id                                                                \n",
      "54a2b6b019925f464b373351            Pickle-Brined Fried Chicken   \n",
      "54a408a019925f464b3733bc                   Spinach Jewish Style   \n",
      "54a408a26529d92b2c003631                  New Year’s Honey Cake   \n",
      "54a408a66529d92b2c003638  The B.L.A.Bagel with Lox and Avocado   \n",
      "54a408a719925f464b3733cc        Shakshuka a la Doktor Shakshuka   \n",
      "\n",
      "                          aggregateRating  \\\n",
      "id                                          \n",
      "54a2b6b019925f464b373351             3.11   \n",
      "54a408a019925f464b3733bc             3.22   \n",
      "54a408a26529d92b2c003631             3.62   \n",
      "54a408a66529d92b2c003638             4.00   \n",
      "54a408a719925f464b3733cc             2.71   \n",
      "\n",
      "                                                                ingredients  \\\n",
      "id                                                                            \n",
      "54a2b6b019925f464b373351  [1 tablespoons yellow mustard seeds, 1 tablesp...   \n",
      "54a408a019925f464b3733bc  [3 pounds small-leaved bulk spinach, Salt, 1/2...   \n",
      "54a408a26529d92b2c003631  [3 1/2 cups all-purpose flour, 1 tablespoon ba...   \n",
      "54a408a66529d92b2c003638  [1 small ripe avocado, preferably Hass (see No...   \n",
      "54a408a719925f464b3733cc  [2 pounds fresh tomatoes, unpeeled and cut in ...   \n",
      "\n",
      "                                                                  prepSteps  \\\n",
      "id                                                                            \n",
      "54a2b6b019925f464b373351  [Toast mustard and coriander seeds in a dry me...   \n",
      "54a408a019925f464b3733bc  [Remove the stems and roots from the spinach. ...   \n",
      "54a408a26529d92b2c003631  [I like this cake best baked in a 9-inch angel...   \n",
      "54a408a66529d92b2c003638  [A short time before serving, mash avocado and...   \n",
      "54a408a719925f464b3733cc  [1. Place the tomatoes, garlic, salt, paprika,...   \n",
      "\n",
      "                          reviewsCount  willMakeAgainPct     cuisine_name  \\\n",
      "id                                                                          \n",
      "54a2b6b019925f464b373351             7               100  Missing Cuisine   \n",
      "54a408a019925f464b3733bc             5                80          Italian   \n",
      "54a408a26529d92b2c003631           105                88           Kosher   \n",
      "54a408a66529d92b2c003638             7               100           Kosher   \n",
      "54a408a719925f464b3733cc             7                83           Kosher   \n",
      "\n",
      "                                               photo_filename  \\\n",
      "id                                                              \n",
      "54a2b6b019925f464b373351       51247610_fried-chicken_1x1.jpg   \n",
      "54a408a019925f464b3733bc  EP_12162015_placeholders_rustic.jpg   \n",
      "54a408a26529d92b2c003631          EP_09022015_honeycake-2.jpg   \n",
      "54a408a66529d92b2c003638  EP_12162015_placeholders_casual.jpg   \n",
      "54a408a719925f464b3733cc  EP_12162015_placeholders_formal.jpg   \n",
      "\n",
      "                                                               photo_credit  \\\n",
      "id                                                                            \n",
      "54a2b6b019925f464b373351                Michael Graydon and Nikole Herriott   \n",
      "54a408a019925f464b3733bc  Photo by Chelsea Kyle, Prop Styling by Anna St...   \n",
      "54a408a26529d92b2c003631  Photo by Chelsea Kyle, Food Styling by Anna St...   \n",
      "54a408a66529d92b2c003638  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a408a719925f464b3733cc  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "\n",
      "                                  author_name            date_published  \\\n",
      "id                                                                        \n",
      "54a2b6b019925f464b373351  Missing Author Name 2014-08-19 04:00:00+00:00   \n",
      "54a408a019925f464b3733bc   Edda Servi Machlin 2008-09-09 04:00:00+00:00   \n",
      "54a408a26529d92b2c003631        Marcy Goldman 2008-09-10 04:00:00+00:00   \n",
      "54a408a66529d92b2c003638            Faye Levy 2008-09-08 04:00:00+00:00   \n",
      "54a408a719925f464b3733cc          Joan Nathan 2008-09-09 04:00:00+00:00   \n",
      "\n",
      "                                                                 recipe_url  \n",
      "id                                                                           \n",
      "54a2b6b019925f464b373351  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a408a019925f464b3733bc  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a408a26529d92b2c003631  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a408a66529d92b2c003638  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a408a719925f464b3733cc  https://www.epicurious.com/recipes/food/views/...  \n",
      "(34656, 13)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2023-12-26 16:03:02.410119, Recipe ingredients:\n",
      "id\n",
      "54a2b6b019925f464b373351                       tablespoon yellow mustard seed\n",
      "54a408a019925f464b3733bc                       pound small leave bulk spinach\n",
      "54a408a26529d92b2c003631                                    cup purpose flour\n",
      "54a408a66529d92b2c003638                     small ripe avocado hass see note\n",
      "54a408a719925f464b3733cc    pound fresh tomato unpeeled cut quarter ounce ...\n",
      "                                                  ...                        \n",
      "59541a31bff3052847ae2107            tablespoon unsalt butter room temperature\n",
      "5954233ad52ca90dc28200e7        tablespoon stick salt butter room temperature\n",
      "595424c2109c972493636f83         tablespoon unsalted butter more greasing pan\n",
      "5956638625dc3d1d829b7166                                          coarse salt\n",
      "59566daa25dc3d1d829b7169    bottle millileter sour beer such almanac citra...\n",
      "Name: ingredients, Length: 34656, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34656/34656 [10:31:18<00:00,  1.09s/it]  \n",
      "/home/awchen/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/scipy/sparse/_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "2023-12-27 02:35:54,492 - BERTopic - Reduced dimensionality\n",
      "2023-12-27 02:45:04,427 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. check_pairwise_arrays expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 114\u001b[0m\n\u001b[1;32m    105\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(tqdm(recipe_ingreds))\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# print('\\n')\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# print('-' * 80)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# train on the recipes' ingredientss\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m topics, probs \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipe_ingreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Instantiate sklearn CountVectorizer\u001b[39;00m\n\u001b[1;32m    118\u001b[0m sklearn_cv_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# 'strip_accents':\"unicode\",\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# 'lowercase':True,\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_pattern\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(?u)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb[a-zA-Z]\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m2,}\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m }\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/bertopic/_bertopic.py:415\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[0;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# Reduce topics\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_topics:\n\u001b[0;32m--> 415\u001b[0m     documents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Save the top 3 most representative documents per topic\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_representative_docs(documents)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/bertopic/_bertopic.py:3603\u001b[0m, in \u001b[0;36mBERTopic._reduce_topics\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m   3601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_topics, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m   3602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_topics \u001b[38;5;241m<\u001b[39m initial_nr_topics:\n\u001b[0;32m-> 3603\u001b[0m         documents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce_to_n_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3604\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_topics, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   3605\u001b[0m     documents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_reduce_topics(documents)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/bertopic/_bertopic.py:3628\u001b[0m, in \u001b[0;36mBERTopic._reduce_to_n_topics\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m   3626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3627\u001b[0m     topic_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_tf_idf_[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outliers:, ]\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m-> 3628\u001b[0m distance_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3629\u001b[0m np\u001b[38;5;241m.\u001b[39mfill_diagonal(distance_matrix, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   3631\u001b[0m \u001b[38;5;66;03m# Cluster the topic embeddings using AgglomerativeClustering\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:1578\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1544\u001b[0m \n\u001b[1;32m   1545\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;124;03m    Returns the cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1578\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1580\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:156\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    153\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype_float\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m X \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    166\u001b[0m         X,\n\u001b[1;32m    167\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    172\u001b[0m     )\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:951\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    949\u001b[0m     )\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 951\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m     )\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m    957\u001b[0m     _assert_all_finite(\n\u001b[1;32m    958\u001b[0m         array,\n\u001b[1;32m    959\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    960\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    961\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    962\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. check_pairwise_arrays expected <= 2."
     ]
    }
   ],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_nlp_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':50,\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True,\n",
    "    # 'min_cluster_size': 10 # Possibly only works if modifying individual HDBSCAN component of BERTopic\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_nlp_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_lightweight_stanza_ingreds_full_set_v1.00\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Raw Dataframe: ', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    # to_nlp_df = pre_proc_df[0:100]\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print(f'{datetime.now()}, Subset Dataframe:', end='\\n')\n",
    "    # print(to_nlp_df.head())\n",
    "    # print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params\n",
    "    )\n",
    "    \n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4\n",
    "                       , 'lemmatize': True}\n",
    "    \n",
    "    recipe_ingreds = pre_proc_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "    \n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    print(recipe_ingreds)\n",
    "\n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(**sklearn_nlp_params)\n",
    "    embeddings = vectorizer.fit_transform(tqdm(recipe_ingreds))\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    # print(recipe_ingreds)\n",
    "\n",
    "    # train on the recipes' ingredientss\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds, embeddings)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    sklearn_cv_params = {\n",
    "        # 'strip_accents':\"unicode\",\n",
    "        # 'lowercase':True,\n",
    "        'token_pattern': r\"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "    }\n",
    "    ingreds_vectorizer_model = CountVectorizer(**sklearn_cv_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=ingreds_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representative Docs:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_full_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_full_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try splitting among CPU and GPU. Try Stanza on CPU due to its memory usage\n",
    "nlp2 = stanza.Pipeline('en', use_gpu=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
