{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: test\n",
    "output-file: template.html\n",
    "title: Template\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import OnlineCountVectorizer\n",
    "import dagshub\n",
    "from datetime import datetime\n",
    "import dill as pickle\n",
    "import dvc.api\n",
    "from hdbscan import HDBSCAN\n",
    "from itertools import tee, islice\n",
    "import joblib\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import nbdev\n",
    "from nbdev.showdoc import *\n",
    "import pandas as pd\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer\n",
    "    , TfidfTransformer\n",
    "    , TfidfVectorizer\n",
    "    , \n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from src.custom_sklearn_text_transformer_mlflow import CustomSKLearnAnalyzer\n",
    "from src.custom_stanza_mlflow import CustomSKLearnWrapper\n",
    "import src.dataframe_preprocessor as dfpp\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | export\n",
    "def foo():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# this function allows us to get the experiment ID from an experiment name\n",
    "def get_experiment_id(name):\n",
    "    exp = mlflow.get_experiment_by_name(name)\n",
    "    if exp is None:\n",
    "      exp_id = mlflow.create_experiment(name)\n",
    "      return exp_id\n",
    "    return exp.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_analyzer(step_list, stanza_pipeline, minNgramLength, maxNgramLength, lemmatize=True):\n",
    "    lowered = \" brk \".join(map(str, [step for step in step_list if step is not None])).lower()\n",
    "\n",
    "    preproc = stanza_pipeline(lowered)\n",
    "    \n",
    "    if lemmatize:\n",
    "        lemmad = \" \".join(map(str,\n",
    "                            [word.lemma\n",
    "                            for sent in preproc.sentences \n",
    "                            for word in sent.words if (\n",
    "                                word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\", \"PUNCT\"]\n",
    "                                and word is not None\n",
    "                            )]\n",
    "                        )\n",
    "                    )\n",
    "    else:\n",
    "        lemmad = \" \".join(map(str,\n",
    "                            [word.text\n",
    "                            for sent in preproc.sentences \n",
    "                            for word in sent.words if (\n",
    "                                word is not None\n",
    "                            )]\n",
    "                        )\n",
    "                    )\n",
    "    # analyze each line of the input string seperately\n",
    "    for ln in lemmad.split(' brk '):\n",
    "        # tokenize the input string (customize the regex as desired)\n",
    "        at_least_two_english_characters_whole_words = \"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "        terms = re.split(at_least_two_english_characters_whole_words, ln)\n",
    "\n",
    "        # loop ngram creation for every number between min and max ngram length\n",
    "        for ngramLength in range(minNgramLength, maxNgramLength+1):\n",
    "\n",
    "            # find and return all ngrams\n",
    "            # for ngram in zip(*[terms[i:] for i in range(3)]): \n",
    "                # <-- solution without a generator (works the same but has higher memory usage)\n",
    "            for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]):   # <-- solution using a generator\n",
    "                \n",
    "                ngram = ' '.join(map(str, ngram))\n",
    "                # yield ngram\n",
    "                return str(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | Below this are blocks to use DagsHub with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@markdown Enter the username of your DAGsHub account:\n",
    "DAGSHUB_USER_NAME = \"AaronWChen\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the email for your DAGsHub account:\n",
    "DAGSHUB_EMAIL = \"awc33@cornell.edu\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the repo name \n",
    "DAGSHUB_REPO_NAME = \"MeaLeon\"\n",
    "\n",
    "#@markdown Enter the name of the branch you are working on \n",
    "BRANCH = \"MLF-1/start-custom-sklearn-mlflow-model\"\n",
    "dagshub.init(repo_name=DAGSHUB_REPO_NAME\n",
    "             , repo_owner=DAGSHUB_USER_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting DEV stage for One Hot Encoded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# starter idea for making an experiment name can be the git branch, but need more specificity\n",
    "experiment_name = f\"{DAGSHUB_EMAIL}/one-hot-encode\"\n",
    "mlflow_exp_id = get_experiment_id(experiment_name)\n",
    "\n",
    "# define model location\n",
    "# model_directory = \"/tmp/sklearn_model\"\n",
    "model_directory = \"../models/sklearn_model\"\n",
    "\n",
    "# Define the required artifacts associated with the saved custom pyfunc\n",
    "# sklearn_path = model_directory + \"\"\n",
    "sklearn_model_path = model_directory + \"/python_model.pkl\"\n",
    "sklearn_transformer_path = model_directory + \"/sklearn_transformer.pkl\"\n",
    "transformed_recipes_path = model_directory + \"/transformed_recipes.pkl\"\n",
    "\n",
    "artifacts = {'sklearn_model': sklearn_model_path,\n",
    "             'sklearn_transformer': sklearn_transformer_path,\n",
    "             'transformed_recipes': transformed_recipes_path\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca32aaf416b346ef93f5b6252dcaeec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 23:03:56 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-02-11 23:03:58 INFO: File exists: /home/awchen/stanza_resources/en/default.zip\n",
      "2024-02-11 23:04:02 INFO: Finished downloading models and saved to /home/awchen/stanza_resources.\n",
      "2024-02-11 23:04:02 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613a540806e84c309fa151b1dcc2bc84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 23:04:03 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2024-02-11 23:04:03 INFO: Using device: cpu\n",
      "2024-02-11 23:04:03 INFO: Loading: tokenize\n",
      "2024-02-11 23:04:03 INFO: Loading: pos\n",
      "2024-02-11 23:04:03 INFO: Loading: lemma\n",
      "2024-02-11 23:04:03 INFO: Loading: constituency\n",
      "2024-02-11 23:04:04 INFO: Loading: depparse\n",
      "2024-02-11 23:04:04 INFO: Loading: sentiment\n",
      "2024-02-11 23:04:04 INFO: Loading: ner\n",
      "2024-02-11 23:04:04 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------\n",
      "Raw Dataframe:\n",
      "                         id  \\\n",
      "0  54a2b6b019925f464b373351   \n",
      "1  54a408a019925f464b3733bc   \n",
      "2  54a408a26529d92b2c003631   \n",
      "3  54a408a66529d92b2c003638   \n",
      "4  54a408a719925f464b3733cc   \n",
      "\n",
      "                                                 dek  \\\n",
      "0  How does fried chicken achieve No. 1 status? B...   \n",
      "1                                Spinaci all'Ebraica   \n",
      "2  This majestic, moist, and richly spiced honey ...   \n",
      "3  The idea for this sandwich came to me when my ...   \n",
      "4  In 1930, Simon Agranat, the chief justice of t...   \n",
      "\n",
      "                                     hed                   pubDate  \\\n",
      "0            Pickle-Brined Fried Chicken  2014-08-19T04:00:00.000Z   \n",
      "1                   Spinach Jewish Style  2008-09-09T04:00:00.000Z   \n",
      "2                  New Year’s Honey Cake  2008-09-10T04:00:00.000Z   \n",
      "3  The B.L.A.Bagel with Lox and Avocado  2008-09-08T04:00:00.000Z   \n",
      "4        Shakshuka a la Doktor Shakshuka  2008-09-09T04:00:00.000Z   \n",
      "\n",
      "                             author    type  \\\n",
      "0                                []  recipe   \n",
      "1  [{'name': 'Edda Servi Machlin'}]  recipe   \n",
      "2       [{'name': 'Marcy Goldman'}]  recipe   \n",
      "3           [{'name': 'Faye Levy'}]  recipe   \n",
      "4         [{'name': 'Joan Nathan'}]  recipe   \n",
      "\n",
      "                                                 url  \\\n",
      "0  /recipes/food/views/pickle-brined-fried-chicke...   \n",
      "1    /recipes/food/views/spinach-jewish-style-350152   \n",
      "2  /recipes/food/views/majestic-and-moist-new-yea...   \n",
      "3  /recipes/food/views/the-b-l-a-bagel-with-lox-a...   \n",
      "4  /recipes/food/views/shakshuka-a-la-doktor-shak...   \n",
      "\n",
      "                                           photoData  \\\n",
      "0  {'id': '54a2b64a6529d92b2c003409', 'filename':...   \n",
      "1  {'id': '56746182accb4c9831e45e0a', 'filename':...   \n",
      "2  {'id': '55e85ba4cf90d6663f728014', 'filename':...   \n",
      "3  {'id': '5674617e47d1a28026045e4f', 'filename':...   \n",
      "4  {'id': '56746183b47c050a284a4e15', 'filename':...   \n",
      "\n",
      "                                                 tag  aggregateRating  \\\n",
      "0  {'category': 'ingredient', 'name': 'Chicken', ...             3.11   \n",
      "1  {'category': 'cuisine', 'name': 'Italian', 'ur...             3.22   \n",
      "2  {'category': 'cuisine', 'name': 'Jewish', 'url...             3.62   \n",
      "3  {'category': 'cuisine', 'name': 'Jewish', 'url...             4.00   \n",
      "4  {'category': 'cuisine', 'name': 'Jewish', 'url...             2.71   \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  [1 tablespoons yellow mustard seeds, 1 tablesp...   \n",
      "1  [3 pounds small-leaved bulk spinach, Salt, 1/2...   \n",
      "2  [3 1/2 cups all-purpose flour, 1 tablespoon ba...   \n",
      "3  [1 small ripe avocado, preferably Hass (see No...   \n",
      "4  [2 pounds fresh tomatoes, unpeeled and cut in ...   \n",
      "\n",
      "                                           prepSteps  reviewsCount  \\\n",
      "0  [Toast mustard and coriander seeds in a dry me...             7   \n",
      "1  [Remove the stems and roots from the spinach. ...             5   \n",
      "2  [I like this cake best baked in a 9-inch angel...           105   \n",
      "3  [A short time before serving, mash avocado and...             7   \n",
      "4  [1. Place the tomatoes, garlic, salt, paprika,...             7   \n",
      "\n",
      "   willMakeAgainPct  dateCrawled  \n",
      "0               100   1498547035  \n",
      "1                80   1498547740  \n",
      "2                88   1498547738  \n",
      "3               100   1498547740  \n",
      "4                83   1498547740  \n",
      "(34756, 15)\n"
     ]
    }
   ],
   "source": [
    "# instantiate stanza pipeline\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', \n",
    "                    depparse_batch_size=50, \n",
    "                    depparse_min_length_to_batch_separately=50,\n",
    "                    verbose=True,\n",
    "                    use_gpu=False, # set to true when on cloud/not on streaming computer\n",
    "                    batch_size=100\n",
    "                    )\n",
    "\n",
    "# load raw data and preprocess/clean\n",
    "data = dvc.api.read(\n",
    "    path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "    , mode='r')\n",
    "raw_df = pd.read_json(data)\n",
    "print('\\n')\n",
    "print('--------------')\n",
    "print('Raw Dataframe:', end='\\n')\n",
    "print(raw_df.head())\n",
    "print(raw_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------\n",
      "Preprocessed Dataframe:\n",
      "                                                                        dek  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc                                                      \n",
      "54a42cde19925f464b3809d2  Green chiles pickled in soy sauce and vinegar ...   \n",
      "54a433036529d92b2c015de3  This soup features the flavors of India: aroma...   \n",
      "54a451926529d92b2c01eda8                                                      \n",
      "54a430876529d92b2c013e2b  Brown sugar and molasses are balanced by fresh...   \n",
      "\n",
      "                                                                        hed  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  Grilled Hearts of Romaine with Blue Cheese Vin...   \n",
      "54a42cde19925f464b3809d2                              Soy-Pickled Jalapeños   \n",
      "54a433036529d92b2c015de3  Curried Potato and Spinach Soup with Onion Sal...   \n",
      "54a451926529d92b2c01eda8                                       Chicken Soup   \n",
      "54a430876529d92b2c013e2b                           Sweet-Hot Barbecue Sauce   \n",
      "\n",
      "                          aggregateRating  \\\n",
      "id                                          \n",
      "54a4270b19925f464b37c1dc             3.64   \n",
      "54a42cde19925f464b3809d2             3.43   \n",
      "54a433036529d92b2c015de3             3.00   \n",
      "54a451926529d92b2c01eda8             3.19   \n",
      "54a430876529d92b2c013e2b             0.00   \n",
      "\n",
      "                                                                ingredients  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  [1 1/2 cups white wine vinegar, 1/2 cup sugar,...   \n",
      "54a42cde19925f464b3809d2  [3 large fresh jalapeños (4 inches), sliced 1/...   \n",
      "54a433036529d92b2c015de3  [4 cups chopped red onions (about 2 large), 1 ...   \n",
      "54a451926529d92b2c01eda8  [1 pound chicken parts, 2 stalks celery, inclu...   \n",
      "54a430876529d92b2c013e2b  [2 tablespoons olive oil, 1 cup chopped onion,...   \n",
      "\n",
      "                                                                  prepSteps  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  [Combine first 5 ingredients and 1/4 teaspoon ...   \n",
      "54a42cde19925f464b3809d2  [Combine all ingredients in a small heavy sauc...   \n",
      "54a433036529d92b2c015de3  [Combine first 5 ingredients in heavy medium s...   \n",
      "54a451926529d92b2c01eda8  [1. Pour 12 cups of cold water into a large st...   \n",
      "54a430876529d92b2c013e2b  [Heat oil in large saucepan over medium-high h...   \n",
      "\n",
      "                          reviewsCount  willMakeAgainPct     cuisine_name  \\\n",
      "id                                                                          \n",
      "54a4270b19925f464b37c1dc             9               100  Missing Cuisine   \n",
      "54a42cde19925f464b3809d2             6               100  Missing Cuisine   \n",
      "54a433036529d92b2c015de3             6                67           Indian   \n",
      "54a451926529d92b2c01eda8            32                87           Kosher   \n",
      "54a430876529d92b2c013e2b             0                 0  Missing Cuisine   \n",
      "\n",
      "                                               photo_filename  \\\n",
      "id                                                              \n",
      "54a4270b19925f464b37c1dc  EP_12162015_placeholders_casual.jpg   \n",
      "54a42cde19925f464b3809d2  EP_12162015_placeholders_rustic.jpg   \n",
      "54a433036529d92b2c015de3                           234125.jpg   \n",
      "54a451926529d92b2c01eda8  EP_12162015_placeholders_formal.jpg   \n",
      "54a430876529d92b2c013e2b  EP_12162015_placeholders_rustic.jpg   \n",
      "\n",
      "                                                               photo_credit  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a42cde19925f464b3809d2  Photo by Chelsea Kyle, Prop Styling by Anna St...   \n",
      "54a433036529d92b2c015de3                                      Brian Leatart   \n",
      "54a451926529d92b2c01eda8  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a430876529d92b2c013e2b  Photo by Chelsea Kyle, Prop Styling by Anna St...   \n",
      "\n",
      "                              author_name            date_published  \\\n",
      "id                                                                    \n",
      "54a4270b19925f464b37c1dc    Kate Higgins  2010-12-16 04:00:00+00:00   \n",
      "54a42cde19925f464b3809d2     Lillian Chou 2009-02-19 04:00:00+00:00   \n",
      "54a433036529d92b2c015de3     Peter Gordon 2006-03-07 04:00:00+00:00   \n",
      "54a451926529d92b2c01eda8  Sharon Lebewohl 2004-08-20 04:00:00+00:00   \n",
      "54a430876529d92b2c013e2b   Suzanne Tracht 2007-12-03 20:11:11+00:00   \n",
      "\n",
      "                                                                 recipe_url  \n",
      "id                                                                           \n",
      "54a4270b19925f464b37c1dc  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a42cde19925f464b3809d2  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a433036529d92b2c015de3  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a451926529d92b2c01eda8  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a430876529d92b2c013e2b  https://www.epicurious.com/recipes/food/views/...  \n",
      "(50, 13)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Subset Dataframe:\n",
      "                                                                        dek  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc                                                      \n",
      "54a42cde19925f464b3809d2  Green chiles pickled in soy sauce and vinegar ...   \n",
      "54a433036529d92b2c015de3  This soup features the flavors of India: aroma...   \n",
      "54a451926529d92b2c01eda8                                                      \n",
      "54a430876529d92b2c013e2b  Brown sugar and molasses are balanced by fresh...   \n",
      "\n",
      "                                                                        hed  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  Grilled Hearts of Romaine with Blue Cheese Vin...   \n",
      "54a42cde19925f464b3809d2                              Soy-Pickled Jalapeños   \n",
      "54a433036529d92b2c015de3  Curried Potato and Spinach Soup with Onion Sal...   \n",
      "54a451926529d92b2c01eda8                                       Chicken Soup   \n",
      "54a430876529d92b2c013e2b                           Sweet-Hot Barbecue Sauce   \n",
      "\n",
      "                          aggregateRating  \\\n",
      "id                                          \n",
      "54a4270b19925f464b37c1dc             3.64   \n",
      "54a42cde19925f464b3809d2             3.43   \n",
      "54a433036529d92b2c015de3             3.00   \n",
      "54a451926529d92b2c01eda8             3.19   \n",
      "54a430876529d92b2c013e2b             0.00   \n",
      "\n",
      "                                                                ingredients  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  [1 1/2 cups white wine vinegar, 1/2 cup sugar,...   \n",
      "54a42cde19925f464b3809d2  [3 large fresh jalapeños (4 inches), sliced 1/...   \n",
      "54a433036529d92b2c015de3  [4 cups chopped red onions (about 2 large), 1 ...   \n",
      "54a451926529d92b2c01eda8  [1 pound chicken parts, 2 stalks celery, inclu...   \n",
      "54a430876529d92b2c013e2b  [2 tablespoons olive oil, 1 cup chopped onion,...   \n",
      "\n",
      "                                                                  prepSteps  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  [Combine first 5 ingredients and 1/4 teaspoon ...   \n",
      "54a42cde19925f464b3809d2  [Combine all ingredients in a small heavy sauc...   \n",
      "54a433036529d92b2c015de3  [Combine first 5 ingredients in heavy medium s...   \n",
      "54a451926529d92b2c01eda8  [1. Pour 12 cups of cold water into a large st...   \n",
      "54a430876529d92b2c013e2b  [Heat oil in large saucepan over medium-high h...   \n",
      "\n",
      "                          reviewsCount  willMakeAgainPct     cuisine_name  \\\n",
      "id                                                                          \n",
      "54a4270b19925f464b37c1dc             9               100  Missing Cuisine   \n",
      "54a42cde19925f464b3809d2             6               100  Missing Cuisine   \n",
      "54a433036529d92b2c015de3             6                67           Indian   \n",
      "54a451926529d92b2c01eda8            32                87           Kosher   \n",
      "54a430876529d92b2c013e2b             0                 0  Missing Cuisine   \n",
      "\n",
      "                                               photo_filename  \\\n",
      "id                                                              \n",
      "54a4270b19925f464b37c1dc  EP_12162015_placeholders_casual.jpg   \n",
      "54a42cde19925f464b3809d2  EP_12162015_placeholders_rustic.jpg   \n",
      "54a433036529d92b2c015de3                           234125.jpg   \n",
      "54a451926529d92b2c01eda8  EP_12162015_placeholders_formal.jpg   \n",
      "54a430876529d92b2c013e2b  EP_12162015_placeholders_rustic.jpg   \n",
      "\n",
      "                                                               photo_credit  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a42cde19925f464b3809d2  Photo by Chelsea Kyle, Prop Styling by Anna St...   \n",
      "54a433036529d92b2c015de3                                      Brian Leatart   \n",
      "54a451926529d92b2c01eda8  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a430876529d92b2c013e2b  Photo by Chelsea Kyle, Prop Styling by Anna St...   \n",
      "\n",
      "                              author_name            date_published  \\\n",
      "id                                                                    \n",
      "54a4270b19925f464b37c1dc    Kate Higgins  2010-12-16 04:00:00+00:00   \n",
      "54a42cde19925f464b3809d2     Lillian Chou 2009-02-19 04:00:00+00:00   \n",
      "54a433036529d92b2c015de3     Peter Gordon 2006-03-07 04:00:00+00:00   \n",
      "54a451926529d92b2c01eda8  Sharon Lebewohl 2004-08-20 04:00:00+00:00   \n",
      "54a430876529d92b2c013e2b   Suzanne Tracht 2007-12-03 20:11:11+00:00   \n",
      "\n",
      "                                                                 recipe_url  \n",
      "id                                                                           \n",
      "54a4270b19925f464b37c1dc  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a42cde19925f464b3809d2  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a433036529d92b2c015de3  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a451926529d92b2c01eda8  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a430876529d92b2c013e2b  https://www.epicurious.com/recipes/food/views/...  \n",
      "(50, 13)\n"
     ]
    }
   ],
   "source": [
    "# take sample and train/test split \n",
    "subset_df = raw_df.sample(n=100, random_state=45)\n",
    "train_df, test_df = train_test_split(subset_df,test_size=0.5, random_state=45)\n",
    "\n",
    "# pre_proc_df is cleaned dataframe\n",
    "pre_proc_df = dfpp.preprocess_dataframe(train_df)\n",
    "print('\\n')\n",
    "print('--------------')\n",
    "print('Preprocessed Dataframe:', end='\\n')\n",
    "print(pre_proc_df.head())\n",
    "print(pre_proc_df.shape)\n",
    "\n",
    "# create subset for dev purposes\n",
    "to_nlp_df = pre_proc_df\n",
    "print('\\n')\n",
    "print('-' * 80)\n",
    "print('Subset Dataframe:', end='\\n')\n",
    "print(to_nlp_df.head())\n",
    "print(to_nlp_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "sklearn fit transform on ingredients:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:36<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Transformed Data:\n",
      "                          cup chop onion  cup olive oil  cup sour cream  \\\n",
      "id                                                                        \n",
      "54a4270b19925f464b37c1dc               0              1               0   \n",
      "54a42cde19925f464b3809d2               0              0               0   \n",
      "54a433036529d92b2c015de3               0              0               0   \n",
      "54a451926529d92b2c01eda8               0              0               0   \n",
      "54a430876529d92b2c013e2b               1              0               0   \n",
      "54a453df6529d92b2c020687               0              0               1   \n",
      "55b0e7116284773353bf4580               0              0               0   \n",
      "54a42bab6529d92b2c00ffa7               0              0               0   \n",
      "54a4748f19925f464b399ef2               0              1               0   \n",
      "54a4356a19925f464b3875bb               0              1               0   \n",
      "54a4697e6529d92b2c0279d3               0              0               0   \n",
      "54a45e426529d92b2c02488f               0              0               0   \n",
      "54a452c96529d92b2c01f889               0              0               0   \n",
      "54a4323619925f464b384bcc               0              0               0   \n",
      "54a4259119925f464b37af9c               0              0               0   \n",
      "54a431da6529d92b2c014ee9               0              1               0   \n",
      "54a426fd19925f464b37c125               0              0               0   \n",
      "54a47bb019925f464b39b9b7               0              0               0   \n",
      "54a434d819925f464b386e62               0              1               0   \n",
      "54a428116529d92b2c00d1a7               0              1               0   \n",
      "54a436036529d92b2c01859e               0              0               0   \n",
      "54a47edf19925f464b39c58d               0              0               0   \n",
      "54a419706529d92b2c006650               0              0               0   \n",
      "54a4349619925f464b386b12               0              0               0   \n",
      "54a4340f6529d92b2c016be8               0              0               0   \n",
      "54a40e546529d92b2c004606               0              0               0   \n",
      "54a428b419925f464b37d5ce               0              0               0   \n",
      "54a453a519925f464b38fd16               0              0               0   \n",
      "54a41cb219925f464b376d82               0              0               0   \n",
      "54a431896529d92b2c014b27               0              0               0   \n",
      "54a423ab19925f464b3799f2               0              0               0   \n",
      "54a47c1419925f464b39bb28               0              0               0   \n",
      "593ee3ba12c27b182380821f               0              0               0   \n",
      "54a456366529d92b2c02235a               1              0               0   \n",
      "54a452d419925f464b38f1b5               0              0               1   \n",
      "54a4659b6529d92b2c026a53               0              0               0   \n",
      "54a46d5d19925f464b3982d3               0              0               0   \n",
      "54a4582119925f464b3927a1               0              0               1   \n",
      "54a4205319925f464b377c9f               0              0               0   \n",
      "54a470cc19925f464b39906b               1              0               0   \n",
      "54a44f4a6529d92b2c01de45               0              0               0   \n",
      "592ef494ae10ad089795ebfa               0              0               0   \n",
      "54a41f016529d92b2c00757d               0              0               0   \n",
      "54a436266529d92b2c01876e               0              0               1   \n",
      "54a428bd19925f464b37d63e               0              0               0   \n",
      "54a45a4f6529d92b2c0234d1               0              0               0   \n",
      "54a41ed76529d92b2c007440               0              0               0   \n",
      "54a42c906529d92b2c010b74               0              0               0   \n",
      "569519a6dc18ea6c22c9b9ab               0              0               0   \n",
      "54a438d56529d92b2c019648               0              1               0   \n",
      "\n",
      "                          cup sugar  cup water  large egg  \\\n",
      "id                                                          \n",
      "54a4270b19925f464b37c1dc          1          1          0   \n",
      "54a42cde19925f464b3809d2          0          1          0   \n",
      "54a433036529d92b2c015de3          0          0          0   \n",
      "54a451926529d92b2c01eda8          0          0          0   \n",
      "54a430876529d92b2c013e2b          0          0          0   \n",
      "54a453df6529d92b2c020687          0          0          0   \n",
      "55b0e7116284773353bf4580          0          0          0   \n",
      "54a42bab6529d92b2c00ffa7          0          0          0   \n",
      "54a4748f19925f464b399ef2          0          0          0   \n",
      "54a4356a19925f464b3875bb          0          0          0   \n",
      "54a4697e6529d92b2c0279d3          0          0          0   \n",
      "54a45e426529d92b2c02488f          0          0          0   \n",
      "54a452c96529d92b2c01f889          0          0          0   \n",
      "54a4323619925f464b384bcc          0          0          0   \n",
      "54a4259119925f464b37af9c          0          0          0   \n",
      "54a431da6529d92b2c014ee9          0          0          0   \n",
      "54a426fd19925f464b37c125          0          0          0   \n",
      "54a47bb019925f464b39b9b7          0          0          0   \n",
      "54a434d819925f464b386e62          0          0          0   \n",
      "54a428116529d92b2c00d1a7          0          0          0   \n",
      "54a436036529d92b2c01859e          1          0          0   \n",
      "54a47edf19925f464b39c58d          1          0          0   \n",
      "54a419706529d92b2c006650          0          0          0   \n",
      "54a4349619925f464b386b12          0          0          1   \n",
      "54a4340f6529d92b2c016be8          0          0          0   \n",
      "54a40e546529d92b2c004606          0          0          0   \n",
      "54a428b419925f464b37d5ce          1          0          0   \n",
      "54a453a519925f464b38fd16          0          1          0   \n",
      "54a41cb219925f464b376d82          0          0          0   \n",
      "54a431896529d92b2c014b27          0          1          0   \n",
      "54a423ab19925f464b3799f2          0          0          0   \n",
      "54a47c1419925f464b39bb28          0          0          0   \n",
      "593ee3ba12c27b182380821f          0          0          0   \n",
      "54a456366529d92b2c02235a          0          0          0   \n",
      "54a452d419925f464b38f1b5          0          0          0   \n",
      "54a4659b6529d92b2c026a53          0          0          0   \n",
      "54a46d5d19925f464b3982d3          1          0          0   \n",
      "54a4582119925f464b3927a1          0          0          0   \n",
      "54a4205319925f464b377c9f          0          0          0   \n",
      "54a470cc19925f464b39906b          0          0          0   \n",
      "54a44f4a6529d92b2c01de45          0          0          0   \n",
      "592ef494ae10ad089795ebfa          0          0          0   \n",
      "54a41f016529d92b2c00757d          0          0          1   \n",
      "54a436266529d92b2c01876e          1          1          0   \n",
      "54a428bd19925f464b37d63e          0          0          0   \n",
      "54a45a4f6529d92b2c0234d1          0          0          0   \n",
      "54a41ed76529d92b2c007440          1          1          0   \n",
      "54a42c906529d92b2c010b74          0          0          1   \n",
      "569519a6dc18ea6c22c9b9ab          0          0          0   \n",
      "54a438d56529d92b2c019648          0          0          0   \n",
      "\n",
      "                          tablespoon extra-virgin olive oil  \\\n",
      "id                                                            \n",
      "54a4270b19925f464b37c1dc                                  0   \n",
      "54a42cde19925f464b3809d2                                  0   \n",
      "54a433036529d92b2c015de3                                  0   \n",
      "54a451926529d92b2c01eda8                                  0   \n",
      "54a430876529d92b2c013e2b                                  0   \n",
      "54a453df6529d92b2c020687                                  0   \n",
      "55b0e7116284773353bf4580                                  0   \n",
      "54a42bab6529d92b2c00ffa7                                  0   \n",
      "54a4748f19925f464b399ef2                                  0   \n",
      "54a4356a19925f464b3875bb                                  0   \n",
      "54a4697e6529d92b2c0279d3                                  0   \n",
      "54a45e426529d92b2c02488f                                  0   \n",
      "54a452c96529d92b2c01f889                                  0   \n",
      "54a4323619925f464b384bcc                                  0   \n",
      "54a4259119925f464b37af9c                                  1   \n",
      "54a431da6529d92b2c014ee9                                  0   \n",
      "54a426fd19925f464b37c125                                  0   \n",
      "54a47bb019925f464b39b9b7                                  0   \n",
      "54a434d819925f464b386e62                                  0   \n",
      "54a428116529d92b2c00d1a7                                  0   \n",
      "54a436036529d92b2c01859e                                  0   \n",
      "54a47edf19925f464b39c58d                                  0   \n",
      "54a419706529d92b2c006650                                  0   \n",
      "54a4349619925f464b386b12                                  0   \n",
      "54a4340f6529d92b2c016be8                                  0   \n",
      "54a40e546529d92b2c004606                                  1   \n",
      "54a428b419925f464b37d5ce                                  0   \n",
      "54a453a519925f464b38fd16                                  0   \n",
      "54a41cb219925f464b376d82                                  0   \n",
      "54a431896529d92b2c014b27                                  0   \n",
      "54a423ab19925f464b3799f2                                  0   \n",
      "54a47c1419925f464b39bb28                                  1   \n",
      "593ee3ba12c27b182380821f                                  0   \n",
      "54a456366529d92b2c02235a                                  0   \n",
      "54a452d419925f464b38f1b5                                  0   \n",
      "54a4659b6529d92b2c026a53                                  0   \n",
      "54a46d5d19925f464b3982d3                                  0   \n",
      "54a4582119925f464b3927a1                                  0   \n",
      "54a4205319925f464b377c9f                                  0   \n",
      "54a470cc19925f464b39906b                                  0   \n",
      "54a44f4a6529d92b2c01de45                                  0   \n",
      "592ef494ae10ad089795ebfa                                  0   \n",
      "54a41f016529d92b2c00757d                                  0   \n",
      "54a436266529d92b2c01876e                                  0   \n",
      "54a428bd19925f464b37d63e                                  0   \n",
      "54a45a4f6529d92b2c0234d1                                  0   \n",
      "54a41ed76529d92b2c007440                                  0   \n",
      "54a42c906529d92b2c010b74                                  0   \n",
      "569519a6dc18ea6c22c9b9ab                                  0   \n",
      "54a438d56529d92b2c019648                                  0   \n",
      "\n",
      "                          tablespoon fresh lemon juice  tablespoon olive oil  \\\n",
      "id                                                                             \n",
      "54a4270b19925f464b37c1dc                             0                     0   \n",
      "54a42cde19925f464b3809d2                             0                     0   \n",
      "54a433036529d92b2c015de3                             1                     1   \n",
      "54a451926529d92b2c01eda8                             0                     0   \n",
      "54a430876529d92b2c013e2b                             0                     1   \n",
      "54a453df6529d92b2c020687                             0                     0   \n",
      "55b0e7116284773353bf4580                             0                     0   \n",
      "54a42bab6529d92b2c00ffa7                             0                     0   \n",
      "54a4748f19925f464b399ef2                             0                     0   \n",
      "54a4356a19925f464b3875bb                             0                     0   \n",
      "54a4697e6529d92b2c0279d3                             0                     0   \n",
      "54a45e426529d92b2c02488f                             0                     0   \n",
      "54a452c96529d92b2c01f889                             0                     0   \n",
      "54a4323619925f464b384bcc                             0                     0   \n",
      "54a4259119925f464b37af9c                             1                     0   \n",
      "54a431da6529d92b2c014ee9                             0                     0   \n",
      "54a426fd19925f464b37c125                             0                     0   \n",
      "54a47bb019925f464b39b9b7                             0                     0   \n",
      "54a434d819925f464b386e62                             0                     0   \n",
      "54a428116529d92b2c00d1a7                             0                     0   \n",
      "54a436036529d92b2c01859e                             0                     0   \n",
      "54a47edf19925f464b39c58d                             0                     0   \n",
      "54a419706529d92b2c006650                             0                     0   \n",
      "54a4349619925f464b386b12                             0                     0   \n",
      "54a4340f6529d92b2c016be8                             0                     0   \n",
      "54a40e546529d92b2c004606                             0                     0   \n",
      "54a428b419925f464b37d5ce                             0                     0   \n",
      "54a453a519925f464b38fd16                             0                     0   \n",
      "54a41cb219925f464b376d82                             0                     0   \n",
      "54a431896529d92b2c014b27                             0                     0   \n",
      "54a423ab19925f464b3799f2                             0                     0   \n",
      "54a47c1419925f464b39bb28                             1                     0   \n",
      "593ee3ba12c27b182380821f                             1                     0   \n",
      "54a456366529d92b2c02235a                             0                     1   \n",
      "54a452d419925f464b38f1b5                             0                     0   \n",
      "54a4659b6529d92b2c026a53                             1                     1   \n",
      "54a46d5d19925f464b3982d3                             0                     0   \n",
      "54a4582119925f464b3927a1                             0                     0   \n",
      "54a4205319925f464b377c9f                             0                     0   \n",
      "54a470cc19925f464b39906b                             0                     1   \n",
      "54a44f4a6529d92b2c01de45                             0                     0   \n",
      "592ef494ae10ad089795ebfa                             1                     0   \n",
      "54a41f016529d92b2c00757d                             0                     0   \n",
      "54a436266529d92b2c01876e                             0                     1   \n",
      "54a428bd19925f464b37d63e                             0                     0   \n",
      "54a45a4f6529d92b2c0234d1                             0                     0   \n",
      "54a41ed76529d92b2c007440                             0                     0   \n",
      "54a42c906529d92b2c010b74                             0                     0   \n",
      "569519a6dc18ea6c22c9b9ab                             0                     0   \n",
      "54a438d56529d92b2c019648                             0                     0   \n",
      "\n",
      "                          tablespoon sugar  tablespoon white wine vinegar  \\\n",
      "id                                                                          \n",
      "54a4270b19925f464b37c1dc                 0                              0   \n",
      "54a42cde19925f464b3809d2                 0                              0   \n",
      "54a433036529d92b2c015de3                 0                              0   \n",
      "54a451926529d92b2c01eda8                 0                              0   \n",
      "54a430876529d92b2c013e2b                 0                              0   \n",
      "54a453df6529d92b2c020687                 0                              0   \n",
      "55b0e7116284773353bf4580                 0                              0   \n",
      "54a42bab6529d92b2c00ffa7                 0                              0   \n",
      "54a4748f19925f464b399ef2                 0                              0   \n",
      "54a4356a19925f464b3875bb                 0                              1   \n",
      "54a4697e6529d92b2c0279d3                 0                              0   \n",
      "54a45e426529d92b2c02488f                 0                              0   \n",
      "54a452c96529d92b2c01f889                 0                              0   \n",
      "54a4323619925f464b384bcc                 1                              0   \n",
      "54a4259119925f464b37af9c                 0                              0   \n",
      "54a431da6529d92b2c014ee9                 0                              0   \n",
      "54a426fd19925f464b37c125                 0                              0   \n",
      "54a47bb019925f464b39b9b7                 0                              0   \n",
      "54a434d819925f464b386e62                 0                              0   \n",
      "54a428116529d92b2c00d1a7                 0                              0   \n",
      "54a436036529d92b2c01859e                 0                              0   \n",
      "54a47edf19925f464b39c58d                 0                              0   \n",
      "54a419706529d92b2c006650                 0                              0   \n",
      "54a4349619925f464b386b12                 1                              0   \n",
      "54a4340f6529d92b2c016be8                 0                              0   \n",
      "54a40e546529d92b2c004606                 0                              0   \n",
      "54a428b419925f464b37d5ce                 0                              0   \n",
      "54a453a519925f464b38fd16                 0                              0   \n",
      "54a41cb219925f464b376d82                 0                              0   \n",
      "54a431896529d92b2c014b27                 0                              0   \n",
      "54a423ab19925f464b3799f2                 0                              0   \n",
      "54a47c1419925f464b39bb28                 0                              0   \n",
      "593ee3ba12c27b182380821f                 0                              1   \n",
      "54a456366529d92b2c02235a                 0                              0   \n",
      "54a452d419925f464b38f1b5                 0                              0   \n",
      "54a4659b6529d92b2c026a53                 0                              0   \n",
      "54a46d5d19925f464b3982d3                 0                              0   \n",
      "54a4582119925f464b3927a1                 0                              0   \n",
      "54a4205319925f464b377c9f                 0                              0   \n",
      "54a470cc19925f464b39906b                 0                              0   \n",
      "54a44f4a6529d92b2c01de45                 0                              0   \n",
      "592ef494ae10ad089795ebfa                 0                              1   \n",
      "54a41f016529d92b2c00757d                 0                              0   \n",
      "54a436266529d92b2c01876e                 0                              0   \n",
      "54a428bd19925f464b37d63e                 0                              0   \n",
      "54a45a4f6529d92b2c0234d1                 1                              0   \n",
      "54a41ed76529d92b2c007440                 0                              0   \n",
      "54a42c906529d92b2c010b74                 0                              0   \n",
      "569519a6dc18ea6c22c9b9ab                 0                              0   \n",
      "54a438d56529d92b2c019648                 0                              0   \n",
      "\n",
      "                          teaspoon ground cumin  teaspoon salt  \\\n",
      "id                                                               \n",
      "54a4270b19925f464b37c1dc                      0              0   \n",
      "54a42cde19925f464b3809d2                      0              0   \n",
      "54a433036529d92b2c015de3                      0              0   \n",
      "54a451926529d92b2c01eda8                      0              1   \n",
      "54a430876529d92b2c013e2b                      0              0   \n",
      "54a453df6529d92b2c020687                      0              1   \n",
      "55b0e7116284773353bf4580                      0              0   \n",
      "54a42bab6529d92b2c00ffa7                      0              0   \n",
      "54a4748f19925f464b399ef2                      1              0   \n",
      "54a4356a19925f464b3875bb                      0              0   \n",
      "54a4697e6529d92b2c0279d3                      0              0   \n",
      "54a45e426529d92b2c02488f                      0              0   \n",
      "54a452c96529d92b2c01f889                      0              0   \n",
      "54a4323619925f464b384bcc                      0              0   \n",
      "54a4259119925f464b37af9c                      0              0   \n",
      "54a431da6529d92b2c014ee9                      0              0   \n",
      "54a426fd19925f464b37c125                      0              0   \n",
      "54a47bb019925f464b39b9b7                      0              1   \n",
      "54a434d819925f464b386e62                      0              0   \n",
      "54a428116529d92b2c00d1a7                      0              0   \n",
      "54a436036529d92b2c01859e                      0              0   \n",
      "54a47edf19925f464b39c58d                      0              0   \n",
      "54a419706529d92b2c006650                      0              0   \n",
      "54a4349619925f464b386b12                      0              0   \n",
      "54a4340f6529d92b2c016be8                      0              0   \n",
      "54a40e546529d92b2c004606                      0              0   \n",
      "54a428b419925f464b37d5ce                      0              0   \n",
      "54a453a519925f464b38fd16                      0              1   \n",
      "54a41cb219925f464b376d82                      0              0   \n",
      "54a431896529d92b2c014b27                      0              0   \n",
      "54a423ab19925f464b3799f2                      0              0   \n",
      "54a47c1419925f464b39bb28                      1              0   \n",
      "593ee3ba12c27b182380821f                      0              0   \n",
      "54a456366529d92b2c02235a                      1              1   \n",
      "54a452d419925f464b38f1b5                      0              0   \n",
      "54a4659b6529d92b2c026a53                      0              0   \n",
      "54a46d5d19925f464b3982d3                      0              0   \n",
      "54a4582119925f464b3927a1                      0              0   \n",
      "54a4205319925f464b377c9f                      0              0   \n",
      "54a470cc19925f464b39906b                      0              0   \n",
      "54a44f4a6529d92b2c01de45                      0              0   \n",
      "592ef494ae10ad089795ebfa                      0              0   \n",
      "54a41f016529d92b2c00757d                      0              0   \n",
      "54a436266529d92b2c01876e                      0              1   \n",
      "54a428bd19925f464b37d63e                      0              0   \n",
      "54a45a4f6529d92b2c0234d1                      0              0   \n",
      "54a41ed76529d92b2c007440                      0              0   \n",
      "54a42c906529d92b2c010b74                      0              0   \n",
      "569519a6dc18ea6c22c9b9ab                      0              0   \n",
      "54a438d56529d92b2c019648                      0              0   \n",
      "\n",
      "                          teaspoon vanilla extract  \n",
      "id                                                  \n",
      "54a4270b19925f464b37c1dc                         0  \n",
      "54a42cde19925f464b3809d2                         0  \n",
      "54a433036529d92b2c015de3                         0  \n",
      "54a451926529d92b2c01eda8                         0  \n",
      "54a430876529d92b2c013e2b                         0  \n",
      "54a453df6529d92b2c020687                         0  \n",
      "55b0e7116284773353bf4580                         0  \n",
      "54a42bab6529d92b2c00ffa7                         0  \n",
      "54a4748f19925f464b399ef2                         0  \n",
      "54a4356a19925f464b3875bb                         0  \n",
      "54a4697e6529d92b2c0279d3                         0  \n",
      "54a45e426529d92b2c02488f                         0  \n",
      "54a452c96529d92b2c01f889                         0  \n",
      "54a4323619925f464b384bcc                         0  \n",
      "54a4259119925f464b37af9c                         0  \n",
      "54a431da6529d92b2c014ee9                         0  \n",
      "54a426fd19925f464b37c125                         0  \n",
      "54a47bb019925f464b39b9b7                         0  \n",
      "54a434d819925f464b386e62                         0  \n",
      "54a428116529d92b2c00d1a7                         0  \n",
      "54a436036529d92b2c01859e                         0  \n",
      "54a47edf19925f464b39c58d                         0  \n",
      "54a419706529d92b2c006650                         0  \n",
      "54a4349619925f464b386b12                         1  \n",
      "54a4340f6529d92b2c016be8                         0  \n",
      "54a40e546529d92b2c004606                         0  \n",
      "54a428b419925f464b37d5ce                         1  \n",
      "54a453a519925f464b38fd16                         0  \n",
      "54a41cb219925f464b376d82                         0  \n",
      "54a431896529d92b2c014b27                         0  \n",
      "54a423ab19925f464b3799f2                         0  \n",
      "54a47c1419925f464b39bb28                         0  \n",
      "593ee3ba12c27b182380821f                         0  \n",
      "54a456366529d92b2c02235a                         0  \n",
      "54a452d419925f464b38f1b5                         0  \n",
      "54a4659b6529d92b2c026a53                         0  \n",
      "54a46d5d19925f464b3982d3                         0  \n",
      "54a4582119925f464b3927a1                         0  \n",
      "54a4205319925f464b377c9f                         0  \n",
      "54a470cc19925f464b39906b                         0  \n",
      "54a44f4a6529d92b2c01de45                         0  \n",
      "592ef494ae10ad089795ebfa                         0  \n",
      "54a41f016529d92b2c00757d                         0  \n",
      "54a436266529d92b2c01876e                         0  \n",
      "54a428bd19925f464b37d63e                         0  \n",
      "54a45a4f6529d92b2c0234d1                         0  \n",
      "54a41ed76529d92b2c007440                         0  \n",
      "54a42c906529d92b2c010b74                         1  \n",
      "569519a6dc18ea6c22c9b9ab                         0  \n",
      "54a438d56529d92b2c019648                         0  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4ca9bec0974d198b1dcc65848760ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a65e598cff4ee5a853685a29fc3521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b5d89a149f48b8bb17f2114973abf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/11 23:05:56 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp9gtrkzjy/model, flavor: python_function), fall back to return ['cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.\n"
     ]
    }
   ],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_transformer_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': CustomSKLearnAnalyzer().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':3,\n",
    "    'binary':True\n",
    "}\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "# bertopic_params = {\n",
    "#     'top_n_words':20,\n",
    "#     'min_topic_size':5,\n",
    "#     'nr_topics':'auto',\n",
    "#     'verbose':True,\n",
    "#     'low_memory':True,\n",
    "#     'calculate_probabilities':True\n",
    "# }\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'OneHotEncoder'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_transformer_params)\n",
    "# pipeline_params.update(bertopic_params)\n",
    "\n",
    "signature = infer_signature(model_input=to_nlp_df['ingredients'],\n",
    "                            )\n",
    "\n",
    "with mlflow.start_run(experiment_id=mlflow_exp_id):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    # Will be useful in STAGING/Evaluation\n",
    "    \n",
    "    # LOG MODEL\n",
    "    # Instantiate sklearn OneHotEncoder\n",
    "    sklearn_transformer = CountVectorizer(**sklearn_transformer_params)\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('sklearn fit transform on ingredients:', end='\\n')\n",
    "\n",
    "    # Do fit transform on data\n",
    "    response = sklearn_transformer.fit_transform(tqdm(to_nlp_df['ingredients'])) \n",
    "    \n",
    "    transformed_recipe = pd.DataFrame(\n",
    "            response.toarray(),\n",
    "            columns=sklearn_transformer.get_feature_names_out(),\n",
    "            index=to_nlp_df.index\n",
    "    )\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('Transformed Data:', end='\\n')\n",
    "    print(transformed_recipe)\n",
    "    \n",
    "    # mlflow.pyfunc.save_model(\n",
    "    #     path=model_directory,\n",
    "    #     code_path=[\"../src/\"],\n",
    "    #     python_model=CustomSKLearnWrapper(),\n",
    "    #     input_example=to_nlp_df['ingredients'][0],    \n",
    "    #     artifacts=artifacts\n",
    "    # )\n",
    "\n",
    "     # joblib.dump(sklearn_transformer, sklearn_transformer_path)\n",
    "    with open(sklearn_transformer_path, \"wb\") as fo:\n",
    "        pickle.dump(sklearn_transformer, fo)\n",
    "        # mlflow.log_artifact(sklearn_transformer_path,\n",
    "        #                     artifact_path='sklearn_transformer')\n",
    "\n",
    "    # joblib.dump(transformed_recipe, transformed_recipes_path)\n",
    "    with open(transformed_recipes_path, \"wb\") as fo:\n",
    "        pickle.dump(transformed_recipe, fo)\n",
    "        # mlflow.log_artifact(transformed_recipes_path,\n",
    "        #                     artifact_path='transformed_recipes')\n",
    "\n",
    "\n",
    "    model_info = mlflow.pyfunc.log_model( \n",
    "        code_path=[\"../src/\"],\n",
    "        python_model=CustomSKLearnWrapper(),\n",
    "        input_example=to_nlp_df['ingredients'][0],\n",
    "        signature=signature,        \n",
    "        artifact_path=\"sklearn_model\",\n",
    "        artifacts=artifacts\n",
    "        ) \n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50x14 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 63 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b48d6e1c5743ea8a25a5bfd1017632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/11 23:21:45 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n"
     ]
    }
   ],
   "source": [
    "test_predictor = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------\n",
      "Preprocessed Dataframe:\n",
      "                                                                        dek  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395             Can be prepared in 45 minutes or less.   \n",
      "54a4630719925f464b395a0c             Can be prepared in 45 minutes or less.   \n",
      "54a4246f6529d92b2c00a770                                                      \n",
      "54a469726529d92b2c0279a4  Easy and colorful, this side dish is nice with...   \n",
      "54a4345019925f464b386748  Goes great with: Couscous flavored with choppe...   \n",
      "\n",
      "                                                                        hed  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395  Braised Chicken with Mushrooms and Sun-Dried T...   \n",
      "54a4630719925f464b395a0c                         Radishes with Chive Butter   \n",
      "54a4246f6529d92b2c00a770                              Fried Blackberry Pies   \n",
      "54a469726529d92b2c0279a4                         Baby Carrots with Tarragon   \n",
      "54a4345019925f464b386748                          Moroccan Slow-Cooked Lamb   \n",
      "\n",
      "                          aggregateRating  \\\n",
      "id                                          \n",
      "54a418b26529d92b2c006395             3.32   \n",
      "54a4630719925f464b395a0c             2.00   \n",
      "54a4246f6529d92b2c00a770             0.00   \n",
      "54a469726529d92b2c0279a4             3.26   \n",
      "54a4345019925f464b386748             3.80   \n",
      "\n",
      "                                                                ingredients  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395  [1/3 cup thinly sliced drained sun-dried tomat...   \n",
      "54a4630719925f464b395a0c  [3/4 stick (6 tablespoons) unsalted butter, so...   \n",
      "54a4246f6529d92b2c00a770  [3 cups all purpose flour, 2 1/2 tablespoons s...   \n",
      "54a469726529d92b2c0279a4  [4 bunches baby carrots (each about 8 ounces),...   \n",
      "54a4345019925f464b386748  [1 tablespoon ground cumin, 2 teaspoons ground...   \n",
      "\n",
      "                                                                  prepSteps  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395  [In a heavy skillet heat the reserved tomato o...   \n",
      "54a4630719925f464b395a0c  [In a small bowl with a fork combine well the ...   \n",
      "54a4246f6529d92b2c00a770  [Whisk flour, sugar, and salt in medium bowl. ...   \n",
      "54a469726529d92b2c0279a4  [Combine carrots, 1/4 cup water, 1 1/2 tablesp...   \n",
      "54a4345019925f464b386748  [Mix first 6 ingredients in large bowl. Add la...   \n",
      "\n",
      "                          reviewsCount  willMakeAgainPct     cuisine_name  \\\n",
      "id                                                                          \n",
      "54a418b26529d92b2c006395            61                92  Missing Cuisine   \n",
      "54a4630719925f464b395a0c             4                50  Missing Cuisine   \n",
      "54a4246f6529d92b2c00a770             1                 0  Missing Cuisine   \n",
      "54a469726529d92b2c0279a4            16                78  Missing Cuisine   \n",
      "54a4345019925f464b386748           182                96          African   \n",
      "\n",
      "                                               photo_filename  \\\n",
      "id                                                              \n",
      "54a418b26529d92b2c006395  EP_12162015_placeholders_formal.jpg   \n",
      "54a4630719925f464b395a0c  EP_12162015_placeholders_formal.jpg   \n",
      "54a4246f6529d92b2c00a770  EP_12162015_placeholders_formal.jpg   \n",
      "54a469726529d92b2c0279a4  EP_12162015_placeholders_formal.jpg   \n",
      "54a4345019925f464b386748                           231597.jpg   \n",
      "\n",
      "                                                               photo_credit  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a4630719925f464b395a0c  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a4246f6529d92b2c00a770  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a469726529d92b2c0279a4  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a4345019925f464b386748                                      Brian Leatart   \n",
      "\n",
      "                                  author_name            date_published  \\\n",
      "id                                                                        \n",
      "54a418b26529d92b2c006395  Missing Author Name 2004-08-20 04:00:00+00:00   \n",
      "54a4630719925f464b395a0c  Missing Author Name 2004-08-20 04:00:00+00:00   \n",
      "54a4246f6529d92b2c00a770  Missing Author Name 2010-05-14 04:00:00+00:00   \n",
      "54a469726529d92b2c0279a4  Missing Author Name 2004-08-20 04:00:00+00:00   \n",
      "54a4345019925f464b386748  Missing Author Name 2005-01-28 21:19:07+00:00   \n",
      "\n",
      "                                                                 recipe_url  \n",
      "id                                                                           \n",
      "54a418b26529d92b2c006395  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a4630719925f464b395a0c  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a4246f6529d92b2c00a770  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a469726529d92b2c0279a4  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a4345019925f464b386748  https://www.epicurious.com/recipes/food/views/...  \n",
      "(49, 13)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Subset Dataframe:\n",
      "                                                                        dek  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395             Can be prepared in 45 minutes or less.   \n",
      "54a4630719925f464b395a0c             Can be prepared in 45 minutes or less.   \n",
      "54a4246f6529d92b2c00a770                                                      \n",
      "54a469726529d92b2c0279a4  Easy and colorful, this side dish is nice with...   \n",
      "54a4345019925f464b386748  Goes great with: Couscous flavored with choppe...   \n",
      "\n",
      "                                                                        hed  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395  Braised Chicken with Mushrooms and Sun-Dried T...   \n",
      "54a4630719925f464b395a0c                         Radishes with Chive Butter   \n",
      "54a4246f6529d92b2c00a770                              Fried Blackberry Pies   \n",
      "54a469726529d92b2c0279a4                         Baby Carrots with Tarragon   \n",
      "54a4345019925f464b386748                          Moroccan Slow-Cooked Lamb   \n",
      "\n",
      "                          aggregateRating  \\\n",
      "id                                          \n",
      "54a418b26529d92b2c006395             3.32   \n",
      "54a4630719925f464b395a0c             2.00   \n",
      "54a4246f6529d92b2c00a770             0.00   \n",
      "54a469726529d92b2c0279a4             3.26   \n",
      "54a4345019925f464b386748             3.80   \n",
      "\n",
      "                                                                ingredients  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395  [1/3 cup thinly sliced drained sun-dried tomat...   \n",
      "54a4630719925f464b395a0c  [3/4 stick (6 tablespoons) unsalted butter, so...   \n",
      "54a4246f6529d92b2c00a770  [3 cups all purpose flour, 2 1/2 tablespoons s...   \n",
      "54a469726529d92b2c0279a4  [4 bunches baby carrots (each about 8 ounces),...   \n",
      "54a4345019925f464b386748  [1 tablespoon ground cumin, 2 teaspoons ground...   \n",
      "\n",
      "                                                                  prepSteps  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395  [In a heavy skillet heat the reserved tomato o...   \n",
      "54a4630719925f464b395a0c  [In a small bowl with a fork combine well the ...   \n",
      "54a4246f6529d92b2c00a770  [Whisk flour, sugar, and salt in medium bowl. ...   \n",
      "54a469726529d92b2c0279a4  [Combine carrots, 1/4 cup water, 1 1/2 tablesp...   \n",
      "54a4345019925f464b386748  [Mix first 6 ingredients in large bowl. Add la...   \n",
      "\n",
      "                          reviewsCount  willMakeAgainPct     cuisine_name  \\\n",
      "id                                                                          \n",
      "54a418b26529d92b2c006395            61                92  Missing Cuisine   \n",
      "54a4630719925f464b395a0c             4                50  Missing Cuisine   \n",
      "54a4246f6529d92b2c00a770             1                 0  Missing Cuisine   \n",
      "54a469726529d92b2c0279a4            16                78  Missing Cuisine   \n",
      "54a4345019925f464b386748           182                96          African   \n",
      "\n",
      "                                               photo_filename  \\\n",
      "id                                                              \n",
      "54a418b26529d92b2c006395  EP_12162015_placeholders_formal.jpg   \n",
      "54a4630719925f464b395a0c  EP_12162015_placeholders_formal.jpg   \n",
      "54a4246f6529d92b2c00a770  EP_12162015_placeholders_formal.jpg   \n",
      "54a469726529d92b2c0279a4  EP_12162015_placeholders_formal.jpg   \n",
      "54a4345019925f464b386748                           231597.jpg   \n",
      "\n",
      "                                                               photo_credit  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a4630719925f464b395a0c  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a4246f6529d92b2c00a770  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a469726529d92b2c0279a4  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a4345019925f464b386748                                      Brian Leatart   \n",
      "\n",
      "                                  author_name            date_published  \\\n",
      "id                                                                        \n",
      "54a418b26529d92b2c006395  Missing Author Name 2004-08-20 04:00:00+00:00   \n",
      "54a4630719925f464b395a0c  Missing Author Name 2004-08-20 04:00:00+00:00   \n",
      "54a4246f6529d92b2c00a770  Missing Author Name 2010-05-14 04:00:00+00:00   \n",
      "54a469726529d92b2c0279a4  Missing Author Name 2004-08-20 04:00:00+00:00   \n",
      "54a4345019925f464b386748  Missing Author Name 2005-01-28 21:19:07+00:00   \n",
      "\n",
      "                                                                 recipe_url  \n",
      "id                                                                           \n",
      "54a418b26529d92b2c006395  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a4630719925f464b395a0c  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a4246f6529d92b2c00a770  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a469726529d92b2c0279a4  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a4345019925f464b386748  https://www.epicurious.com/recipes/food/views/...  \n",
      "(49, 13)\n"
     ]
    }
   ],
   "source": [
    "# pre_proc_df is cleaned dataframe\n",
    "pre_proc_test_df = dfpp.preprocess_dataframe(test_df)\n",
    "print('\\n')\n",
    "print('--------------')\n",
    "print('Preprocessed Dataframe:', end='\\n')\n",
    "print(pre_proc_test_df.head())\n",
    "print(pre_proc_test_df.shape)\n",
    "\n",
    "# create subset for dev purposes\n",
    "to_nlp_test_df = pre_proc_test_df\n",
    "print('\\n')\n",
    "print('-' * 80)\n",
    "print('Subset Dataframe:', end='\\n')\n",
    "print(to_nlp_test_df.head())\n",
    "print(to_nlp_test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1, 14), indices imply (49, 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_nlp_test_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mingredients\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py:491\u001b[0m, in \u001b[0;36mPyFuncModel.predict\u001b[0;34m(self, data, params)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _MLFLOW_TESTING\u001b[38;5;241m.\u001b[39mget():\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py:477\u001b[0m, in \u001b[0;36mPyFuncModel.predict.<locals>._predict\u001b[0;34m()\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m():\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Models saved prior to MLflow 2.5.0 do not support `params` in the pyfunc `predict()`\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# function definition, nor do they support `**kwargs`. Accordingly, we only pass\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# `params` to the `predict()` method if it defines the `params` argument\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_fn)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 477\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m     _log_warning_if_params_not_in_predict_signature(_logger, params)\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_fn(data)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/mlflow/pyfunc/model.py:469\u001b[0m, in \u001b[0;36m_PythonModelPyfuncWrapper.predict\u001b[0;34m(self, model_input, params)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m:param model_input: Model input data.\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m:param params: Additional parameters to pass to the model for inference.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m:return: Model predictions.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_model\u001b[38;5;241m.\u001b[39mpredict)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m _log_warning_if_params_not_in_predict_signature(_logger, params)\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_model\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(model_input))\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/src/custom_stanza_mlflow.py:76\u001b[0m, in \u001b[0;36mCustomSKLearnWrapper.predict\u001b[0;34m(self, context, model_input, params)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03mThis method is needed to override the default predict.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mIt needs to function essentially as a wrapper and returns back the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    the sklearn/Stanza text processing\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msklearn_transformer\u001b[38;5;241m.\u001b[39mtransform(model_input)\n\u001b[0;32m---> 76\u001b[0m transformed_recipe \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msklearn_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transformed_recipe\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/frame.py:722\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    712\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    713\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    714\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    719\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    720\u001b[0m         )\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:349\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    345\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[1;32m    346\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[1;32m    347\u001b[0m )\n\u001b[0;32m--> 349\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1, 14), indices imply (49, 14)"
     ]
    }
   ],
   "source": [
    "test_predictor.predict(to_nlp_test_df['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_nlp_test_df['ingredients'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_nlp_test_df['ingredients'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictor.predict(to_nlp_test_df['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictor.signature.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info.signature.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# load dataframes from artifacts\n",
    "# mlflow.artifacts.download_artifacts(\n",
    "#     run_id=mlflow_run_id\n",
    "# )\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':'auto',\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(cv_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_stanza_ingreds_full_set_v1\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Raw Dataframe:', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    # to_nlp_df = pre_proc_df[0:50]\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Subset Dataframe:', end='\\n')\n",
    "    # print(to_nlp_df.head())\n",
    "    # print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params,\n",
    "    )\n",
    "\n",
    "    def custom_analyzer(step_list, stanza_pipeline, minNgramLength, maxNgramLength):\n",
    "            lowered = \" brk \".join(map(str, [step for step in step_list if step is not None])).lower()\n",
    "\n",
    "            preproc = stanza_pipeline(lowered)\n",
    "            \n",
    "            lemmad = \" \".join(map(str,\n",
    "                                [word.text\n",
    "                                for sent in preproc.sentences \n",
    "                                for word in sent.words if (\n",
    "                                    word is not None\n",
    "                                )]\n",
    "                            )\n",
    "                        )\n",
    "            \n",
    "            # analyze each line of the input string seperately\n",
    "            for ln in lemmad.split(' brk '):\n",
    "                \n",
    "                # tokenize the input string (customize the regex as desired)\n",
    "                at_least_two_english_characters_whole_words = \"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "                terms = re.split(at_least_two_english_characters_whole_words, ln)\n",
    "\n",
    "                # loop ngram creation for every number between min and max ngram length\n",
    "                for ngramLength in range(minNgramLength, maxNgramLength+1):\n",
    "\n",
    "                    # find and return all ngrams\n",
    "                    # for ngram in zip(*[terms[i:] for i in range(3)]): \n",
    "                        # <-- solution without a generator (works the same but has higher memory usage)\n",
    "                    for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]):   # <-- solution using a generator\n",
    "                        \n",
    "                        ngram = ' '.join(map(str, ngram))\n",
    "                        # yield ngram\n",
    "                        return str(ngram)\n",
    "\n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4}\n",
    "    \n",
    "    recipe_ingreds = pre_proc_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('Recipe ingredients:', end='\\n')\n",
    "    print(recipe_ingreds)\n",
    "\n",
    "    # train on the recipes' steps\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    # steps_vectorizer_model = CountVectorizer(**cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    steps_vectorizer_model = OnlineCountVectorizer(**cv_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=steps_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_full_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_full_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt run with lighter weight configuration\n",
    "#### This attempt will still use Stanza processing on the ingredients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_nlp_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# create sklearn pipeline as in BERTopic lightweight configuration\n",
    "# pipe = make_pipeline(\n",
    "#     TfidfVectorizer(**sklearn_nlp_params),\n",
    "#     TruncatedSVD(100)\n",
    "# )\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    # 'embedding_model': TfidfVectorizer(**sklearn_nlp_params),\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':50,\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True,\n",
    "    # 'min_cluster_size': 10 # Possibly only works if modifying individual HDBSCAN component of BERTopic\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_nlp_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_lightweight_stanza_ingreds_small_set_v1\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Raw Dataframe: ', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    to_nlp_df = pre_proc_df[0:100]\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Subset Dataframe:', end='\\n')\n",
    "    print(to_nlp_df.head())\n",
    "    print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params\n",
    "    )\n",
    "    \n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4}\n",
    "    \n",
    "    recipe_ingreds = to_nlp_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(**sklearn_nlp_params)\n",
    "    embeddings = vectorizer.fit_transform(recipe_ingreds)\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    print(recipe_ingreds)\n",
    "\n",
    "    # train on the recipes' ingredientss\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds, embeddings)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    sklearn_cv_params = {\n",
    "        'strip_accents':\"unicode\",\n",
    "        'lowercase':True,\n",
    "        'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "        # 'min_df':10,\n",
    "    }\n",
    "    steps_vectorizer_model = CountVectorizer(**sklearn_cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    # steps_vectorizer_model = OnlineCountVectorizer(**sklearn_nlp_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=steps_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representative Docs:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_full_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_full_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()['Representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()['Representation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_nlp_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# create sklearn pipeline as in BERTopic lightweight configuration\n",
    "# pipe = make_pipeline(\n",
    "#     TfidfVectorizer(**sklearn_nlp_params),\n",
    "#     TruncatedSVD(100)\n",
    "# )\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    # 'embedding_model': TfidfVectorizer(**sklearn_nlp_params),\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':50,\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True,\n",
    "    # 'min_cluster_size': 10 # Possibly only works if modifying individual HDBSCAN component of BERTopic\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_nlp_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_lightweight_stanza_ingreds_small_set_v1\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Raw Dataframe: ', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    to_nlp_df = pre_proc_df[0:100]\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Subset Dataframe:', end='\\n')\n",
    "    print(to_nlp_df.head())\n",
    "    print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params\n",
    "    )\n",
    "    \n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4}\n",
    "    \n",
    "    recipe_ingreds = to_nlp_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(**sklearn_nlp_params)\n",
    "    embeddings = vectorizer.fit_transform(recipe_ingreds)\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    print(recipe_ingreds)\n",
    "\n",
    "    # train on the recipes' ingredientss\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds, embeddings)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    sklearn_cv_params = {\n",
    "        'strip_accents':\"unicode\",\n",
    "        'lowercase':True,\n",
    "        # 'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "        # 'min_df':10,\n",
    "    }\n",
    "    steps_vectorizer_model = CountVectorizer(**sklearn_cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    # steps_vectorizer_model = OnlineCountVectorizer(**sklearn_nlp_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=steps_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representative Docs:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_full_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_full_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_nlp_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# create sklearn pipeline as in BERTopic lightweight configuration\n",
    "# pipe = make_pipeline(\n",
    "#     TfidfVectorizer(**sklearn_nlp_params),\n",
    "#     TruncatedSVD(100)\n",
    "# )\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    # 'embedding_model': TfidfVectorizer(**sklearn_nlp_params),\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':50,\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True,\n",
    "    # 'min_cluster_size': 10 # Possibly only works if modifying individual HDBSCAN component of BERTopic\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_nlp_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_lightweight_stanza_ingreds_small_set_v1.01\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Raw Dataframe: ', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    to_nlp_df = pre_proc_df[0:100]\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Subset Dataframe:', end='\\n')\n",
    "    print(to_nlp_df.head())\n",
    "    print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params\n",
    "    )\n",
    "    \n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4\n",
    "                       , 'lemmatize': True}\n",
    "    \n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    recipe_ingreds = to_nlp_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    print([ingred for ingred in recipe_ingreds])\n",
    "\n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(**sklearn_nlp_params)\n",
    "    embeddings = vectorizer.fit_transform(tqdm(recipe_ingreds))\n",
    "\n",
    "    # train on the recipes' ingredientss\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds, embeddings)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    sklearn_cv_params = {\n",
    "        # 'strip_accents':\"unicode\",\n",
    "        # 'lowercase':True,\n",
    "        # 'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "        # 'min_df':10,\n",
    "        'token_pattern': r\"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "    }\n",
    "    ingreds_vectorizer_model = CountVectorizer(**sklearn_cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    # steps_vectorizer_model = OnlineCountVectorizer(**sklearn_nlp_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=ingreds_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representative Docs:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_small_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_small_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_nlp_df['ingredients'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipe_ingreds = to_nlp_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i think i should start leaving out units/including stopwords again since i'm not using Stanza's deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_nlp_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':50,\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True,\n",
    "    # 'min_cluster_size': 10 # Possibly only works if modifying individual HDBSCAN component of BERTopic\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_nlp_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_lightweight_stanza_ingreds_full_set_v1.00\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Raw Dataframe: ', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    # to_nlp_df = pre_proc_df[0:100]\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print(f'{datetime.now()}, Subset Dataframe:', end='\\n')\n",
    "    # print(to_nlp_df.head())\n",
    "    # print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params\n",
    "    )\n",
    "    \n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4\n",
    "                       , 'lemmatize': True}\n",
    "    \n",
    "    recipe_ingreds = pre_proc_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "    \n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    print(recipe_ingreds)\n",
    "\n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(**sklearn_nlp_params)\n",
    "    embeddings = vectorizer.fit_transform(tqdm(recipe_ingreds))\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    # print(recipe_ingreds)\n",
    "\n",
    "    # train on the recipes' ingredientss\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds, embeddings)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    sklearn_cv_params = {\n",
    "        # 'strip_accents':\"unicode\",\n",
    "        # 'lowercase':True,\n",
    "        'token_pattern': r\"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "    }\n",
    "    ingreds_vectorizer_model = CountVectorizer(**sklearn_cv_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=ingreds_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representative Docs:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_full_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_full_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try splitting among CPU and GPU. Try Stanza on CPU due to its memory usage\n",
    "nlp2 = stanza.Pipeline('en', use_gpu=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
