{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: test\n",
    "output-file: template.html\n",
    "title: Template\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import OnlineCountVectorizer\n",
    "import dagshub\n",
    "from datetime import datetime\n",
    "import dill as pickle\n",
    "import dvc.api\n",
    "from hdbscan import HDBSCAN\n",
    "from itertools import tee, islice\n",
    "import joblib\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import nbdev\n",
    "from nbdev.showdoc import *\n",
    "import pandas as pd\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer\n",
    "    , TfidfTransformer\n",
    "    , TfidfVectorizer\n",
    "    , \n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from src.custom_sklearn_text_transformer_mlflow import CustomSKLearnAnalyzer\n",
    "from src.custom_stanza_mlflow import CustomSKLearnWrapper\n",
    "import src.dataframe_preprocessor as dfpp\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | export\n",
    "def foo():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# this function allows us to get the experiment ID from an experiment name\n",
    "def get_experiment_id(name):\n",
    "    exp = mlflow.get_experiment_by_name(name)\n",
    "    if exp is None:\n",
    "      exp_id = mlflow.create_experiment(name)\n",
    "      return exp_id\n",
    "    return exp.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_analyzer(step_list, stanza_pipeline, minNgramLength, maxNgramLength, lemmatize=True):\n",
    "    lowered = \" brk \".join(map(str, [step for step in step_list if step is not None])).lower()\n",
    "\n",
    "    preproc = stanza_pipeline(lowered)\n",
    "    \n",
    "    if lemmatize:\n",
    "        lemmad = \" \".join(map(str,\n",
    "                            [word.lemma\n",
    "                            for sent in preproc.sentences \n",
    "                            for word in sent.words if (\n",
    "                                word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\", \"PUNCT\"]\n",
    "                                and word is not None\n",
    "                            )]\n",
    "                        )\n",
    "                    )\n",
    "    else:\n",
    "        lemmad = \" \".join(map(str,\n",
    "                            [word.text\n",
    "                            for sent in preproc.sentences \n",
    "                            for word in sent.words if (\n",
    "                                word is not None\n",
    "                            )]\n",
    "                        )\n",
    "                    )\n",
    "    # analyze each line of the input string seperately\n",
    "    for ln in lemmad.split(' brk '):\n",
    "        # tokenize the input string (customize the regex as desired)\n",
    "        at_least_two_english_characters_whole_words = \"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "        terms = re.split(at_least_two_english_characters_whole_words, ln)\n",
    "\n",
    "        # loop ngram creation for every number between min and max ngram length\n",
    "        for ngramLength in range(minNgramLength, maxNgramLength+1):\n",
    "\n",
    "            # find and return all ngrams\n",
    "            # for ngram in zip(*[terms[i:] for i in range(3)]): \n",
    "                # <-- solution without a generator (works the same but has higher memory usage)\n",
    "            for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]):   # <-- solution using a generator\n",
    "                \n",
    "                ngram = ' '.join(map(str, ngram))\n",
    "                # yield ngram\n",
    "                return str(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | Below this are blocks to use DagsHub with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@markdown Enter the username of your DAGsHub account:\n",
    "DAGSHUB_USER_NAME = \"AaronWChen\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the email for your DAGsHub account:\n",
    "DAGSHUB_EMAIL = \"awc33@cornell.edu\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the repo name \n",
    "DAGSHUB_REPO_NAME = \"MeaLeon\"\n",
    "\n",
    "#@markdown Enter the name of the branch you are working on \n",
    "BRANCH = \"MLF-1/start-custom-sklearn-mlflow-model\"\n",
    "dagshub.init(repo_name=DAGSHUB_REPO_NAME\n",
    "             , repo_owner=DAGSHUB_USER_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting DEV stage for One Hot Encoded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# starter idea for making an experiment name can be the git branch, but need more specificity\n",
    "experiment_name = f\"{DAGSHUB_EMAIL}/one-hot-encode\"\n",
    "mlflow_exp_id = get_experiment_id(experiment_name)\n",
    "\n",
    "# define model location\n",
    "# model_directory = \"/tmp/sklearn_model\"\n",
    "model_directory = \"../models/sklearn_model\"\n",
    "\n",
    "# Define the required artifacts associated with the saved custom pyfunc\n",
    "# sklearn_path = model_directory + \"\"\n",
    "sklearn_model_path = model_directory + \"/python_model.pkl\"\n",
    "sklearn_transformer_path = model_directory + \"/sklearn_transformer.pkl\"\n",
    "transformed_recipes_path = model_directory + \"/transformed_recipes.pkl\"\n",
    "\n",
    "artifacts = {'sklearn_model': sklearn_model_path,\n",
    "             'sklearn_transformer': sklearn_transformer_path,\n",
    "             'transformed_recipes': transformed_recipes_path\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45df185140244992852f6d67d22ce2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 23:45:20 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-02-07 23:45:21 INFO: File exists: /home/awchen/stanza_resources/en/default.zip\n",
      "2024-02-07 23:45:24 INFO: Finished downloading models and saved to /home/awchen/stanza_resources.\n",
      "2024-02-07 23:45:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696d1140069e40f3960c317b13678d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 23:45:25 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2024-02-07 23:45:25 INFO: Using device: cpu\n",
      "2024-02-07 23:45:25 INFO: Loading: tokenize\n",
      "2024-02-07 23:45:25 INFO: Loading: pos\n",
      "2024-02-07 23:45:25 INFO: Loading: lemma\n",
      "2024-02-07 23:45:26 INFO: Loading: constituency\n",
      "2024-02-07 23:45:26 INFO: Loading: depparse\n",
      "2024-02-07 23:45:26 INFO: Loading: sentiment\n",
      "2024-02-07 23:45:26 INFO: Loading: ner\n",
      "2024-02-07 23:45:27 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------\n",
      "Raw Dataframe:\n",
      "                         id  \\\n",
      "0  54a2b6b019925f464b373351   \n",
      "1  54a408a019925f464b3733bc   \n",
      "2  54a408a26529d92b2c003631   \n",
      "3  54a408a66529d92b2c003638   \n",
      "4  54a408a719925f464b3733cc   \n",
      "\n",
      "                                                 dek  \\\n",
      "0  How does fried chicken achieve No. 1 status? B...   \n",
      "1                                Spinaci all'Ebraica   \n",
      "2  This majestic, moist, and richly spiced honey ...   \n",
      "3  The idea for this sandwich came to me when my ...   \n",
      "4  In 1930, Simon Agranat, the chief justice of t...   \n",
      "\n",
      "                                     hed                   pubDate  \\\n",
      "0            Pickle-Brined Fried Chicken  2014-08-19T04:00:00.000Z   \n",
      "1                   Spinach Jewish Style  2008-09-09T04:00:00.000Z   \n",
      "2                  New Year’s Honey Cake  2008-09-10T04:00:00.000Z   \n",
      "3  The B.L.A.Bagel with Lox and Avocado  2008-09-08T04:00:00.000Z   \n",
      "4        Shakshuka a la Doktor Shakshuka  2008-09-09T04:00:00.000Z   \n",
      "\n",
      "                             author    type  \\\n",
      "0                                []  recipe   \n",
      "1  [{'name': 'Edda Servi Machlin'}]  recipe   \n",
      "2       [{'name': 'Marcy Goldman'}]  recipe   \n",
      "3           [{'name': 'Faye Levy'}]  recipe   \n",
      "4         [{'name': 'Joan Nathan'}]  recipe   \n",
      "\n",
      "                                                 url  \\\n",
      "0  /recipes/food/views/pickle-brined-fried-chicke...   \n",
      "1    /recipes/food/views/spinach-jewish-style-350152   \n",
      "2  /recipes/food/views/majestic-and-moist-new-yea...   \n",
      "3  /recipes/food/views/the-b-l-a-bagel-with-lox-a...   \n",
      "4  /recipes/food/views/shakshuka-a-la-doktor-shak...   \n",
      "\n",
      "                                           photoData  \\\n",
      "0  {'id': '54a2b64a6529d92b2c003409', 'filename':...   \n",
      "1  {'id': '56746182accb4c9831e45e0a', 'filename':...   \n",
      "2  {'id': '55e85ba4cf90d6663f728014', 'filename':...   \n",
      "3  {'id': '5674617e47d1a28026045e4f', 'filename':...   \n",
      "4  {'id': '56746183b47c050a284a4e15', 'filename':...   \n",
      "\n",
      "                                                 tag  aggregateRating  \\\n",
      "0  {'category': 'ingredient', 'name': 'Chicken', ...             3.11   \n",
      "1  {'category': 'cuisine', 'name': 'Italian', 'ur...             3.22   \n",
      "2  {'category': 'cuisine', 'name': 'Jewish', 'url...             3.62   \n",
      "3  {'category': 'cuisine', 'name': 'Jewish', 'url...             4.00   \n",
      "4  {'category': 'cuisine', 'name': 'Jewish', 'url...             2.71   \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  [1 tablespoons yellow mustard seeds, 1 tablesp...   \n",
      "1  [3 pounds small-leaved bulk spinach, Salt, 1/2...   \n",
      "2  [3 1/2 cups all-purpose flour, 1 tablespoon ba...   \n",
      "3  [1 small ripe avocado, preferably Hass (see No...   \n",
      "4  [2 pounds fresh tomatoes, unpeeled and cut in ...   \n",
      "\n",
      "                                           prepSteps  reviewsCount  \\\n",
      "0  [Toast mustard and coriander seeds in a dry me...             7   \n",
      "1  [Remove the stems and roots from the spinach. ...             5   \n",
      "2  [I like this cake best baked in a 9-inch angel...           105   \n",
      "3  [A short time before serving, mash avocado and...             7   \n",
      "4  [1. Place the tomatoes, garlic, salt, paprika,...             7   \n",
      "\n",
      "   willMakeAgainPct  dateCrawled  \n",
      "0               100   1498547035  \n",
      "1                80   1498547740  \n",
      "2                88   1498547738  \n",
      "3               100   1498547740  \n",
      "4                83   1498547740  \n",
      "(34756, 15)\n"
     ]
    }
   ],
   "source": [
    "# instantiate stanza pipeline\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', \n",
    "                    depparse_batch_size=50, \n",
    "                    depparse_min_length_to_batch_separately=50,\n",
    "                    verbose=True,\n",
    "                    use_gpu=False, # set to true when on cloud/not on streaming computer\n",
    "                    batch_size=100\n",
    "                    )\n",
    "\n",
    "# load raw data and preprocess/clean\n",
    "data = dvc.api.read(\n",
    "    path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "    , mode='r')\n",
    "raw_df = pd.read_json(data)\n",
    "print('\\n')\n",
    "print('--------------')\n",
    "print('Raw Dataframe:', end='\\n')\n",
    "print(raw_df.head())\n",
    "print(raw_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------\n",
      "Preprocessed Dataframe:\n",
      "                                                                        dek  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc                                                      \n",
      "54a42cde19925f464b3809d2  Green chiles pickled in soy sauce and vinegar ...   \n",
      "54a433036529d92b2c015de3  This soup features the flavors of India: aroma...   \n",
      "54a451926529d92b2c01eda8                                                      \n",
      "54a430876529d92b2c013e2b  Brown sugar and molasses are balanced by fresh...   \n",
      "\n",
      "                                                                        hed  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  Grilled Hearts of Romaine with Blue Cheese Vin...   \n",
      "54a42cde19925f464b3809d2                              Soy-Pickled Jalapeños   \n",
      "54a433036529d92b2c015de3  Curried Potato and Spinach Soup with Onion Sal...   \n",
      "54a451926529d92b2c01eda8                                       Chicken Soup   \n",
      "54a430876529d92b2c013e2b                           Sweet-Hot Barbecue Sauce   \n",
      "\n",
      "                          aggregateRating  \\\n",
      "id                                          \n",
      "54a4270b19925f464b37c1dc             3.64   \n",
      "54a42cde19925f464b3809d2             3.43   \n",
      "54a433036529d92b2c015de3             3.00   \n",
      "54a451926529d92b2c01eda8             3.19   \n",
      "54a430876529d92b2c013e2b             0.00   \n",
      "\n",
      "                                                                ingredients  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  [1 1/2 cups white wine vinegar, 1/2 cup sugar,...   \n",
      "54a42cde19925f464b3809d2  [3 large fresh jalapeños (4 inches), sliced 1/...   \n",
      "54a433036529d92b2c015de3  [4 cups chopped red onions (about 2 large), 1 ...   \n",
      "54a451926529d92b2c01eda8  [1 pound chicken parts, 2 stalks celery, inclu...   \n",
      "54a430876529d92b2c013e2b  [2 tablespoons olive oil, 1 cup chopped onion,...   \n",
      "\n",
      "                                                                  prepSteps  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  [Combine first 5 ingredients and 1/4 teaspoon ...   \n",
      "54a42cde19925f464b3809d2  [Combine all ingredients in a small heavy sauc...   \n",
      "54a433036529d92b2c015de3  [Combine first 5 ingredients in heavy medium s...   \n",
      "54a451926529d92b2c01eda8  [1. Pour 12 cups of cold water into a large st...   \n",
      "54a430876529d92b2c013e2b  [Heat oil in large saucepan over medium-high h...   \n",
      "\n",
      "                          reviewsCount  willMakeAgainPct     cuisine_name  \\\n",
      "id                                                                          \n",
      "54a4270b19925f464b37c1dc             9               100  Missing Cuisine   \n",
      "54a42cde19925f464b3809d2             6               100  Missing Cuisine   \n",
      "54a433036529d92b2c015de3             6                67           Indian   \n",
      "54a451926529d92b2c01eda8            32                87           Kosher   \n",
      "54a430876529d92b2c013e2b             0                 0  Missing Cuisine   \n",
      "\n",
      "                                               photo_filename  \\\n",
      "id                                                              \n",
      "54a4270b19925f464b37c1dc  EP_12162015_placeholders_casual.jpg   \n",
      "54a42cde19925f464b3809d2  EP_12162015_placeholders_rustic.jpg   \n",
      "54a433036529d92b2c015de3                           234125.jpg   \n",
      "54a451926529d92b2c01eda8  EP_12162015_placeholders_formal.jpg   \n",
      "54a430876529d92b2c013e2b  EP_12162015_placeholders_rustic.jpg   \n",
      "\n",
      "                                                               photo_credit  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a42cde19925f464b3809d2  Photo by Chelsea Kyle, Prop Styling by Anna St...   \n",
      "54a433036529d92b2c015de3                                      Brian Leatart   \n",
      "54a451926529d92b2c01eda8  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a430876529d92b2c013e2b  Photo by Chelsea Kyle, Prop Styling by Anna St...   \n",
      "\n",
      "                              author_name            date_published  \\\n",
      "id                                                                    \n",
      "54a4270b19925f464b37c1dc    Kate Higgins  2010-12-16 04:00:00+00:00   \n",
      "54a42cde19925f464b3809d2     Lillian Chou 2009-02-19 04:00:00+00:00   \n",
      "54a433036529d92b2c015de3     Peter Gordon 2006-03-07 04:00:00+00:00   \n",
      "54a451926529d92b2c01eda8  Sharon Lebewohl 2004-08-20 04:00:00+00:00   \n",
      "54a430876529d92b2c013e2b   Suzanne Tracht 2007-12-03 20:11:11+00:00   \n",
      "\n",
      "                                                                 recipe_url  \n",
      "id                                                                           \n",
      "54a4270b19925f464b37c1dc  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a42cde19925f464b3809d2  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a433036529d92b2c015de3  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a451926529d92b2c01eda8  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a430876529d92b2c013e2b  https://www.epicurious.com/recipes/food/views/...  \n",
      "(50, 13)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Subset Dataframe:\n",
      "                                                                        dek  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc                                                      \n",
      "54a42cde19925f464b3809d2  Green chiles pickled in soy sauce and vinegar ...   \n",
      "54a433036529d92b2c015de3  This soup features the flavors of India: aroma...   \n",
      "54a451926529d92b2c01eda8                                                      \n",
      "54a430876529d92b2c013e2b  Brown sugar and molasses are balanced by fresh...   \n",
      "\n",
      "                                                                        hed  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  Grilled Hearts of Romaine with Blue Cheese Vin...   \n",
      "54a42cde19925f464b3809d2                              Soy-Pickled Jalapeños   \n",
      "54a433036529d92b2c015de3  Curried Potato and Spinach Soup with Onion Sal...   \n",
      "54a451926529d92b2c01eda8                                       Chicken Soup   \n",
      "54a430876529d92b2c013e2b                           Sweet-Hot Barbecue Sauce   \n",
      "\n",
      "                          aggregateRating  \\\n",
      "id                                          \n",
      "54a4270b19925f464b37c1dc             3.64   \n",
      "54a42cde19925f464b3809d2             3.43   \n",
      "54a433036529d92b2c015de3             3.00   \n",
      "54a451926529d92b2c01eda8             3.19   \n",
      "54a430876529d92b2c013e2b             0.00   \n",
      "\n",
      "                                                                ingredients  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  [1 1/2 cups white wine vinegar, 1/2 cup sugar,...   \n",
      "54a42cde19925f464b3809d2  [3 large fresh jalapeños (4 inches), sliced 1/...   \n",
      "54a433036529d92b2c015de3  [4 cups chopped red onions (about 2 large), 1 ...   \n",
      "54a451926529d92b2c01eda8  [1 pound chicken parts, 2 stalks celery, inclu...   \n",
      "54a430876529d92b2c013e2b  [2 tablespoons olive oil, 1 cup chopped onion,...   \n",
      "\n",
      "                                                                  prepSteps  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  [Combine first 5 ingredients and 1/4 teaspoon ...   \n",
      "54a42cde19925f464b3809d2  [Combine all ingredients in a small heavy sauc...   \n",
      "54a433036529d92b2c015de3  [Combine first 5 ingredients in heavy medium s...   \n",
      "54a451926529d92b2c01eda8  [1. Pour 12 cups of cold water into a large st...   \n",
      "54a430876529d92b2c013e2b  [Heat oil in large saucepan over medium-high h...   \n",
      "\n",
      "                          reviewsCount  willMakeAgainPct     cuisine_name  \\\n",
      "id                                                                          \n",
      "54a4270b19925f464b37c1dc             9               100  Missing Cuisine   \n",
      "54a42cde19925f464b3809d2             6               100  Missing Cuisine   \n",
      "54a433036529d92b2c015de3             6                67           Indian   \n",
      "54a451926529d92b2c01eda8            32                87           Kosher   \n",
      "54a430876529d92b2c013e2b             0                 0  Missing Cuisine   \n",
      "\n",
      "                                               photo_filename  \\\n",
      "id                                                              \n",
      "54a4270b19925f464b37c1dc  EP_12162015_placeholders_casual.jpg   \n",
      "54a42cde19925f464b3809d2  EP_12162015_placeholders_rustic.jpg   \n",
      "54a433036529d92b2c015de3                           234125.jpg   \n",
      "54a451926529d92b2c01eda8  EP_12162015_placeholders_formal.jpg   \n",
      "54a430876529d92b2c013e2b  EP_12162015_placeholders_rustic.jpg   \n",
      "\n",
      "                                                               photo_credit  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a42cde19925f464b3809d2  Photo by Chelsea Kyle, Prop Styling by Anna St...   \n",
      "54a433036529d92b2c015de3                                      Brian Leatart   \n",
      "54a451926529d92b2c01eda8  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a430876529d92b2c013e2b  Photo by Chelsea Kyle, Prop Styling by Anna St...   \n",
      "\n",
      "                              author_name            date_published  \\\n",
      "id                                                                    \n",
      "54a4270b19925f464b37c1dc    Kate Higgins  2010-12-16 04:00:00+00:00   \n",
      "54a42cde19925f464b3809d2     Lillian Chou 2009-02-19 04:00:00+00:00   \n",
      "54a433036529d92b2c015de3     Peter Gordon 2006-03-07 04:00:00+00:00   \n",
      "54a451926529d92b2c01eda8  Sharon Lebewohl 2004-08-20 04:00:00+00:00   \n",
      "54a430876529d92b2c013e2b   Suzanne Tracht 2007-12-03 20:11:11+00:00   \n",
      "\n",
      "                                                                 recipe_url  \n",
      "id                                                                           \n",
      "54a4270b19925f464b37c1dc  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a42cde19925f464b3809d2  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a433036529d92b2c015de3  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a451926529d92b2c01eda8  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a430876529d92b2c013e2b  https://www.epicurious.com/recipes/food/views/...  \n",
      "(50, 13)\n"
     ]
    }
   ],
   "source": [
    "# take sample and train/test split \n",
    "subset_df = raw_df.sample(n=100, random_state=45)\n",
    "train_df, test_df = train_test_split(subset_df,test_size=0.5, random_state=45)\n",
    "\n",
    "# pre_proc_df is cleaned dataframe\n",
    "pre_proc_df = dfpp.preprocess_dataframe(train_df)\n",
    "print('\\n')\n",
    "print('--------------')\n",
    "print('Preprocessed Dataframe:', end='\\n')\n",
    "print(pre_proc_df.head())\n",
    "print(pre_proc_df.shape)\n",
    "\n",
    "# create subset for dev purposes\n",
    "to_nlp_df = pre_proc_df\n",
    "print('\\n')\n",
    "print('-' * 80)\n",
    "print('Subset Dataframe:', end='\\n')\n",
    "print(to_nlp_df.head())\n",
    "print(to_nlp_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "sklearn fit transform on ingredients:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:31<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Transformed Data:\n",
      "                          cup chop onion  cup olive oil  cup sour cream  \\\n",
      "id                                                                        \n",
      "54a4270b19925f464b37c1dc               0              1               0   \n",
      "54a42cde19925f464b3809d2               0              0               0   \n",
      "54a433036529d92b2c015de3               0              0               0   \n",
      "54a451926529d92b2c01eda8               0              0               0   \n",
      "54a430876529d92b2c013e2b               1              0               0   \n",
      "54a453df6529d92b2c020687               0              0               1   \n",
      "55b0e7116284773353bf4580               0              0               0   \n",
      "54a42bab6529d92b2c00ffa7               0              0               0   \n",
      "54a4748f19925f464b399ef2               0              1               0   \n",
      "54a4356a19925f464b3875bb               0              1               0   \n",
      "54a4697e6529d92b2c0279d3               0              0               0   \n",
      "54a45e426529d92b2c02488f               0              0               0   \n",
      "54a452c96529d92b2c01f889               0              0               0   \n",
      "54a4323619925f464b384bcc               0              0               0   \n",
      "54a4259119925f464b37af9c               0              0               0   \n",
      "54a431da6529d92b2c014ee9               0              1               0   \n",
      "54a426fd19925f464b37c125               0              0               0   \n",
      "54a47bb019925f464b39b9b7               0              0               0   \n",
      "54a434d819925f464b386e62               0              1               0   \n",
      "54a428116529d92b2c00d1a7               0              1               0   \n",
      "54a436036529d92b2c01859e               0              0               0   \n",
      "54a47edf19925f464b39c58d               0              0               0   \n",
      "54a419706529d92b2c006650               0              0               0   \n",
      "54a4349619925f464b386b12               0              0               0   \n",
      "54a4340f6529d92b2c016be8               0              0               0   \n",
      "54a40e546529d92b2c004606               0              0               0   \n",
      "54a428b419925f464b37d5ce               0              0               0   \n",
      "54a453a519925f464b38fd16               0              0               0   \n",
      "54a41cb219925f464b376d82               0              0               0   \n",
      "54a431896529d92b2c014b27               0              0               0   \n",
      "54a423ab19925f464b3799f2               0              0               0   \n",
      "54a47c1419925f464b39bb28               0              0               0   \n",
      "593ee3ba12c27b182380821f               0              0               0   \n",
      "54a456366529d92b2c02235a               1              0               0   \n",
      "54a452d419925f464b38f1b5               0              0               1   \n",
      "54a4659b6529d92b2c026a53               0              0               0   \n",
      "54a46d5d19925f464b3982d3               0              0               0   \n",
      "54a4582119925f464b3927a1               0              0               1   \n",
      "54a4205319925f464b377c9f               0              0               0   \n",
      "54a470cc19925f464b39906b               1              0               0   \n",
      "54a44f4a6529d92b2c01de45               0              0               0   \n",
      "592ef494ae10ad089795ebfa               0              0               0   \n",
      "54a41f016529d92b2c00757d               0              0               0   \n",
      "54a436266529d92b2c01876e               0              0               1   \n",
      "54a428bd19925f464b37d63e               0              0               0   \n",
      "54a45a4f6529d92b2c0234d1               0              0               0   \n",
      "54a41ed76529d92b2c007440               0              0               0   \n",
      "54a42c906529d92b2c010b74               0              0               0   \n",
      "569519a6dc18ea6c22c9b9ab               0              0               0   \n",
      "54a438d56529d92b2c019648               0              1               0   \n",
      "\n",
      "                          cup sugar  cup water  large egg  \\\n",
      "id                                                          \n",
      "54a4270b19925f464b37c1dc          1          1          0   \n",
      "54a42cde19925f464b3809d2          0          1          0   \n",
      "54a433036529d92b2c015de3          0          0          0   \n",
      "54a451926529d92b2c01eda8          0          0          0   \n",
      "54a430876529d92b2c013e2b          0          0          0   \n",
      "54a453df6529d92b2c020687          0          0          0   \n",
      "55b0e7116284773353bf4580          0          0          0   \n",
      "54a42bab6529d92b2c00ffa7          0          0          0   \n",
      "54a4748f19925f464b399ef2          0          0          0   \n",
      "54a4356a19925f464b3875bb          0          0          0   \n",
      "54a4697e6529d92b2c0279d3          0          0          0   \n",
      "54a45e426529d92b2c02488f          0          0          0   \n",
      "54a452c96529d92b2c01f889          0          0          0   \n",
      "54a4323619925f464b384bcc          0          0          0   \n",
      "54a4259119925f464b37af9c          0          0          0   \n",
      "54a431da6529d92b2c014ee9          0          0          0   \n",
      "54a426fd19925f464b37c125          0          0          0   \n",
      "54a47bb019925f464b39b9b7          0          0          0   \n",
      "54a434d819925f464b386e62          0          0          0   \n",
      "54a428116529d92b2c00d1a7          0          0          0   \n",
      "54a436036529d92b2c01859e          1          0          0   \n",
      "54a47edf19925f464b39c58d          1          0          0   \n",
      "54a419706529d92b2c006650          0          0          0   \n",
      "54a4349619925f464b386b12          0          0          1   \n",
      "54a4340f6529d92b2c016be8          0          0          0   \n",
      "54a40e546529d92b2c004606          0          0          0   \n",
      "54a428b419925f464b37d5ce          1          0          0   \n",
      "54a453a519925f464b38fd16          0          1          0   \n",
      "54a41cb219925f464b376d82          0          0          0   \n",
      "54a431896529d92b2c014b27          0          1          0   \n",
      "54a423ab19925f464b3799f2          0          0          0   \n",
      "54a47c1419925f464b39bb28          0          0          0   \n",
      "593ee3ba12c27b182380821f          0          0          0   \n",
      "54a456366529d92b2c02235a          0          0          0   \n",
      "54a452d419925f464b38f1b5          0          0          0   \n",
      "54a4659b6529d92b2c026a53          0          0          0   \n",
      "54a46d5d19925f464b3982d3          1          0          0   \n",
      "54a4582119925f464b3927a1          0          0          0   \n",
      "54a4205319925f464b377c9f          0          0          0   \n",
      "54a470cc19925f464b39906b          0          0          0   \n",
      "54a44f4a6529d92b2c01de45          0          0          0   \n",
      "592ef494ae10ad089795ebfa          0          0          0   \n",
      "54a41f016529d92b2c00757d          0          0          1   \n",
      "54a436266529d92b2c01876e          1          1          0   \n",
      "54a428bd19925f464b37d63e          0          0          0   \n",
      "54a45a4f6529d92b2c0234d1          0          0          0   \n",
      "54a41ed76529d92b2c007440          1          1          0   \n",
      "54a42c906529d92b2c010b74          0          0          1   \n",
      "569519a6dc18ea6c22c9b9ab          0          0          0   \n",
      "54a438d56529d92b2c019648          0          0          0   \n",
      "\n",
      "                          tablespoon extra-virgin olive oil  \\\n",
      "id                                                            \n",
      "54a4270b19925f464b37c1dc                                  0   \n",
      "54a42cde19925f464b3809d2                                  0   \n",
      "54a433036529d92b2c015de3                                  0   \n",
      "54a451926529d92b2c01eda8                                  0   \n",
      "54a430876529d92b2c013e2b                                  0   \n",
      "54a453df6529d92b2c020687                                  0   \n",
      "55b0e7116284773353bf4580                                  0   \n",
      "54a42bab6529d92b2c00ffa7                                  0   \n",
      "54a4748f19925f464b399ef2                                  0   \n",
      "54a4356a19925f464b3875bb                                  0   \n",
      "54a4697e6529d92b2c0279d3                                  0   \n",
      "54a45e426529d92b2c02488f                                  0   \n",
      "54a452c96529d92b2c01f889                                  0   \n",
      "54a4323619925f464b384bcc                                  0   \n",
      "54a4259119925f464b37af9c                                  1   \n",
      "54a431da6529d92b2c014ee9                                  0   \n",
      "54a426fd19925f464b37c125                                  0   \n",
      "54a47bb019925f464b39b9b7                                  0   \n",
      "54a434d819925f464b386e62                                  0   \n",
      "54a428116529d92b2c00d1a7                                  0   \n",
      "54a436036529d92b2c01859e                                  0   \n",
      "54a47edf19925f464b39c58d                                  0   \n",
      "54a419706529d92b2c006650                                  0   \n",
      "54a4349619925f464b386b12                                  0   \n",
      "54a4340f6529d92b2c016be8                                  0   \n",
      "54a40e546529d92b2c004606                                  1   \n",
      "54a428b419925f464b37d5ce                                  0   \n",
      "54a453a519925f464b38fd16                                  0   \n",
      "54a41cb219925f464b376d82                                  0   \n",
      "54a431896529d92b2c014b27                                  0   \n",
      "54a423ab19925f464b3799f2                                  0   \n",
      "54a47c1419925f464b39bb28                                  1   \n",
      "593ee3ba12c27b182380821f                                  0   \n",
      "54a456366529d92b2c02235a                                  0   \n",
      "54a452d419925f464b38f1b5                                  0   \n",
      "54a4659b6529d92b2c026a53                                  0   \n",
      "54a46d5d19925f464b3982d3                                  0   \n",
      "54a4582119925f464b3927a1                                  0   \n",
      "54a4205319925f464b377c9f                                  0   \n",
      "54a470cc19925f464b39906b                                  0   \n",
      "54a44f4a6529d92b2c01de45                                  0   \n",
      "592ef494ae10ad089795ebfa                                  0   \n",
      "54a41f016529d92b2c00757d                                  0   \n",
      "54a436266529d92b2c01876e                                  0   \n",
      "54a428bd19925f464b37d63e                                  0   \n",
      "54a45a4f6529d92b2c0234d1                                  0   \n",
      "54a41ed76529d92b2c007440                                  0   \n",
      "54a42c906529d92b2c010b74                                  0   \n",
      "569519a6dc18ea6c22c9b9ab                                  0   \n",
      "54a438d56529d92b2c019648                                  0   \n",
      "\n",
      "                          tablespoon fresh lemon juice  tablespoon olive oil  \\\n",
      "id                                                                             \n",
      "54a4270b19925f464b37c1dc                             0                     0   \n",
      "54a42cde19925f464b3809d2                             0                     0   \n",
      "54a433036529d92b2c015de3                             1                     1   \n",
      "54a451926529d92b2c01eda8                             0                     0   \n",
      "54a430876529d92b2c013e2b                             0                     1   \n",
      "54a453df6529d92b2c020687                             0                     0   \n",
      "55b0e7116284773353bf4580                             0                     0   \n",
      "54a42bab6529d92b2c00ffa7                             0                     0   \n",
      "54a4748f19925f464b399ef2                             0                     0   \n",
      "54a4356a19925f464b3875bb                             0                     0   \n",
      "54a4697e6529d92b2c0279d3                             0                     0   \n",
      "54a45e426529d92b2c02488f                             0                     0   \n",
      "54a452c96529d92b2c01f889                             0                     0   \n",
      "54a4323619925f464b384bcc                             0                     0   \n",
      "54a4259119925f464b37af9c                             1                     0   \n",
      "54a431da6529d92b2c014ee9                             0                     0   \n",
      "54a426fd19925f464b37c125                             0                     0   \n",
      "54a47bb019925f464b39b9b7                             0                     0   \n",
      "54a434d819925f464b386e62                             0                     0   \n",
      "54a428116529d92b2c00d1a7                             0                     0   \n",
      "54a436036529d92b2c01859e                             0                     0   \n",
      "54a47edf19925f464b39c58d                             0                     0   \n",
      "54a419706529d92b2c006650                             0                     0   \n",
      "54a4349619925f464b386b12                             0                     0   \n",
      "54a4340f6529d92b2c016be8                             0                     0   \n",
      "54a40e546529d92b2c004606                             0                     0   \n",
      "54a428b419925f464b37d5ce                             0                     0   \n",
      "54a453a519925f464b38fd16                             0                     0   \n",
      "54a41cb219925f464b376d82                             0                     0   \n",
      "54a431896529d92b2c014b27                             0                     0   \n",
      "54a423ab19925f464b3799f2                             0                     0   \n",
      "54a47c1419925f464b39bb28                             1                     0   \n",
      "593ee3ba12c27b182380821f                             1                     0   \n",
      "54a456366529d92b2c02235a                             0                     1   \n",
      "54a452d419925f464b38f1b5                             0                     0   \n",
      "54a4659b6529d92b2c026a53                             1                     1   \n",
      "54a46d5d19925f464b3982d3                             0                     0   \n",
      "54a4582119925f464b3927a1                             0                     0   \n",
      "54a4205319925f464b377c9f                             0                     0   \n",
      "54a470cc19925f464b39906b                             0                     1   \n",
      "54a44f4a6529d92b2c01de45                             0                     0   \n",
      "592ef494ae10ad089795ebfa                             1                     0   \n",
      "54a41f016529d92b2c00757d                             0                     0   \n",
      "54a436266529d92b2c01876e                             0                     1   \n",
      "54a428bd19925f464b37d63e                             0                     0   \n",
      "54a45a4f6529d92b2c0234d1                             0                     0   \n",
      "54a41ed76529d92b2c007440                             0                     0   \n",
      "54a42c906529d92b2c010b74                             0                     0   \n",
      "569519a6dc18ea6c22c9b9ab                             0                     0   \n",
      "54a438d56529d92b2c019648                             0                     0   \n",
      "\n",
      "                          tablespoon sugar  tablespoon white wine vinegar  \\\n",
      "id                                                                          \n",
      "54a4270b19925f464b37c1dc                 0                              0   \n",
      "54a42cde19925f464b3809d2                 0                              0   \n",
      "54a433036529d92b2c015de3                 0                              0   \n",
      "54a451926529d92b2c01eda8                 0                              0   \n",
      "54a430876529d92b2c013e2b                 0                              0   \n",
      "54a453df6529d92b2c020687                 0                              0   \n",
      "55b0e7116284773353bf4580                 0                              0   \n",
      "54a42bab6529d92b2c00ffa7                 0                              0   \n",
      "54a4748f19925f464b399ef2                 0                              0   \n",
      "54a4356a19925f464b3875bb                 0                              1   \n",
      "54a4697e6529d92b2c0279d3                 0                              0   \n",
      "54a45e426529d92b2c02488f                 0                              0   \n",
      "54a452c96529d92b2c01f889                 0                              0   \n",
      "54a4323619925f464b384bcc                 1                              0   \n",
      "54a4259119925f464b37af9c                 0                              0   \n",
      "54a431da6529d92b2c014ee9                 0                              0   \n",
      "54a426fd19925f464b37c125                 0                              0   \n",
      "54a47bb019925f464b39b9b7                 0                              0   \n",
      "54a434d819925f464b386e62                 0                              0   \n",
      "54a428116529d92b2c00d1a7                 0                              0   \n",
      "54a436036529d92b2c01859e                 0                              0   \n",
      "54a47edf19925f464b39c58d                 0                              0   \n",
      "54a419706529d92b2c006650                 0                              0   \n",
      "54a4349619925f464b386b12                 1                              0   \n",
      "54a4340f6529d92b2c016be8                 0                              0   \n",
      "54a40e546529d92b2c004606                 0                              0   \n",
      "54a428b419925f464b37d5ce                 0                              0   \n",
      "54a453a519925f464b38fd16                 0                              0   \n",
      "54a41cb219925f464b376d82                 0                              0   \n",
      "54a431896529d92b2c014b27                 0                              0   \n",
      "54a423ab19925f464b3799f2                 0                              0   \n",
      "54a47c1419925f464b39bb28                 0                              0   \n",
      "593ee3ba12c27b182380821f                 0                              1   \n",
      "54a456366529d92b2c02235a                 0                              0   \n",
      "54a452d419925f464b38f1b5                 0                              0   \n",
      "54a4659b6529d92b2c026a53                 0                              0   \n",
      "54a46d5d19925f464b3982d3                 0                              0   \n",
      "54a4582119925f464b3927a1                 0                              0   \n",
      "54a4205319925f464b377c9f                 0                              0   \n",
      "54a470cc19925f464b39906b                 0                              0   \n",
      "54a44f4a6529d92b2c01de45                 0                              0   \n",
      "592ef494ae10ad089795ebfa                 0                              1   \n",
      "54a41f016529d92b2c00757d                 0                              0   \n",
      "54a436266529d92b2c01876e                 0                              0   \n",
      "54a428bd19925f464b37d63e                 0                              0   \n",
      "54a45a4f6529d92b2c0234d1                 1                              0   \n",
      "54a41ed76529d92b2c007440                 0                              0   \n",
      "54a42c906529d92b2c010b74                 0                              0   \n",
      "569519a6dc18ea6c22c9b9ab                 0                              0   \n",
      "54a438d56529d92b2c019648                 0                              0   \n",
      "\n",
      "                          teaspoon ground cumin  teaspoon salt  \\\n",
      "id                                                               \n",
      "54a4270b19925f464b37c1dc                      0              0   \n",
      "54a42cde19925f464b3809d2                      0              0   \n",
      "54a433036529d92b2c015de3                      0              0   \n",
      "54a451926529d92b2c01eda8                      0              1   \n",
      "54a430876529d92b2c013e2b                      0              0   \n",
      "54a453df6529d92b2c020687                      0              1   \n",
      "55b0e7116284773353bf4580                      0              0   \n",
      "54a42bab6529d92b2c00ffa7                      0              0   \n",
      "54a4748f19925f464b399ef2                      1              0   \n",
      "54a4356a19925f464b3875bb                      0              0   \n",
      "54a4697e6529d92b2c0279d3                      0              0   \n",
      "54a45e426529d92b2c02488f                      0              0   \n",
      "54a452c96529d92b2c01f889                      0              0   \n",
      "54a4323619925f464b384bcc                      0              0   \n",
      "54a4259119925f464b37af9c                      0              0   \n",
      "54a431da6529d92b2c014ee9                      0              0   \n",
      "54a426fd19925f464b37c125                      0              0   \n",
      "54a47bb019925f464b39b9b7                      0              1   \n",
      "54a434d819925f464b386e62                      0              0   \n",
      "54a428116529d92b2c00d1a7                      0              0   \n",
      "54a436036529d92b2c01859e                      0              0   \n",
      "54a47edf19925f464b39c58d                      0              0   \n",
      "54a419706529d92b2c006650                      0              0   \n",
      "54a4349619925f464b386b12                      0              0   \n",
      "54a4340f6529d92b2c016be8                      0              0   \n",
      "54a40e546529d92b2c004606                      0              0   \n",
      "54a428b419925f464b37d5ce                      0              0   \n",
      "54a453a519925f464b38fd16                      0              1   \n",
      "54a41cb219925f464b376d82                      0              0   \n",
      "54a431896529d92b2c014b27                      0              0   \n",
      "54a423ab19925f464b3799f2                      0              0   \n",
      "54a47c1419925f464b39bb28                      1              0   \n",
      "593ee3ba12c27b182380821f                      0              0   \n",
      "54a456366529d92b2c02235a                      1              1   \n",
      "54a452d419925f464b38f1b5                      0              0   \n",
      "54a4659b6529d92b2c026a53                      0              0   \n",
      "54a46d5d19925f464b3982d3                      0              0   \n",
      "54a4582119925f464b3927a1                      0              0   \n",
      "54a4205319925f464b377c9f                      0              0   \n",
      "54a470cc19925f464b39906b                      0              0   \n",
      "54a44f4a6529d92b2c01de45                      0              0   \n",
      "592ef494ae10ad089795ebfa                      0              0   \n",
      "54a41f016529d92b2c00757d                      0              0   \n",
      "54a436266529d92b2c01876e                      0              1   \n",
      "54a428bd19925f464b37d63e                      0              0   \n",
      "54a45a4f6529d92b2c0234d1                      0              0   \n",
      "54a41ed76529d92b2c007440                      0              0   \n",
      "54a42c906529d92b2c010b74                      0              0   \n",
      "569519a6dc18ea6c22c9b9ab                      0              0   \n",
      "54a438d56529d92b2c019648                      0              0   \n",
      "\n",
      "                          teaspoon vanilla extract  \n",
      "id                                                  \n",
      "54a4270b19925f464b37c1dc                         0  \n",
      "54a42cde19925f464b3809d2                         0  \n",
      "54a433036529d92b2c015de3                         0  \n",
      "54a451926529d92b2c01eda8                         0  \n",
      "54a430876529d92b2c013e2b                         0  \n",
      "54a453df6529d92b2c020687                         0  \n",
      "55b0e7116284773353bf4580                         0  \n",
      "54a42bab6529d92b2c00ffa7                         0  \n",
      "54a4748f19925f464b399ef2                         0  \n",
      "54a4356a19925f464b3875bb                         0  \n",
      "54a4697e6529d92b2c0279d3                         0  \n",
      "54a45e426529d92b2c02488f                         0  \n",
      "54a452c96529d92b2c01f889                         0  \n",
      "54a4323619925f464b384bcc                         0  \n",
      "54a4259119925f464b37af9c                         0  \n",
      "54a431da6529d92b2c014ee9                         0  \n",
      "54a426fd19925f464b37c125                         0  \n",
      "54a47bb019925f464b39b9b7                         0  \n",
      "54a434d819925f464b386e62                         0  \n",
      "54a428116529d92b2c00d1a7                         0  \n",
      "54a436036529d92b2c01859e                         0  \n",
      "54a47edf19925f464b39c58d                         0  \n",
      "54a419706529d92b2c006650                         0  \n",
      "54a4349619925f464b386b12                         1  \n",
      "54a4340f6529d92b2c016be8                         0  \n",
      "54a40e546529d92b2c004606                         0  \n",
      "54a428b419925f464b37d5ce                         1  \n",
      "54a453a519925f464b38fd16                         0  \n",
      "54a41cb219925f464b376d82                         0  \n",
      "54a431896529d92b2c014b27                         0  \n",
      "54a423ab19925f464b3799f2                         0  \n",
      "54a47c1419925f464b39bb28                         0  \n",
      "593ee3ba12c27b182380821f                         0  \n",
      "54a456366529d92b2c02235a                         0  \n",
      "54a452d419925f464b38f1b5                         0  \n",
      "54a4659b6529d92b2c026a53                         0  \n",
      "54a46d5d19925f464b3982d3                         0  \n",
      "54a4582119925f464b3927a1                         0  \n",
      "54a4205319925f464b377c9f                         0  \n",
      "54a470cc19925f464b39906b                         0  \n",
      "54a44f4a6529d92b2c01de45                         0  \n",
      "592ef494ae10ad089795ebfa                         0  \n",
      "54a41f016529d92b2c00757d                         0  \n",
      "54a436266529d92b2c01876e                         0  \n",
      "54a428bd19925f464b37d63e                         0  \n",
      "54a45a4f6529d92b2c0234d1                         0  \n",
      "54a41ed76529d92b2c007440                         0  \n",
      "54a42c906529d92b2c010b74                         1  \n",
      "569519a6dc18ea6c22c9b9ab                         0  \n",
      "54a438d56529d92b2c019648                         0  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba63328c5fa49848c36e6d7a39bb466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9510becb278a4284b8e408890b8e6d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe9debcba094fdbb8d74224a7b5b592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/07 23:47:12 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp8u_il0xt/model, flavor: python_function), fall back to return ['cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.\n"
     ]
    }
   ],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_transformer_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': CustomSKLearnAnalyzer().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':3,\n",
    "    'binary':True\n",
    "}\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "# bertopic_params = {\n",
    "#     'top_n_words':20,\n",
    "#     'min_topic_size':5,\n",
    "#     'nr_topics':'auto',\n",
    "#     'verbose':True,\n",
    "#     'low_memory':True,\n",
    "#     'calculate_probabilities':True\n",
    "# }\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'OneHotEncoder'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_transformer_params)\n",
    "# pipeline_params.update(bertopic_params)\n",
    "\n",
    "signature = infer_signature(to_nlp_df['ingredients'][0])\n",
    "\n",
    "with mlflow.start_run(experiment_id=mlflow_exp_id):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    # Will be useful in STAGING/Evaluation\n",
    "    \n",
    "    # LOG MODEL\n",
    "    # Instantiate sklearn OneHotEncoder\n",
    "    sklearn_transformer = CountVectorizer(**sklearn_transformer_params)\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('sklearn fit transform on ingredients:', end='\\n')\n",
    "\n",
    "    # Do fit transform on data\n",
    "    response = sklearn_transformer.fit_transform(tqdm(to_nlp_df['ingredients'])) \n",
    "    \n",
    "    transformed_recipe = pd.DataFrame(\n",
    "            response.toarray(),\n",
    "            columns=sklearn_transformer.get_feature_names_out(),\n",
    "            index=to_nlp_df.index\n",
    "    )\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('Transformed Data:', end='\\n')\n",
    "    print(transformed_recipe)\n",
    "    \n",
    "    # mlflow.pyfunc.save_model(\n",
    "    #     path=model_directory,\n",
    "    #     code_path=[\"../src/\"],\n",
    "    #     python_model=CustomSKLearnWrapper(),\n",
    "    #     input_example=to_nlp_df['ingredients'][0],    \n",
    "    #     artifacts=artifacts\n",
    "    # )\n",
    "\n",
    "     # joblib.dump(sklearn_transformer, sklearn_transformer_path)\n",
    "    with open(sklearn_transformer_path, \"wb\") as fo:\n",
    "        pickle.dump(sklearn_transformer, fo)\n",
    "        # mlflow.log_artifact(sklearn_transformer_path,\n",
    "        #                     artifact_path='sklearn_transformer')\n",
    "\n",
    "    # joblib.dump(transformed_recipe, transformed_recipes_path)\n",
    "    with open(transformed_recipes_path, \"wb\") as fo:\n",
    "        pickle.dump(transformed_recipe, fo)\n",
    "        # mlflow.log_artifact(transformed_recipes_path,\n",
    "        #                     artifact_path='transformed_recipes')\n",
    "\n",
    "\n",
    "    model_info = mlflow.pyfunc.log_model( \n",
    "        code_path=[\"../src/\"],\n",
    "        python_model=CustomSKLearnWrapper(),\n",
    "        input_example=to_nlp_df['ingredients'][0],\n",
    "        signature=signature,        \n",
    "        artifact_path=\"sklearn_model\",\n",
    "        artifacts=artifacts\n",
    "        ) \n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50x14 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 63 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa88371d6bc4fc6863bddaabbf9f927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/08 09:26:04 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n"
     ]
    }
   ],
   "source": [
    "test_predictor = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------\n",
      "Preprocessed Dataframe:\n",
      "                                                                        dek  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395             Can be prepared in 45 minutes or less.   \n",
      "54a4630719925f464b395a0c             Can be prepared in 45 minutes or less.   \n",
      "54a4246f6529d92b2c00a770                                                      \n",
      "54a469726529d92b2c0279a4  Easy and colorful, this side dish is nice with...   \n",
      "54a4345019925f464b386748  Goes great with: Couscous flavored with choppe...   \n",
      "\n",
      "                                                                        hed  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395  Braised Chicken with Mushrooms and Sun-Dried T...   \n",
      "54a4630719925f464b395a0c                         Radishes with Chive Butter   \n",
      "54a4246f6529d92b2c00a770                              Fried Blackberry Pies   \n",
      "54a469726529d92b2c0279a4                         Baby Carrots with Tarragon   \n",
      "54a4345019925f464b386748                          Moroccan Slow-Cooked Lamb   \n",
      "\n",
      "                          aggregateRating  \\\n",
      "id                                          \n",
      "54a418b26529d92b2c006395             3.32   \n",
      "54a4630719925f464b395a0c             2.00   \n",
      "54a4246f6529d92b2c00a770             0.00   \n",
      "54a469726529d92b2c0279a4             3.26   \n",
      "54a4345019925f464b386748             3.80   \n",
      "\n",
      "                                                                ingredients  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395  [1/3 cup thinly sliced drained sun-dried tomat...   \n",
      "54a4630719925f464b395a0c  [3/4 stick (6 tablespoons) unsalted butter, so...   \n",
      "54a4246f6529d92b2c00a770  [3 cups all purpose flour, 2 1/2 tablespoons s...   \n",
      "54a469726529d92b2c0279a4  [4 bunches baby carrots (each about 8 ounces),...   \n",
      "54a4345019925f464b386748  [1 tablespoon ground cumin, 2 teaspoons ground...   \n",
      "\n",
      "                                                                  prepSteps  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395  [In a heavy skillet heat the reserved tomato o...   \n",
      "54a4630719925f464b395a0c  [In a small bowl with a fork combine well the ...   \n",
      "54a4246f6529d92b2c00a770  [Whisk flour, sugar, and salt in medium bowl. ...   \n",
      "54a469726529d92b2c0279a4  [Combine carrots, 1/4 cup water, 1 1/2 tablesp...   \n",
      "54a4345019925f464b386748  [Mix first 6 ingredients in large bowl. Add la...   \n",
      "\n",
      "                          reviewsCount  willMakeAgainPct     cuisine_name  \\\n",
      "id                                                                          \n",
      "54a418b26529d92b2c006395            61                92  Missing Cuisine   \n",
      "54a4630719925f464b395a0c             4                50  Missing Cuisine   \n",
      "54a4246f6529d92b2c00a770             1                 0  Missing Cuisine   \n",
      "54a469726529d92b2c0279a4            16                78  Missing Cuisine   \n",
      "54a4345019925f464b386748           182                96          African   \n",
      "\n",
      "                                               photo_filename  \\\n",
      "id                                                              \n",
      "54a418b26529d92b2c006395  EP_12162015_placeholders_formal.jpg   \n",
      "54a4630719925f464b395a0c  EP_12162015_placeholders_formal.jpg   \n",
      "54a4246f6529d92b2c00a770  EP_12162015_placeholders_formal.jpg   \n",
      "54a469726529d92b2c0279a4  EP_12162015_placeholders_formal.jpg   \n",
      "54a4345019925f464b386748                           231597.jpg   \n",
      "\n",
      "                                                               photo_credit  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a4630719925f464b395a0c  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a4246f6529d92b2c00a770  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a469726529d92b2c0279a4  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a4345019925f464b386748                                      Brian Leatart   \n",
      "\n",
      "                                  author_name            date_published  \\\n",
      "id                                                                        \n",
      "54a418b26529d92b2c006395  Missing Author Name 2004-08-20 04:00:00+00:00   \n",
      "54a4630719925f464b395a0c  Missing Author Name 2004-08-20 04:00:00+00:00   \n",
      "54a4246f6529d92b2c00a770  Missing Author Name 2010-05-14 04:00:00+00:00   \n",
      "54a469726529d92b2c0279a4  Missing Author Name 2004-08-20 04:00:00+00:00   \n",
      "54a4345019925f464b386748  Missing Author Name 2005-01-28 21:19:07+00:00   \n",
      "\n",
      "                                                                 recipe_url  \n",
      "id                                                                           \n",
      "54a418b26529d92b2c006395  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a4630719925f464b395a0c  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a4246f6529d92b2c00a770  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a469726529d92b2c0279a4  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a4345019925f464b386748  https://www.epicurious.com/recipes/food/views/...  \n",
      "(49, 13)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Subset Dataframe:\n",
      "                                                                        dek  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395             Can be prepared in 45 minutes or less.   \n",
      "54a4630719925f464b395a0c             Can be prepared in 45 minutes or less.   \n",
      "54a4246f6529d92b2c00a770                                                      \n",
      "54a469726529d92b2c0279a4  Easy and colorful, this side dish is nice with...   \n",
      "54a4345019925f464b386748  Goes great with: Couscous flavored with choppe...   \n",
      "\n",
      "                                                                        hed  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395  Braised Chicken with Mushrooms and Sun-Dried T...   \n",
      "54a4630719925f464b395a0c                         Radishes with Chive Butter   \n",
      "54a4246f6529d92b2c00a770                              Fried Blackberry Pies   \n",
      "54a469726529d92b2c0279a4                         Baby Carrots with Tarragon   \n",
      "54a4345019925f464b386748                          Moroccan Slow-Cooked Lamb   \n",
      "\n",
      "                          aggregateRating  \\\n",
      "id                                          \n",
      "54a418b26529d92b2c006395             3.32   \n",
      "54a4630719925f464b395a0c             2.00   \n",
      "54a4246f6529d92b2c00a770             0.00   \n",
      "54a469726529d92b2c0279a4             3.26   \n",
      "54a4345019925f464b386748             3.80   \n",
      "\n",
      "                                                                ingredients  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395  [1/3 cup thinly sliced drained sun-dried tomat...   \n",
      "54a4630719925f464b395a0c  [3/4 stick (6 tablespoons) unsalted butter, so...   \n",
      "54a4246f6529d92b2c00a770  [3 cups all purpose flour, 2 1/2 tablespoons s...   \n",
      "54a469726529d92b2c0279a4  [4 bunches baby carrots (each about 8 ounces),...   \n",
      "54a4345019925f464b386748  [1 tablespoon ground cumin, 2 teaspoons ground...   \n",
      "\n",
      "                                                                  prepSteps  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395  [In a heavy skillet heat the reserved tomato o...   \n",
      "54a4630719925f464b395a0c  [In a small bowl with a fork combine well the ...   \n",
      "54a4246f6529d92b2c00a770  [Whisk flour, sugar, and salt in medium bowl. ...   \n",
      "54a469726529d92b2c0279a4  [Combine carrots, 1/4 cup water, 1 1/2 tablesp...   \n",
      "54a4345019925f464b386748  [Mix first 6 ingredients in large bowl. Add la...   \n",
      "\n",
      "                          reviewsCount  willMakeAgainPct     cuisine_name  \\\n",
      "id                                                                          \n",
      "54a418b26529d92b2c006395            61                92  Missing Cuisine   \n",
      "54a4630719925f464b395a0c             4                50  Missing Cuisine   \n",
      "54a4246f6529d92b2c00a770             1                 0  Missing Cuisine   \n",
      "54a469726529d92b2c0279a4            16                78  Missing Cuisine   \n",
      "54a4345019925f464b386748           182                96          African   \n",
      "\n",
      "                                               photo_filename  \\\n",
      "id                                                              \n",
      "54a418b26529d92b2c006395  EP_12162015_placeholders_formal.jpg   \n",
      "54a4630719925f464b395a0c  EP_12162015_placeholders_formal.jpg   \n",
      "54a4246f6529d92b2c00a770  EP_12162015_placeholders_formal.jpg   \n",
      "54a469726529d92b2c0279a4  EP_12162015_placeholders_formal.jpg   \n",
      "54a4345019925f464b386748                           231597.jpg   \n",
      "\n",
      "                                                               photo_credit  \\\n",
      "id                                                                            \n",
      "54a418b26529d92b2c006395  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a4630719925f464b395a0c  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a4246f6529d92b2c00a770  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a469726529d92b2c0279a4  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a4345019925f464b386748                                      Brian Leatart   \n",
      "\n",
      "                                  author_name            date_published  \\\n",
      "id                                                                        \n",
      "54a418b26529d92b2c006395  Missing Author Name 2004-08-20 04:00:00+00:00   \n",
      "54a4630719925f464b395a0c  Missing Author Name 2004-08-20 04:00:00+00:00   \n",
      "54a4246f6529d92b2c00a770  Missing Author Name 2010-05-14 04:00:00+00:00   \n",
      "54a469726529d92b2c0279a4  Missing Author Name 2004-08-20 04:00:00+00:00   \n",
      "54a4345019925f464b386748  Missing Author Name 2005-01-28 21:19:07+00:00   \n",
      "\n",
      "                                                                 recipe_url  \n",
      "id                                                                           \n",
      "54a418b26529d92b2c006395  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a4630719925f464b395a0c  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a4246f6529d92b2c00a770  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a469726529d92b2c0279a4  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a4345019925f464b386748  https://www.epicurious.com/recipes/food/views/...  \n",
      "(49, 13)\n"
     ]
    }
   ],
   "source": [
    "# pre_proc_df is cleaned dataframe\n",
    "pre_proc_test_df = dfpp.preprocess_dataframe(test_df)\n",
    "print('\\n')\n",
    "print('--------------')\n",
    "print('Preprocessed Dataframe:', end='\\n')\n",
    "print(pre_proc_test_df.head())\n",
    "print(pre_proc_test_df.shape)\n",
    "\n",
    "# create subset for dev purposes\n",
    "to_nlp_test_df = pre_proc_test_df\n",
    "print('\\n')\n",
    "print('-' * 80)\n",
    "print('Subset Dataframe:', end='\\n')\n",
    "print(to_nlp_test_df.head())\n",
    "print(to_nlp_test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1, 14), indices imply (49, 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_nlp_test_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mingredients\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py:491\u001b[0m, in \u001b[0;36mPyFuncModel.predict\u001b[0;34m(self, data, params)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _MLFLOW_TESTING\u001b[38;5;241m.\u001b[39mget():\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py:477\u001b[0m, in \u001b[0;36mPyFuncModel.predict.<locals>._predict\u001b[0;34m()\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m():\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Models saved prior to MLflow 2.5.0 do not support `params` in the pyfunc `predict()`\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# function definition, nor do they support `**kwargs`. Accordingly, we only pass\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# `params` to the `predict()` method if it defines the `params` argument\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_fn)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 477\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m     _log_warning_if_params_not_in_predict_signature(_logger, params)\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_fn(data)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/mlflow/pyfunc/model.py:469\u001b[0m, in \u001b[0;36m_PythonModelPyfuncWrapper.predict\u001b[0;34m(self, model_input, params)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m:param model_input: Model input data.\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m:param params: Additional parameters to pass to the model for inference.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m:return: Model predictions.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_model\u001b[38;5;241m.\u001b[39mpredict)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m _log_warning_if_params_not_in_predict_signature(_logger, params)\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_model\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(model_input))\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/src/custom_stanza_mlflow.py:76\u001b[0m, in \u001b[0;36mCustomSKLearnWrapper.predict\u001b[0;34m(self, context, model_input, params)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03mThis method is needed to override the default predict.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mIt needs to function essentially as a wrapper and returns back the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    the sklearn/Stanza text processing\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msklearn_transformer\u001b[38;5;241m.\u001b[39mtransform(model_input)\n\u001b[0;32m---> 76\u001b[0m transformed_recipe \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msklearn_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transformed_recipe\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/frame.py:722\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    712\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    713\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    714\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    719\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    720\u001b[0m         )\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:349\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    345\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[1;32m    346\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[1;32m    347\u001b[0m )\n\u001b[0;32m--> 349\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1, 14), indices imply (49, 14)"
     ]
    }
   ],
   "source": [
    "test_predictor.predict(to_nlp_test_df['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['54a418b26529d92b2c006395', '54a4630719925f464b395a0c',\n",
       "       '54a4246f6529d92b2c00a770', '54a469726529d92b2c0279a4',\n",
       "       '54a4345019925f464b386748', '54a4094019925f464b37361b',\n",
       "       '54a4369c19925f464b388371', '54a44bf019925f464b38c15c',\n",
       "       '54a4566419925f464b391f5e', '54a44f5619925f464b38d6d9',\n",
       "       '54a408dc6529d92b2c003702', '54a44ba019925f464b38be1f',\n",
       "       '54a440dc19925f464b38ad84', '54a423a86529d92b2c009d6c',\n",
       "       '54a427b56529d92b2c00ccd0', '54a44a2719925f464b38b03f',\n",
       "       '54a4538619925f464b38fb70', '54a4095819925f464b373662',\n",
       "       '54a42acc6529d92b2c00f5cf', '54a46cc719925f464b398085',\n",
       "       '54a435da19925f464b387b6b', '54a4264d19925f464b37b80f',\n",
       "       '54a465176529d92b2c026847', '54a429a36529d92b2c00e659',\n",
       "       '54a43cb06529d92b2c01aa4b', '58336c799db780ce306c9fed',\n",
       "       '54a4233f19925f464b379582', '54a4323c19925f464b384c12',\n",
       "       '54a40db819925f464b374260', '54a4553019925f464b3910ed',\n",
       "       '54a45ebb19925f464b39455d', '54a47f6319925f464b39c799',\n",
       "       '54a436486529d92b2c018902', '54a425f219925f464b37b3e5',\n",
       "       '56c4ea8d301378bd5bee6042', '54a462bb19925f464b3958d4',\n",
       "       '54a4704a6529d92b2c029543', '54a424856529d92b2c00a8a1',\n",
       "       '54a47bd56529d92b2c02c185', '54a45ba56529d92b2c023d64',\n",
       "       '54a450586529d92b2c01e575', '578f82f9bf39bbd23d3047cc',\n",
       "       '54a42ce06529d92b2c010f7d', '54a456c96529d92b2c02290a',\n",
       "       '54a46ae36529d92b2c027f76', '54a440cb19925f464b38ad36',\n",
       "       '54a4406d6529d92b2c01b3e1', '54a437ff6529d92b2c0191e0',\n",
       "       '54a416596529d92b2c005df3'],\n",
       "      dtype='object', name='id')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_nlp_test_df['ingredients'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['1/3 cup thinly sliced drained sun-dried tomatoes packed in oil, reserving 1 1/2 tablespoons of the oil', '1 large whole chicken breast with skin and bones (about 1 1/4 pounds), halved', '1 small onion, chopped fine', '2 large garlic cloves, minced', '1/2 teaspoon dried basil, crumbled', '1/4 teaspoon dried hot red pepper flakes, or to taste', '1/2 pound mushrooms, sliced', '1/2 cup dry red wine', '1/2 cup chicken broth', '2 tablespoons tomato paste', 'a beurre manié made by kneading together 1 1/2 teaspoons softened unsalted butter and 1 1/2 teaspoons minced all-purpose flour', '', '', '3 tablespoons minced fresh parsley leaves (preferably flat-leafed)']),\n",
       "       list(['3/4 stick (6 tablespoons) unsalted butter, softened', '2 tablespoons cream cheese, softened', '1 tablespoon minced fresh chives', '1/4 teaspoon freshly grated lemon zest', 'Tabasco to taste', '18 radishes (about 2 bunches), the leaves trimmed, leaving 1 inch of the stem, and the radishes halved lengthwise', '36 fresh flat-leafed parsley leaves for garnish']),\n",
       "       list(['3 cups all purpose flour', '2 1/2 tablespoons sugar', '3/4 teaspoon salt', '1 cup (scant) chilled solid vegetable shortening (preferably non-hydrogenated), cut into 1/2-inch cubes', '6 tablespoons (or more) ice water', '1 12-ounce bag frozen blackberries (3 cups; do not thaw)', '1 cup sugar', '1 teaspoon vanilla extract', '1/4 teaspoon ground cinnamon', 'Pinch of salt', '1 egg beaten with 1 tablespoon water (for glaze)', 'Vegetable oil (for frying)', 'Powdered sugar', 'Vanilla ice cream']),\n",
       "       list(['4 bunches baby carrots (each about 8 ounces), peeled, trimmed 3 inches of stems left intact', '1/4 cup water', '3 tablespoons minced fresh tarragon or 3 teaspoons dried', '2 tablespoons (1/4 stick) butter', '1 tablespoon white wine vinegar', '1 tablespoon honey']),\n",
       "       list(['1 tablespoon ground cumin', '2 teaspoons ground coriander', '1 1/2 teaspoons salt', '1 teaspoon fennel seeds', '1/2 teaspoon cayenne pepper', '1/2 teaspoon ground black pepper', '2 1/2 pounds trimmed boned lamb shoulder, cut into 1 1/2- to 2-inch pieces', '4 tablespoons olive oil, divided', '1 large onion, finely chopped', '1 tablespoon tomato paste', '2 cups low-salt chicken broth', '1 15 1/2-ounce can garbanzo beans (chickpeas), drained', '1 cup dried apricots (about 5 ounces)', '2 large plum tomatoes, chopped', '2 cinnamon sticks', '1 tablespoon minced peeled fresh ginger', '2 teaspoons (packed) grated lemon peel', '2 tablespoons chopped fresh cilantro']),\n",
       "       list(['2 anchovy fillets, finely chopped', '3 garlic cloves, minced', '1/4 to 1/2 teaspoon hot red-pepper flakes', '3 tablespoons extra-virgin olive oil', '2 plum tomatoes, coarsely chopped', '1/3 cup pitted Kalamata olives, coarsely chopped', '1 tablespoon drained capers, chopped', '1 (19-ounce) can cannellini beans, rinsed and drained', '1/4 cup chopped basil']),\n",
       "       list(['1 (5-pound) ripe honeydew melon, seeded and cut into 1-inch chunks (8 cups)', '1 (3/4- to 1-pound) seedless cucumber (usually plastic-wrapped), peeled and coarsely chopped', '1/2 cup fresh lime juice', '1 cup ice cubes', '2/3 cup coconut sorbet', '1/4 cup fresh mint leaves']),\n",
       "       list(['2 large egg yolks', '4 tablespoons water', '4 (17 1/4-oz) packages frozen puff pastry sheets, thawed but kept chilled', '2 (8-oz) packages cream cheese', '1/2 cup granulated sugar', '1/2 cup chopped crystallized ginger (4 oz)', '2 pt strawberries (1 lb), hulled, then halved or quartered if large', '3/4 cup red currant jelly', '4 pt blueberries (2 1/2 lb)', 'Garnish: confectioners sugar', 'Special Equipment: parchment paper and a large flat board (at least 20 inches square) for serving', '']),\n",
       "       list(['1/2 teaspoon finely grated fresh orange zest', '3/4 cup fresh orange juice', '1/2 teaspoon fresh lemon juice', '1/4 teaspoon vanilla', '6 tablespoons mint sugar', '12 large firm-ripe figs (about 1 1/2 pounds)', '6 ripe apricots (about 10 ounces)', '1 1/4 cups raspberries (about 1 1/2 half-pints)', '6 tablespoons turbinado sugar, such as Sugar in the Raw, or packed light brown sugar', 'Accompaniment: crème fraîche', 'Special equipment: blowtorch']),\n",
       "       list(['1/2 cup semolina (sometimes labeled \"semolina flour\"; resembles fine yellow cornmeal)*', '2 cups fat-free milk', '3/4 teaspoon salt', '1 tablespoon unsalted butter, melted', '1 oz finely grated Parmigiano-Reggiano (preferably grated with a rasp; about 1 1/4 cups)']),\n",
       "       list(['1 1/2 pounds rhubarb stalks, trimmed and cut into 1-inch pieces', \"2 tablespoons confectioners' sugar\", '1 (10-ounce) package thawed frozen \"lite\" strawberries in syrup', '2 tablespoons light corn syrup', '1 1/2 pints low-fat superpremium vanilla ice cream, softened']),\n",
       "       list(['1 1/2 cups crushed plain cornflakes', 'Salt, pepper and paprika, to taste', '', '', '1 egg', '1/4 cup skim milk', '4 skinless and boneless chicken breast halves (about 4 ounces each)', '4 cups flat-leaf parsley leaves', '2 shallots, thinly sliced lengthwise', '2 tablespoons drained tiny capers', '2 tablespoons olive oil', 'Juice of 1 lemon', '1 lemon, quartered, for garnish']),\n",
       "       list(['1/2 cup mayonnaise', '2 tablespoons fresh Meyer lemon juice', '1 1/2 teaspoons finely grated Meyer lemon peel', '1 1/2 teaspoons Dijon mustard', '1 teaspoon Sherry wine vinegar', '1/4 cup extra-virgin olive oil', '2 large eggs, separated', '1 tablespoon crème fraîche or sour cream', '1 cup (packed) fresh crabmeat, picked over, broken into 1/2-inch pieces (about 6 ounces)', '2 tablespoons chopped fresh chives', '1 tablespoon finely chopped shallot', '1 tablespoon chopped fresh Italian parsley', '4 medium-size fresh shiitake mushrooms, stemmed, caps thinly sliced', '1 tablespoon butter', '2 tablespoons extra-virgin olive oil', '1 tablespoon fresh Meyer lemon juice', '1/2 cup (packed) small fresh basil leaves', '1/2 cup (packed) fresh Italian parsley leaves']),\n",
       "       list(['1/2 cup Sugar in the Raw', '1/2 cup kosher salt', '3 tablespoons chili powder', '3 tablespoons paprika', '1 teaspoon garlic powder', '1 teaspoon onion powder', '1/2 teaspoon black pepper', '1/2 teaspoon lemon pepper', '1/2 teaspoon ground coffee', '1/4 teaspoon cayenne pepper']),\n",
       "       list(['1 cup sugar', '2 teaspoons ground cinnamon', '2 tablespoons (1/4 stick) unsalted butter', '1 pound Pink Lady apples (about 3), peeled, cored, cut into 1/3- to 1/2-inch cubes (about 3 cups)', '2 tablespoons sugar', '1/4 teaspoon ground cinnamon', '1/2 cup sparkling apple cider', '1 tablespoon apple cider vinegar', '1 3/4 cups all purpose flour', '1/2 cup sugar', '1 teaspoon finely grated lemon peel', '1 teaspoon baking powder', '1 teaspoon ground cinnamon', '1/4 teaspoon ground nutmeg', '2/3 cup buttermilk', '2 large eggs, separated', '2 1/2 tablespoons unsalted butter, melted', '1/4 teaspoon coarse kosher salt', 'Safflower oil (for deep-frying)', 'Bourbon Ice Cream', 'Special equipment: Deep-fry equipment']),\n",
       "       list(['4 Idaho potatoes, peeled and cut into 1/2-inch slices', '2 tablespoons vegetable oil', '1 (5- to 6-pound) first-cut or flat-cut brisket', '2 large onions, peeled and cut into 1/4-inch slices', '4 garlic cloves, peeled and minced', '1 (12-ounce) can beer', '1/2 cup low-sodium beef broth', '4 celery stalks, cut into 1/2-inch slices', '1/2 cup tomato paste', '1 (14 1/2-ounce) can stewed tomatoes', '2 bay leaves', '1/2 cup packed dark brown sugar', '1/3 cup Dijon mustard', '1/2 cup red wine vinegar', '1/4 cup regular molasses', '1/3 cup soy sauce', '1 teaspoon paprika', 'Salt and freshly ground black pepper to taste', 'N/A freshly ground black pepper']),\n",
       "       list(['10 ounces pearl onions', '4 1/2 pounds veal shoulder, boned, trimmed, cut into 1-inch pieces', '9 cups (or more) chicken stock or canned low-salt chicken broth', '3 fresh thyme sprigs', '2 bay leaves', '5 tablespoons butter', '1 1/2 pounds celery root (celeriac), peeled, cut into 1 1/2-inch pieces', '4 large carrots, peeled, cut into 1 1/2-inch lengths', '3 medium turnips, peeled, each cut into 6 pieces', '8 ounces button mushrooms', '6 ounces haricots verts or other green beans, ends trimmed', '3 tablespoons all purpose flour', '1/2 cup whipping cream', '1/2 tablespoon (about) fresh lemon juice', '1/2 bunch fresh chives, cut into 2-inch pieces (optional)']),\n",
       "       list(['2 3/4 cups water', '12 dried Chinese black mushrooms', '1/2 teaspoon white pepper', '2 teaspoons kosher salt, divided', '1 tablespoon soy sauce', '2 teaspoon sugar', '1 cup sliced peeled ginger', '1/4 cup Shaoxing wine or dry Oloroso Sherry', '1/4 cup dry white wine', '2 small chickens (2 1/2 to 3 pounds), each cut into 6 pieces (legs, thighs, and breasts without backbones)', '2 3/4 cups water', '2 tablespoons corn oil', '1 tablespoon plus 2 teaspoon kosher salt, divided', '1/4 cup finely julienned peeled ginger', '1/2 cup dry white wine', '3/4 cup Shaoxing wine or dry Oloroso Sherry', '3/4 cup dry Oloroso Sherry', '2 1/2 cups reduced-sodium chicken broth', '1 1/2 cups plus 2 tablespoon cold water, divided', '3 1/2 teaspoons sugar', '2 tablespoons soy sauce', '1 teaspoon white pepper', '2 tablespoons cornstarch', '7 to 8 ounces bottled roasted chestnuts', '1/4 pound thinly sliced country ham or speck', 'Accompaniment: steamed jasmine rice']),\n",
       "       list(['3 tablespoons olive oil', '2 stalks celery, cut into 1/2-inch slices', '8 ounces unpeeled small red potatoes, cut into 1-inch cubes', '1 teaspoon kosher salt', '1 1/4 teaspoons freshly ground white pepper', '1 bunch green onions, coarsely chopped, including half of greens', '1 1/2 cups corn kernels (about 2 ears)', '4 cups fish or chicken stock', '1 cup heavy cream', '2 pounds salmon fillets, skin and pin bones removed, cut into 2-inch pieces', 'Grated zest and juice of 1 lemon', '1/4 cup minced fresh dill']),\n",
       "       list(['1/4 cup fresh lemon juice', '2 large garlic cloves, coarsely chopped', '1/2 cup olive oil', '1/2 cup fresh grated Parmesan cheese (about 1 1/2 ounces)', '6 3/4-inch-thick slices sourdough flute and baguette', '1/4 cup olivada* or prepared tapenade spread', '2 heads Belgian endive, coarsely chopped', '1 head curly endive, center leaves only, torn into pieces', '1/2 cup pine nuts (about 2 ounces), toasted', '*An olive spread available at Italian markets, specialty foods stores and some supermarkets. If unavailable, puree pitted black, brine-cured olives.']),\n",
       "       list(['15 dried corn husks (2 oz, or 1/4 package)', '2 cups masa harina for tamales (see Tips, below)', '1/4 cup sugar', '1/4 vanilla bean, or 3/4 teaspoon pure nonalcoholic vanilla flavoring (see Tips, below) or vanilla extract', '1 cup unsweetened shredded coconut', '1/3 cup vegetable shortening or butter, softened', '1/4 teaspoon baking powder (optional, see Tips, below)', '1/3 cup raisins']),\n",
       "       list(['2 pounds medium boiling potatoes, peeled and cut into 1-inch-thick wedges', '3 tablespoons olive oil', '5 to 6 large rosemary sprigs (about 5 inches long)', '4 large garlic cloves, smashed and peeled']),\n",
       "       list(['3/4 pound (about 2 cups) canned or vacuum-packed whole chestnuts, rinsed, drained \\xa0well, and patted dry if using canned', '1 stick (1/2 cup) unsalted butter, softened', '4 tablespoons dark rum', '10 ounces fine-quality bittersweet chocolate, chopped and melted', '6 large eggs, separated', '1/4 teaspoon salt', '1/2 cup sugar', '6 ounces fine-quality bittersweet chocolate, chopped fine', '1/2 cup heavy cream', '1 tablespoon dark rum', '8 marrons glacés (candied chestnuts, available at specialty foods shops)', '1 cup well-chilled heavy cream', '1 tablespoon sugar, or to taste', '1 tablespoon dark rum if desired', '3/4 cup chopped marrons glacés']),\n",
       "       list(['1 teaspoon cornstarch', '2 pinches of salt', '2 cups half-and-half or 1 cup whole milk plus 1 cup heavy cream', '6 ounces bittersweet (60% cacao) chocolate, broken into small pieces or chopped', '2 tablespoons unsweetened cocoa powder', '1/2 teaspoon vanilla extract']),\n",
       "       list(['1 small fennel bulb (sometimes called anise) with fronds', '1 (4- to 5-inch-long) fresh red chile, chopped, including seeds', '2 scallions, finely chopped', '1/3 cup extra-virgin olive oil', '2/3 cup Chardonnay or other dry white wine', '8 (1/4-lb) pieces monkfish fillet (about 1 1/2 inches thick), trimmed of all membranes', '1/4 lb feta (preferably Greek), crumbled']),\n",
       "       list(['Kosher salt', '1 cup sorghum', '1/2 large butternut squash, peeled, cut into 1-inch pieces (about 2 cups)', '6 tablespoons unsalted butter, cut into small pieces, divided', '4 bone-in chicken legs (thigh and drumstick; about 3 pounds total)', '1 tablespoon olive oil', 'Freshly ground black pepper', '1/2 cup low-sodium chicken broth', '1 tablespoon pure maple syrup', '1 tablespoon (or more) sherry vinegar', '2 tablespoons fresh lemon juice', '1/2 cup coarsely chopped parsley', '1/2 cup pomegranate seeds']),\n",
       "       list(['1 bulb fennel', '1 tsp/5 ml olive oil', 'Salt and pepper', '', '2 large eggs', '4 (4\"/10 cm) pieces of baguette, sliced in half', '4 tsp/20 g tapenade', '2 cups/40 g mesclun', '4 thick slices beefsteak tomato', '6 oz/168 g olive oil-packed tuna, drained']),\n",
       "       list(['1 1/2 ounces (1 jigger) applejack', '3 ounces (2 jiggers) apple juice', '1 teaspoon fresh lemon juice', 'Chilled seltzer or club soda']),\n",
       "       list(['1/2 lb fresh bratwurst sausages', '1/2 lb smoked bratwurst sausages', '3/4 lb Mexican chorizo (spicy fresh pork sausages)', '12 dozen oysters, well scrubbed', '4 dozen small hard-shelled (littleneck) clams, well scrubbed', '1 (12-oz) bottle of beer (not dark)', 'Accompaniments: Lowcountry aïoli , cocktail sauce, lemon wedges, and melted butter', '', '', '', 'Special equipment: about 2 yards burlap (for grill method) or heavy-duty foil, 12 oyster knives, and 12 oven mitts or thick kitchen towels', '', '']),\n",
       "       list(['2 1-pound boneless pork tenderloins, each cut crosswise into 4 equal pieces', '3 1/2 cups fresh breadcrumbs made from French bread', '1 cup chopped walnuts', '1/3 cup chopped fresh parsley', '1/4 teaspoon ground cinnamon', '1/4 teaspoon ground nutmeg', '1 cup all purpose flour', '2 large eggs, lightly beaten', '1/3 cup (or more) vegetable oil', 'Mustard Cream Sauce', 'Red Cabbage with Raspberries, Onions and Apples']),\n",
       "       list(['1 12-ounce piece of kasseri cheese, room temperature', '1/2 cup ouzo (unsweetened anise liqueur)', '1/4 cup olive oil', '1 tablespoon minced fresh parsley', '1 teaspoon chopped fresh thyme', '1 teaspoon coarsely ground pepper', 'Pita bread']),\n",
       "       list(['3 cups chopped pecans (about 12 ounces)', '2 cups chopped candied pineapple (about 10 ounces)', '3/4 cup chopped candied cherries (about 5 ounces)', '1/3 cup chopped candied orange peel (about 1 1/2 ounces)', '1 3/4 cups plus 3 tablespoons all purpose flour', '1 cup (2 sticks) butter, room temperature', '1 cup sugar', '5 eggs', '1 tablespoon vanilla extract', '1 tablespoon lemon extract', '1/2 teaspoon baking powder', 'Pinch of salt', 'Powdered sugar']),\n",
       "       list(['1 red bell pepper, cut into 1 by 1/4-inch strips', '3 cups cauliflower florets', '3/4 cup jicama, cut into 1 by 1/4 by 1/4-inch strips', '2 red Thai chiles, cut crosswise into 1/4-inch rounds', '3 cups white wine vinegar', '1 1/2 cups sugar', '1 1/2 teaspoons salt', '1 bunch fresh mint', '3 tablespoons vegetable oil', '2 shallots, minced', '1 clove garlic, minced', '2 tablespoons minced lemongrass', '2 tablespoons white sesame seeds, lightly toasted and coarsely ground', '2 tablespoons oyster sauce', '1 tablespoon fish sauce', '1 teaspoon sugar', '1 1/2 lbs pork shoulder', 'Salt to taste']),\n",
       "       list(['3 tablespoons extra-virgin olive oil', '1 1/2 pounds onions (2 large), sliced into 1/2-inch-thick rings', 'Salt and freshly ground black pepper', '', '3 tablespoons seasoned rice vinegar', '1/2 pound sliced bacon, frozen 1 hour', '1 1/2 pounds ground beef chuck', '1 tablespoon Worcestershire sauce', 'Salt', 'Freshly ground black pepper', '2 tablespoons unsalted butter, melted', '4 to 8 thin slices extra-sharp Cheddar', '4 hamburger buns or alternatives such as kaiser or brioche rolls', 'Sliced tomato', 'Soft-leaf lettuce leaves or baby arugula', 'Special equipment: An instant-read thermometer']),\n",
       "       list(['1 cup all-purpose flour', '2/3 cup coarse-grind cornmeal', '2 tablespoons sugar', '1/2 teaspoon kosher salt', '1/4 teaspoon baking powder', '1 large egg', '1 large egg white', '1 1/4 cups milk', '1/4 cup unsalted butter, melted; plus 2 teaspoons room temperature', 'Sour cream and sliced scallions (for serving)', 'Flaky sea salt']),\n",
       "       list(['4 15-ounce cans black-eyed peas, rinsed, well drained', '4 cups diced peeled cored fresh pineapple (1 large pineapple)', '1 7-ounce jar roasted bell peppers, drained, diced', '1 1/2 cups minced celery', '1 small red onion, minced', '2/3 cup minced fresh cilantro', '4 jalapeño chilies, seeded, chopped', '1/4 cup country-style Dijon mustard', '2 tablespoons cider vinegar', '1/2 cup olive oil', 'Ornamental kale', 'Fresh cilantro sprigs']),\n",
       "       list(['15 vine-ripened plum tomatoes', '1/2 cup packed fresh basil or flat-leafed parsley leaves, washed well and spun dry', '4 garlic cloves', '1/4 cup extra-virgin olive oil', 'fine sea salt to taste', 'freshly ground black pepper to taste']),\n",
       "       list(['1 4-pound piece flat-cut beef brisket, untrimmed', 'Kosher salt', '1/4 cup Dijon mustard', '1/4 cup (packed) dark brown sugar', '1 tablespoon grated peeled ginger', '2 tablespoons bacon fat or vegetable oil', '2 medium yellow onions, thinly sliced', '1/4 cup all-purpose flour', '1 bay leaf', '1 750-milliliter bottle Belgian-style tripel ale', '4 cups beef stock or low-sodium chicken broth']),\n",
       "       list(['1 rounded 1/4 teaspoon saffron threads', '1/4 cup olive oil', 'a 3 1/2-pound chicken, cut into serving pieces', '2 onions, chopped', '2 small green bell peppers, chopped', '3/4 pound plum tomatoes (about 6), peeled, seeded, and chopped', '2 garlic cloves, minced', '4 teaspoons paprika', '3 cups Arborio rice (Italian short-grain rice, available at Italian markets and some specialty foods shops)', '6 cups chicken broth', '1 large red bell pepper, roasted and cut into strips', '1 cup thawed frozen peas', '1/4 cup minced fresh parsley leaves if desired']),\n",
       "       list(['1 cup plus 2 tablespoons beer (not dark)', '1 cup all-purpose flour', '1/2 teaspoon table salt', '1/2 teaspoon freshly ground pepper', 'vegetable oil for deep-frying the kale leaves', '12 small kale leaves, washed well and spun dry', 'coarse salt for sprinkling the leaves', 'lemon wedges as an accompaniment']),\n",
       "       list(['3/4 cup plus 7 tablespoons olive oil', '2 medium onions, thinly sliced', '2 red bell peppers, very thinly sliced', '1 yellow bell pepper, very thinly sliced', '6 garlic cloves, minced', '1 tablespoon chopped fresh thyme', '1 tablespoon sugar', '1/4 cup fresh lemon juice', '4 skinless boneless chicken breast halves', '3/4 cup all purpose flour']),\n",
       "       list(['6 ounces vodka', '5 ounces simple syrup', '4 ounces chilled brewed black tea', '3 ounces fresh lime juice', 'Mint sprigs (for serving)']),\n",
       "       list(['1 1/4 cups apricot preserves', '2 tablespoons golden rum or water']),\n",
       "       list(['2 1/2 cups all-purpose flour (not unbleached)', '2 teaspoons sugar', '3/4 teaspoon salt', '2 sticks (1 cup) cold unsalted butter, cut into 1/2-inch cubes', '9 to 12 tablespoons ice water', 'Special equipment: a pastry or bench scraper']),\n",
       "       list(['1/2 cup warm water', '1 package dry yeast', '2 cups warm milk', '1/2 cup melted butter', '1 teaspoon salt', '1 teaspoon sugar', '2 cups all-purpose flour', '2 eggs', '1/4 teaspoon baking soda']),\n",
       "       list(['1 1/2 ounces gin', '1/2 ounce Lillet', 'Dash Angostura bitters', '3 or 4 ice cubes', 'Twist orange peel']),\n",
       "       list(['3 tablespoons olive oil', '1 medium onion, chopped', '6 cups low-salt chicken broth', '2 carrots, peeled, cut into 1/2-inch-thick rounds', '2 celery stalks, cut into 1/2-inch pieces', '4 small red-skinned potatoes, quartered', '1/2 pound green beans, trimmed, cut into 1-inch pieces', '3 small zucchini, halved lengthwise, cut into 1/2-inch pieces', '1 15-ounce can cannellini (white kidney beans), drained', '2 tomatoes, peeled, crushed', '2 cups fresh spinach leaves, chopped', '6 tablespoons Classic Pesto', 'Freshly grated Parmesan cheese']),\n",
       "       list(['Easy Tart Crust', '1 large egg', '2 tablespoons whipping cream', '4 large firm but ripe Bosc pears, peeled, quartered, cored, each quarter cut lengthwise into 3 slices', '3 tablespoons chopped husked toasted hazelnuts', '2 tablespoons coarsely chopped imported milk chocolate', '2 tablespoons coarsely chopped bittersweet (not unsweetened) or semisweet chocolate', '6 tablespoons raw sugar, divided', '1/4 teaspoon coarse sea salt (optional)']),\n",
       "       list(['1 vanilla bean', '1/2 cup sugar', '3 tablespoons cornstarch', '1/4 teaspoon salt', \"2 cups sweet Muscat such as Val d'Orbieu St.-Jean-de-Minervois\", '5 cups picked-over blackberries', '4 cups picked-over raspberries', '1 1/4 cups packed fresh basil sprigs', '1/2 teaspoon fresh lemon juice, or to taste', 'Accompaniment: crème fraîche or sour cream'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_nlp_test_df['ingredients'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1, 14), indices imply (49, 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_nlp_test_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py:491\u001b[0m, in \u001b[0;36mPyFuncModel.predict\u001b[0;34m(self, data, params)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _MLFLOW_TESTING\u001b[38;5;241m.\u001b[39mget():\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py:477\u001b[0m, in \u001b[0;36mPyFuncModel.predict.<locals>._predict\u001b[0;34m()\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m():\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Models saved prior to MLflow 2.5.0 do not support `params` in the pyfunc `predict()`\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# function definition, nor do they support `**kwargs`. Accordingly, we only pass\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# `params` to the `predict()` method if it defines the `params` argument\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_fn)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 477\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m     _log_warning_if_params_not_in_predict_signature(_logger, params)\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_fn(data)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/mlflow/pyfunc/model.py:469\u001b[0m, in \u001b[0;36m_PythonModelPyfuncWrapper.predict\u001b[0;34m(self, model_input, params)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m:param model_input: Model input data.\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m:param params: Additional parameters to pass to the model for inference.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m:return: Model predictions.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_model\u001b[38;5;241m.\u001b[39mpredict)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m _log_warning_if_params_not_in_predict_signature(_logger, params)\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_model\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(model_input))\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/src/custom_stanza_mlflow.py:76\u001b[0m, in \u001b[0;36mCustomSKLearnWrapper.predict\u001b[0;34m(self, context, model_input, params)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03mThis method is needed to override the default predict.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mIt needs to function essentially as a wrapper and returns back the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    the sklearn/Stanza text processing\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msklearn_transformer\u001b[38;5;241m.\u001b[39mtransform(model_input)\n\u001b[0;32m---> 76\u001b[0m transformed_recipe \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msklearn_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transformed_recipe\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/frame.py:722\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    712\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    713\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    714\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    719\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    720\u001b[0m         )\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:349\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    345\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[1;32m    346\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[1;32m    347\u001b[0m )\n\u001b[0;32m--> 349\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1, 14), indices imply (49, 14)"
     ]
    }
   ],
   "source": [
    "test_predictor.predict(to_nlp_test_df['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PyFuncModel' object has no attribute 'signature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PyFuncModel' object has no attribute 'signature'"
     ]
    }
   ],
   "source": [
    "test_predictor.signature.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': '[{\"type\": \"string\"}]', 'outputs': None, 'params': None}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info.signature.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# load dataframes from artifacts\n",
    "# mlflow.artifacts.download_artifacts(\n",
    "#     run_id=mlflow_run_id\n",
    "# )\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':'auto',\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(cv_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_stanza_ingreds_full_set_v1\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Raw Dataframe:', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    # to_nlp_df = pre_proc_df[0:50]\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Subset Dataframe:', end='\\n')\n",
    "    # print(to_nlp_df.head())\n",
    "    # print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params,\n",
    "    )\n",
    "\n",
    "    def custom_analyzer(step_list, stanza_pipeline, minNgramLength, maxNgramLength):\n",
    "            lowered = \" brk \".join(map(str, [step for step in step_list if step is not None])).lower()\n",
    "\n",
    "            preproc = stanza_pipeline(lowered)\n",
    "            \n",
    "            lemmad = \" \".join(map(str,\n",
    "                                [word.text\n",
    "                                for sent in preproc.sentences \n",
    "                                for word in sent.words if (\n",
    "                                    word is not None\n",
    "                                )]\n",
    "                            )\n",
    "                        )\n",
    "            \n",
    "            # analyze each line of the input string seperately\n",
    "            for ln in lemmad.split(' brk '):\n",
    "                \n",
    "                # tokenize the input string (customize the regex as desired)\n",
    "                at_least_two_english_characters_whole_words = \"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "                terms = re.split(at_least_two_english_characters_whole_words, ln)\n",
    "\n",
    "                # loop ngram creation for every number between min and max ngram length\n",
    "                for ngramLength in range(minNgramLength, maxNgramLength+1):\n",
    "\n",
    "                    # find and return all ngrams\n",
    "                    # for ngram in zip(*[terms[i:] for i in range(3)]): \n",
    "                        # <-- solution without a generator (works the same but has higher memory usage)\n",
    "                    for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]):   # <-- solution using a generator\n",
    "                        \n",
    "                        ngram = ' '.join(map(str, ngram))\n",
    "                        # yield ngram\n",
    "                        return str(ngram)\n",
    "\n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4}\n",
    "    \n",
    "    recipe_ingreds = pre_proc_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('Recipe ingredients:', end='\\n')\n",
    "    print(recipe_ingreds)\n",
    "\n",
    "    # train on the recipes' steps\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    # steps_vectorizer_model = CountVectorizer(**cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    steps_vectorizer_model = OnlineCountVectorizer(**cv_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=steps_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_full_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_full_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt run with lighter weight configuration\n",
    "#### This attempt will still use Stanza processing on the ingredients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_nlp_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# create sklearn pipeline as in BERTopic lightweight configuration\n",
    "# pipe = make_pipeline(\n",
    "#     TfidfVectorizer(**sklearn_nlp_params),\n",
    "#     TruncatedSVD(100)\n",
    "# )\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    # 'embedding_model': TfidfVectorizer(**sklearn_nlp_params),\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':50,\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True,\n",
    "    # 'min_cluster_size': 10 # Possibly only works if modifying individual HDBSCAN component of BERTopic\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_nlp_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_lightweight_stanza_ingreds_small_set_v1\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Raw Dataframe: ', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    to_nlp_df = pre_proc_df[0:100]\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Subset Dataframe:', end='\\n')\n",
    "    print(to_nlp_df.head())\n",
    "    print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params\n",
    "    )\n",
    "    \n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4}\n",
    "    \n",
    "    recipe_ingreds = to_nlp_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(**sklearn_nlp_params)\n",
    "    embeddings = vectorizer.fit_transform(recipe_ingreds)\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    print(recipe_ingreds)\n",
    "\n",
    "    # train on the recipes' ingredientss\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds, embeddings)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    sklearn_cv_params = {\n",
    "        'strip_accents':\"unicode\",\n",
    "        'lowercase':True,\n",
    "        'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "        # 'min_df':10,\n",
    "    }\n",
    "    steps_vectorizer_model = CountVectorizer(**sklearn_cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    # steps_vectorizer_model = OnlineCountVectorizer(**sklearn_nlp_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=steps_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representative Docs:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_full_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_full_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()['Representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()['Representation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_nlp_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# create sklearn pipeline as in BERTopic lightweight configuration\n",
    "# pipe = make_pipeline(\n",
    "#     TfidfVectorizer(**sklearn_nlp_params),\n",
    "#     TruncatedSVD(100)\n",
    "# )\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    # 'embedding_model': TfidfVectorizer(**sklearn_nlp_params),\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':50,\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True,\n",
    "    # 'min_cluster_size': 10 # Possibly only works if modifying individual HDBSCAN component of BERTopic\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_nlp_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_lightweight_stanza_ingreds_small_set_v1\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Raw Dataframe: ', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    to_nlp_df = pre_proc_df[0:100]\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Subset Dataframe:', end='\\n')\n",
    "    print(to_nlp_df.head())\n",
    "    print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params\n",
    "    )\n",
    "    \n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4}\n",
    "    \n",
    "    recipe_ingreds = to_nlp_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(**sklearn_nlp_params)\n",
    "    embeddings = vectorizer.fit_transform(recipe_ingreds)\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    print(recipe_ingreds)\n",
    "\n",
    "    # train on the recipes' ingredientss\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds, embeddings)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    sklearn_cv_params = {\n",
    "        'strip_accents':\"unicode\",\n",
    "        'lowercase':True,\n",
    "        # 'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "        # 'min_df':10,\n",
    "    }\n",
    "    steps_vectorizer_model = CountVectorizer(**sklearn_cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    # steps_vectorizer_model = OnlineCountVectorizer(**sklearn_nlp_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=steps_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representative Docs:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_full_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_full_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_nlp_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# create sklearn pipeline as in BERTopic lightweight configuration\n",
    "# pipe = make_pipeline(\n",
    "#     TfidfVectorizer(**sklearn_nlp_params),\n",
    "#     TruncatedSVD(100)\n",
    "# )\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    # 'embedding_model': TfidfVectorizer(**sklearn_nlp_params),\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':50,\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True,\n",
    "    # 'min_cluster_size': 10 # Possibly only works if modifying individual HDBSCAN component of BERTopic\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_nlp_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_lightweight_stanza_ingreds_small_set_v1.01\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Raw Dataframe: ', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    to_nlp_df = pre_proc_df[0:100]\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Subset Dataframe:', end='\\n')\n",
    "    print(to_nlp_df.head())\n",
    "    print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params\n",
    "    )\n",
    "    \n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4\n",
    "                       , 'lemmatize': True}\n",
    "    \n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    recipe_ingreds = to_nlp_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    print([ingred for ingred in recipe_ingreds])\n",
    "\n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(**sklearn_nlp_params)\n",
    "    embeddings = vectorizer.fit_transform(tqdm(recipe_ingreds))\n",
    "\n",
    "    # train on the recipes' ingredientss\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds, embeddings)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    sklearn_cv_params = {\n",
    "        # 'strip_accents':\"unicode\",\n",
    "        # 'lowercase':True,\n",
    "        # 'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "        # 'min_df':10,\n",
    "        'token_pattern': r\"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "    }\n",
    "    ingreds_vectorizer_model = CountVectorizer(**sklearn_cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    # steps_vectorizer_model = OnlineCountVectorizer(**sklearn_nlp_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=ingreds_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representative Docs:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_small_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_small_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_nlp_df['ingredients'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipe_ingreds = to_nlp_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i think i should start leaving out units/including stopwords again since i'm not using Stanza's deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_nlp_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':50,\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True,\n",
    "    # 'min_cluster_size': 10 # Possibly only works if modifying individual HDBSCAN component of BERTopic\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_nlp_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_lightweight_stanza_ingreds_full_set_v1.00\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Raw Dataframe: ', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    # to_nlp_df = pre_proc_df[0:100]\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print(f'{datetime.now()}, Subset Dataframe:', end='\\n')\n",
    "    # print(to_nlp_df.head())\n",
    "    # print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params\n",
    "    )\n",
    "    \n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4\n",
    "                       , 'lemmatize': True}\n",
    "    \n",
    "    recipe_ingreds = pre_proc_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "    \n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    print(recipe_ingreds)\n",
    "\n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(**sklearn_nlp_params)\n",
    "    embeddings = vectorizer.fit_transform(tqdm(recipe_ingreds))\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    # print(recipe_ingreds)\n",
    "\n",
    "    # train on the recipes' ingredientss\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds, embeddings)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    sklearn_cv_params = {\n",
    "        # 'strip_accents':\"unicode\",\n",
    "        # 'lowercase':True,\n",
    "        'token_pattern': r\"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "    }\n",
    "    ingreds_vectorizer_model = CountVectorizer(**sklearn_cv_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=ingreds_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representative Docs:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_full_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_full_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try splitting among CPU and GPU. Try Stanza on CPU due to its memory usage\n",
    "nlp2 = stanza.Pipeline('en', use_gpu=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
