{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: test\n",
    "output-file: template.html\n",
    "title: Template\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import OnlineCountVectorizer\n",
    "import dagshub\n",
    "from datetime import datetime\n",
    "import dill as pickle\n",
    "import dvc.api\n",
    "from hdbscan import HDBSCAN\n",
    "from itertools import tee, islice\n",
    "import joblib\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import nbdev\n",
    "from nbdev.showdoc import *\n",
    "import pandas as pd\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer\n",
    "    , TfidfTransformer\n",
    "    , TfidfVectorizer\n",
    "    , \n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from src.custom_sklearn_text_transformer_mlflow import CustomSKLearnAnalyzer\n",
    "from src.custom_stanza_mlflow import CustomSKLearnWrapper\n",
    "import src.dataframe_preprocessor as dfpp\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | export\n",
    "def foo():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# this function allows us to get the experiment ID from an experiment name\n",
    "def get_experiment_id(name):\n",
    "    exp = mlflow.get_experiment_by_name(name)\n",
    "    if exp is None:\n",
    "      exp_id = mlflow.create_experiment(name)\n",
    "      return exp_id\n",
    "    return exp.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_analyzer(step_list, stanza_pipeline, minNgramLength, maxNgramLength, lemmatize=True):\n",
    "    lowered = \" brk \".join(map(str, [step for step in step_list if step is not None])).lower()\n",
    "\n",
    "    preproc = stanza_pipeline(lowered)\n",
    "    \n",
    "    if lemmatize:\n",
    "        lemmad = \" \".join(map(str,\n",
    "                            [word.lemma\n",
    "                            for sent in preproc.sentences \n",
    "                            for word in sent.words if (\n",
    "                                word.upos not in [\"NUM\", \"DET\", \"ADV\", \"CCONJ\", \"ADP\", \"SCONJ\", \"PUNCT\"]\n",
    "                                and word is not None\n",
    "                            )]\n",
    "                        )\n",
    "                    )\n",
    "    else:\n",
    "        lemmad = \" \".join(map(str,\n",
    "                            [word.text\n",
    "                            for sent in preproc.sentences \n",
    "                            for word in sent.words if (\n",
    "                                word is not None\n",
    "                            )]\n",
    "                        )\n",
    "                    )\n",
    "    # analyze each line of the input string seperately\n",
    "    for ln in lemmad.split(' brk '):\n",
    "        # tokenize the input string (customize the regex as desired)\n",
    "        at_least_two_english_characters_whole_words = \"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "        terms = re.split(at_least_two_english_characters_whole_words, ln)\n",
    "\n",
    "        # loop ngram creation for every number between min and max ngram length\n",
    "        for ngramLength in range(minNgramLength, maxNgramLength+1):\n",
    "\n",
    "            # find and return all ngrams\n",
    "            # for ngram in zip(*[terms[i:] for i in range(3)]): \n",
    "                # <-- solution without a generator (works the same but has higher memory usage)\n",
    "            for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]):   # <-- solution using a generator\n",
    "                \n",
    "                ngram = ' '.join(map(str, ngram))\n",
    "                # yield ngram\n",
    "                return str(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# | Below this are blocks to use DagsHub with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@markdown Enter the username of your DAGsHub account:\n",
    "DAGSHUB_USER_NAME = \"AaronWChen\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the email for your DAGsHub account:\n",
    "DAGSHUB_EMAIL = \"awc33@cornell.edu\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the repo name \n",
    "DAGSHUB_REPO_NAME = \"MeaLeon\"\n",
    "\n",
    "#@markdown Enter the name of the branch you are working on \n",
    "BRANCH = \"MLF-1/start-custom-sklearn-mlflow-model\"\n",
    "dagshub.init(repo_name=DAGSHUB_REPO_NAME\n",
    "             , repo_owner=DAGSHUB_USER_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting DEV stage for One Hot Encoded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# starter idea for making an experiment name can be the git branch, but need more specificity\n",
    "experiment_name = f\"{DAGSHUB_EMAIL}/one-hot-encode\"\n",
    "mlflow_exp_id = get_experiment_id(experiment_name)\n",
    "\n",
    "# define model location\n",
    "# model_directory = \"/tmp/sklearn_model\"\n",
    "model_directory = \"../models/\"\n",
    "\n",
    "# Define the required artifacts associated with the saved custom pyfunc\n",
    "sklearn_path = model_directory + \"sklearn_model\"\n",
    "sklearn_model_path = sklearn_path + \"/python_model.pkl\"\n",
    "sklearn_transformer_path = sklearn_path + \"/sklearn_transformer.pkl\"\n",
    "transformed_recipes_path = sklearn_path + \"/transformer.pkl\"\n",
    "\n",
    "artifacts = {'sklearn_model': sklearn_model_path,\n",
    "             'sklearn_transformer': sklearn_transformer_path,\n",
    "             'transformed_recipes': transformed_recipes_path}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804e682778b843e6be587a23c7f28bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 21:23:03 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-01-31 21:23:04 INFO: File exists: /home/awchen/stanza_resources/en/default.zip\n",
      "2024-01-31 21:23:07 INFO: Finished downloading models and saved to /home/awchen/stanza_resources.\n",
      "2024-01-31 21:23:07 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cb166605ad4422b4264bbfb98eb2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 21:23:08 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2024-01-31 21:23:08 INFO: Using device: cpu\n",
      "2024-01-31 21:23:08 INFO: Loading: tokenize\n",
      "2024-01-31 21:23:08 INFO: Loading: pos\n",
      "2024-01-31 21:23:08 INFO: Loading: lemma\n",
      "2024-01-31 21:23:09 INFO: Loading: constituency\n",
      "2024-01-31 21:23:09 INFO: Loading: depparse\n",
      "2024-01-31 21:23:09 INFO: Loading: sentiment\n",
      "2024-01-31 21:23:10 INFO: Loading: ner\n",
      "2024-01-31 21:23:10 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------\n",
      "Raw Dataframe:\n",
      "                         id  \\\n",
      "0  54a2b6b019925f464b373351   \n",
      "1  54a408a019925f464b3733bc   \n",
      "2  54a408a26529d92b2c003631   \n",
      "3  54a408a66529d92b2c003638   \n",
      "4  54a408a719925f464b3733cc   \n",
      "\n",
      "                                                 dek  \\\n",
      "0  How does fried chicken achieve No. 1 status? B...   \n",
      "1                                Spinaci all'Ebraica   \n",
      "2  This majestic, moist, and richly spiced honey ...   \n",
      "3  The idea for this sandwich came to me when my ...   \n",
      "4  In 1930, Simon Agranat, the chief justice of t...   \n",
      "\n",
      "                                     hed                   pubDate  \\\n",
      "0            Pickle-Brined Fried Chicken  2014-08-19T04:00:00.000Z   \n",
      "1                   Spinach Jewish Style  2008-09-09T04:00:00.000Z   \n",
      "2                  New Year’s Honey Cake  2008-09-10T04:00:00.000Z   \n",
      "3  The B.L.A.Bagel with Lox and Avocado  2008-09-08T04:00:00.000Z   \n",
      "4        Shakshuka a la Doktor Shakshuka  2008-09-09T04:00:00.000Z   \n",
      "\n",
      "                             author    type  \\\n",
      "0                                []  recipe   \n",
      "1  [{'name': 'Edda Servi Machlin'}]  recipe   \n",
      "2       [{'name': 'Marcy Goldman'}]  recipe   \n",
      "3           [{'name': 'Faye Levy'}]  recipe   \n",
      "4         [{'name': 'Joan Nathan'}]  recipe   \n",
      "\n",
      "                                                 url  \\\n",
      "0  /recipes/food/views/pickle-brined-fried-chicke...   \n",
      "1    /recipes/food/views/spinach-jewish-style-350152   \n",
      "2  /recipes/food/views/majestic-and-moist-new-yea...   \n",
      "3  /recipes/food/views/the-b-l-a-bagel-with-lox-a...   \n",
      "4  /recipes/food/views/shakshuka-a-la-doktor-shak...   \n",
      "\n",
      "                                           photoData  \\\n",
      "0  {'id': '54a2b64a6529d92b2c003409', 'filename':...   \n",
      "1  {'id': '56746182accb4c9831e45e0a', 'filename':...   \n",
      "2  {'id': '55e85ba4cf90d6663f728014', 'filename':...   \n",
      "3  {'id': '5674617e47d1a28026045e4f', 'filename':...   \n",
      "4  {'id': '56746183b47c050a284a4e15', 'filename':...   \n",
      "\n",
      "                                                 tag  aggregateRating  \\\n",
      "0  {'category': 'ingredient', 'name': 'Chicken', ...             3.11   \n",
      "1  {'category': 'cuisine', 'name': 'Italian', 'ur...             3.22   \n",
      "2  {'category': 'cuisine', 'name': 'Jewish', 'url...             3.62   \n",
      "3  {'category': 'cuisine', 'name': 'Jewish', 'url...             4.00   \n",
      "4  {'category': 'cuisine', 'name': 'Jewish', 'url...             2.71   \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  [1 tablespoons yellow mustard seeds, 1 tablesp...   \n",
      "1  [3 pounds small-leaved bulk spinach, Salt, 1/2...   \n",
      "2  [3 1/2 cups all-purpose flour, 1 tablespoon ba...   \n",
      "3  [1 small ripe avocado, preferably Hass (see No...   \n",
      "4  [2 pounds fresh tomatoes, unpeeled and cut in ...   \n",
      "\n",
      "                                           prepSteps  reviewsCount  \\\n",
      "0  [Toast mustard and coriander seeds in a dry me...             7   \n",
      "1  [Remove the stems and roots from the spinach. ...             5   \n",
      "2  [I like this cake best baked in a 9-inch angel...           105   \n",
      "3  [A short time before serving, mash avocado and...             7   \n",
      "4  [1. Place the tomatoes, garlic, salt, paprika,...             7   \n",
      "\n",
      "   willMakeAgainPct  dateCrawled  \n",
      "0               100   1498547035  \n",
      "1                80   1498547740  \n",
      "2                88   1498547738  \n",
      "3               100   1498547740  \n",
      "4                83   1498547740  \n",
      "(34756, 15)\n"
     ]
    }
   ],
   "source": [
    "# instantiate stanza pipeline\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', \n",
    "                    depparse_batch_size=50, \n",
    "                    depparse_min_length_to_batch_separately=50,\n",
    "                    verbose=True,\n",
    "                    use_gpu=False, # set to true when on cloud/not on streaming computer\n",
    "                    batch_size=100\n",
    "                    )\n",
    "\n",
    "# load raw data and preprocess/clean\n",
    "data = dvc.api.read(\n",
    "    path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "    , mode='r')\n",
    "raw_df = pd.read_json(data)\n",
    "print('\\n')\n",
    "print('--------------')\n",
    "print('Raw Dataframe:', end='\\n')\n",
    "print(raw_df.head())\n",
    "print(raw_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------\n",
      "Preprocessed Dataframe:\n",
      "                                                                        dek  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc                                                      \n",
      "54a42cde19925f464b3809d2  Green chiles pickled in soy sauce and vinegar ...   \n",
      "54a433036529d92b2c015de3  This soup features the flavors of India: aroma...   \n",
      "54a451926529d92b2c01eda8                                                      \n",
      "54a430876529d92b2c013e2b  Brown sugar and molasses are balanced by fresh...   \n",
      "\n",
      "                                                                        hed  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  Grilled Hearts of Romaine with Blue Cheese Vin...   \n",
      "54a42cde19925f464b3809d2                              Soy-Pickled Jalapeños   \n",
      "54a433036529d92b2c015de3  Curried Potato and Spinach Soup with Onion Sal...   \n",
      "54a451926529d92b2c01eda8                                       Chicken Soup   \n",
      "54a430876529d92b2c013e2b                           Sweet-Hot Barbecue Sauce   \n",
      "\n",
      "                          aggregateRating  \\\n",
      "id                                          \n",
      "54a4270b19925f464b37c1dc             3.64   \n",
      "54a42cde19925f464b3809d2             3.43   \n",
      "54a433036529d92b2c015de3             3.00   \n",
      "54a451926529d92b2c01eda8             3.19   \n",
      "54a430876529d92b2c013e2b             0.00   \n",
      "\n",
      "                                                                ingredients  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  [1 1/2 cups white wine vinegar, 1/2 cup sugar,...   \n",
      "54a42cde19925f464b3809d2  [3 large fresh jalapeños (4 inches), sliced 1/...   \n",
      "54a433036529d92b2c015de3  [4 cups chopped red onions (about 2 large), 1 ...   \n",
      "54a451926529d92b2c01eda8  [1 pound chicken parts, 2 stalks celery, inclu...   \n",
      "54a430876529d92b2c013e2b  [2 tablespoons olive oil, 1 cup chopped onion,...   \n",
      "\n",
      "                                                                  prepSteps  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  [Combine first 5 ingredients and 1/4 teaspoon ...   \n",
      "54a42cde19925f464b3809d2  [Combine all ingredients in a small heavy sauc...   \n",
      "54a433036529d92b2c015de3  [Combine first 5 ingredients in heavy medium s...   \n",
      "54a451926529d92b2c01eda8  [1. Pour 12 cups of cold water into a large st...   \n",
      "54a430876529d92b2c013e2b  [Heat oil in large saucepan over medium-high h...   \n",
      "\n",
      "                          reviewsCount  willMakeAgainPct     cuisine_name  \\\n",
      "id                                                                          \n",
      "54a4270b19925f464b37c1dc             9               100  Missing Cuisine   \n",
      "54a42cde19925f464b3809d2             6               100  Missing Cuisine   \n",
      "54a433036529d92b2c015de3             6                67           Indian   \n",
      "54a451926529d92b2c01eda8            32                87           Kosher   \n",
      "54a430876529d92b2c013e2b             0                 0  Missing Cuisine   \n",
      "\n",
      "                                               photo_filename  \\\n",
      "id                                                              \n",
      "54a4270b19925f464b37c1dc  EP_12162015_placeholders_casual.jpg   \n",
      "54a42cde19925f464b3809d2  EP_12162015_placeholders_rustic.jpg   \n",
      "54a433036529d92b2c015de3                           234125.jpg   \n",
      "54a451926529d92b2c01eda8  EP_12162015_placeholders_formal.jpg   \n",
      "54a430876529d92b2c013e2b  EP_12162015_placeholders_rustic.jpg   \n",
      "\n",
      "                                                               photo_credit  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a42cde19925f464b3809d2  Photo by Chelsea Kyle, Prop Styling by Anna St...   \n",
      "54a433036529d92b2c015de3                                      Brian Leatart   \n",
      "54a451926529d92b2c01eda8  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a430876529d92b2c013e2b  Photo by Chelsea Kyle, Prop Styling by Anna St...   \n",
      "\n",
      "                              author_name            date_published  \\\n",
      "id                                                                    \n",
      "54a4270b19925f464b37c1dc    Kate Higgins  2010-12-16 04:00:00+00:00   \n",
      "54a42cde19925f464b3809d2     Lillian Chou 2009-02-19 04:00:00+00:00   \n",
      "54a433036529d92b2c015de3     Peter Gordon 2006-03-07 04:00:00+00:00   \n",
      "54a451926529d92b2c01eda8  Sharon Lebewohl 2004-08-20 04:00:00+00:00   \n",
      "54a430876529d92b2c013e2b   Suzanne Tracht 2007-12-03 20:11:11+00:00   \n",
      "\n",
      "                                                                 recipe_url  \n",
      "id                                                                           \n",
      "54a4270b19925f464b37c1dc  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a42cde19925f464b3809d2  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a433036529d92b2c015de3  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a451926529d92b2c01eda8  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a430876529d92b2c013e2b  https://www.epicurious.com/recipes/food/views/...  \n",
      "(50, 13)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Subset Dataframe:\n",
      "                                                                        dek  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc                                                      \n",
      "54a42cde19925f464b3809d2  Green chiles pickled in soy sauce and vinegar ...   \n",
      "54a433036529d92b2c015de3  This soup features the flavors of India: aroma...   \n",
      "54a451926529d92b2c01eda8                                                      \n",
      "54a430876529d92b2c013e2b  Brown sugar and molasses are balanced by fresh...   \n",
      "\n",
      "                                                                        hed  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  Grilled Hearts of Romaine with Blue Cheese Vin...   \n",
      "54a42cde19925f464b3809d2                              Soy-Pickled Jalapeños   \n",
      "54a433036529d92b2c015de3  Curried Potato and Spinach Soup with Onion Sal...   \n",
      "54a451926529d92b2c01eda8                                       Chicken Soup   \n",
      "54a430876529d92b2c013e2b                           Sweet-Hot Barbecue Sauce   \n",
      "\n",
      "                          aggregateRating  \\\n",
      "id                                          \n",
      "54a4270b19925f464b37c1dc             3.64   \n",
      "54a42cde19925f464b3809d2             3.43   \n",
      "54a433036529d92b2c015de3             3.00   \n",
      "54a451926529d92b2c01eda8             3.19   \n",
      "54a430876529d92b2c013e2b             0.00   \n",
      "\n",
      "                                                                ingredients  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  [1 1/2 cups white wine vinegar, 1/2 cup sugar,...   \n",
      "54a42cde19925f464b3809d2  [3 large fresh jalapeños (4 inches), sliced 1/...   \n",
      "54a433036529d92b2c015de3  [4 cups chopped red onions (about 2 large), 1 ...   \n",
      "54a451926529d92b2c01eda8  [1 pound chicken parts, 2 stalks celery, inclu...   \n",
      "54a430876529d92b2c013e2b  [2 tablespoons olive oil, 1 cup chopped onion,...   \n",
      "\n",
      "                                                                  prepSteps  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  [Combine first 5 ingredients and 1/4 teaspoon ...   \n",
      "54a42cde19925f464b3809d2  [Combine all ingredients in a small heavy sauc...   \n",
      "54a433036529d92b2c015de3  [Combine first 5 ingredients in heavy medium s...   \n",
      "54a451926529d92b2c01eda8  [1. Pour 12 cups of cold water into a large st...   \n",
      "54a430876529d92b2c013e2b  [Heat oil in large saucepan over medium-high h...   \n",
      "\n",
      "                          reviewsCount  willMakeAgainPct     cuisine_name  \\\n",
      "id                                                                          \n",
      "54a4270b19925f464b37c1dc             9               100  Missing Cuisine   \n",
      "54a42cde19925f464b3809d2             6               100  Missing Cuisine   \n",
      "54a433036529d92b2c015de3             6                67           Indian   \n",
      "54a451926529d92b2c01eda8            32                87           Kosher   \n",
      "54a430876529d92b2c013e2b             0                 0  Missing Cuisine   \n",
      "\n",
      "                                               photo_filename  \\\n",
      "id                                                              \n",
      "54a4270b19925f464b37c1dc  EP_12162015_placeholders_casual.jpg   \n",
      "54a42cde19925f464b3809d2  EP_12162015_placeholders_rustic.jpg   \n",
      "54a433036529d92b2c015de3                           234125.jpg   \n",
      "54a451926529d92b2c01eda8  EP_12162015_placeholders_formal.jpg   \n",
      "54a430876529d92b2c013e2b  EP_12162015_placeholders_rustic.jpg   \n",
      "\n",
      "                                                               photo_credit  \\\n",
      "id                                                                            \n",
      "54a4270b19925f464b37c1dc  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a42cde19925f464b3809d2  Photo by Chelsea Kyle, Prop Styling by Anna St...   \n",
      "54a433036529d92b2c015de3                                      Brian Leatart   \n",
      "54a451926529d92b2c01eda8  Photo by Chelsea Kyle, Prop Styling by Rhoda B...   \n",
      "54a430876529d92b2c013e2b  Photo by Chelsea Kyle, Prop Styling by Anna St...   \n",
      "\n",
      "                              author_name            date_published  \\\n",
      "id                                                                    \n",
      "54a4270b19925f464b37c1dc    Kate Higgins  2010-12-16 04:00:00+00:00   \n",
      "54a42cde19925f464b3809d2     Lillian Chou 2009-02-19 04:00:00+00:00   \n",
      "54a433036529d92b2c015de3     Peter Gordon 2006-03-07 04:00:00+00:00   \n",
      "54a451926529d92b2c01eda8  Sharon Lebewohl 2004-08-20 04:00:00+00:00   \n",
      "54a430876529d92b2c013e2b   Suzanne Tracht 2007-12-03 20:11:11+00:00   \n",
      "\n",
      "                                                                 recipe_url  \n",
      "id                                                                           \n",
      "54a4270b19925f464b37c1dc  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a42cde19925f464b3809d2  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a433036529d92b2c015de3  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a451926529d92b2c01eda8  https://www.epicurious.com/recipes/food/views/...  \n",
      "54a430876529d92b2c013e2b  https://www.epicurious.com/recipes/food/views/...  \n",
      "(50, 13)\n"
     ]
    }
   ],
   "source": [
    "# take sample and train/test split \n",
    "subset_df = raw_df.sample(n=100, random_state=45)\n",
    "train_df, test_df = train_test_split(subset_df,test_size=0.5, random_state=45)\n",
    "\n",
    "# pre_proc_df is cleaned dataframe\n",
    "pre_proc_df = dfpp.preprocess_dataframe(train_df)\n",
    "print('\\n')\n",
    "print('--------------')\n",
    "print('Preprocessed Dataframe:', end='\\n')\n",
    "print(pre_proc_df.head())\n",
    "print(pre_proc_df.shape)\n",
    "\n",
    "# create subset for dev purposes\n",
    "to_nlp_df = pre_proc_df\n",
    "print('\\n')\n",
    "print('-' * 80)\n",
    "print('Subset Dataframe:', end='\\n')\n",
    "print(to_nlp_df.head())\n",
    "print(to_nlp_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "sklearn fit transform on ingredients:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:49<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Transformed Data:\n",
      "                          cup chop onion  cup olive oil  cup sour cream  \\\n",
      "id                                                                        \n",
      "54a4270b19925f464b37c1dc               0              1               0   \n",
      "54a42cde19925f464b3809d2               0              0               0   \n",
      "54a433036529d92b2c015de3               0              0               0   \n",
      "54a451926529d92b2c01eda8               0              0               0   \n",
      "54a430876529d92b2c013e2b               1              0               0   \n",
      "54a453df6529d92b2c020687               0              0               1   \n",
      "55b0e7116284773353bf4580               0              0               0   \n",
      "54a42bab6529d92b2c00ffa7               0              0               0   \n",
      "54a4748f19925f464b399ef2               0              1               0   \n",
      "54a4356a19925f464b3875bb               0              1               0   \n",
      "54a4697e6529d92b2c0279d3               0              0               0   \n",
      "54a45e426529d92b2c02488f               0              0               0   \n",
      "54a452c96529d92b2c01f889               0              0               0   \n",
      "54a4323619925f464b384bcc               0              0               0   \n",
      "54a4259119925f464b37af9c               0              0               0   \n",
      "54a431da6529d92b2c014ee9               0              1               0   \n",
      "54a426fd19925f464b37c125               0              0               0   \n",
      "54a47bb019925f464b39b9b7               0              0               0   \n",
      "54a434d819925f464b386e62               0              1               0   \n",
      "54a428116529d92b2c00d1a7               0              1               0   \n",
      "54a436036529d92b2c01859e               0              0               0   \n",
      "54a47edf19925f464b39c58d               0              0               0   \n",
      "54a419706529d92b2c006650               0              0               0   \n",
      "54a4349619925f464b386b12               0              0               0   \n",
      "54a4340f6529d92b2c016be8               0              0               0   \n",
      "54a40e546529d92b2c004606               0              0               0   \n",
      "54a428b419925f464b37d5ce               0              0               0   \n",
      "54a453a519925f464b38fd16               0              0               0   \n",
      "54a41cb219925f464b376d82               0              0               0   \n",
      "54a431896529d92b2c014b27               0              0               0   \n",
      "54a423ab19925f464b3799f2               0              0               0   \n",
      "54a47c1419925f464b39bb28               0              0               0   \n",
      "593ee3ba12c27b182380821f               0              0               0   \n",
      "54a456366529d92b2c02235a               1              0               0   \n",
      "54a452d419925f464b38f1b5               0              0               1   \n",
      "54a4659b6529d92b2c026a53               0              0               0   \n",
      "54a46d5d19925f464b3982d3               0              0               0   \n",
      "54a4582119925f464b3927a1               0              0               1   \n",
      "54a4205319925f464b377c9f               0              0               0   \n",
      "54a470cc19925f464b39906b               1              0               0   \n",
      "54a44f4a6529d92b2c01de45               0              0               0   \n",
      "592ef494ae10ad089795ebfa               0              0               0   \n",
      "54a41f016529d92b2c00757d               0              0               0   \n",
      "54a436266529d92b2c01876e               0              0               1   \n",
      "54a428bd19925f464b37d63e               0              0               0   \n",
      "54a45a4f6529d92b2c0234d1               0              0               0   \n",
      "54a41ed76529d92b2c007440               0              0               0   \n",
      "54a42c906529d92b2c010b74               0              0               0   \n",
      "569519a6dc18ea6c22c9b9ab               0              0               0   \n",
      "54a438d56529d92b2c019648               0              1               0   \n",
      "\n",
      "                          cup sugar  cup water  large egg  \\\n",
      "id                                                          \n",
      "54a4270b19925f464b37c1dc          1          1          0   \n",
      "54a42cde19925f464b3809d2          0          1          0   \n",
      "54a433036529d92b2c015de3          0          0          0   \n",
      "54a451926529d92b2c01eda8          0          0          0   \n",
      "54a430876529d92b2c013e2b          0          0          0   \n",
      "54a453df6529d92b2c020687          0          0          0   \n",
      "55b0e7116284773353bf4580          0          0          0   \n",
      "54a42bab6529d92b2c00ffa7          0          0          0   \n",
      "54a4748f19925f464b399ef2          0          0          0   \n",
      "54a4356a19925f464b3875bb          0          0          0   \n",
      "54a4697e6529d92b2c0279d3          0          0          0   \n",
      "54a45e426529d92b2c02488f          0          0          0   \n",
      "54a452c96529d92b2c01f889          0          0          0   \n",
      "54a4323619925f464b384bcc          0          0          0   \n",
      "54a4259119925f464b37af9c          0          0          0   \n",
      "54a431da6529d92b2c014ee9          0          0          0   \n",
      "54a426fd19925f464b37c125          0          0          0   \n",
      "54a47bb019925f464b39b9b7          0          0          0   \n",
      "54a434d819925f464b386e62          0          0          0   \n",
      "54a428116529d92b2c00d1a7          0          0          0   \n",
      "54a436036529d92b2c01859e          1          0          0   \n",
      "54a47edf19925f464b39c58d          1          0          0   \n",
      "54a419706529d92b2c006650          0          0          0   \n",
      "54a4349619925f464b386b12          0          0          1   \n",
      "54a4340f6529d92b2c016be8          0          0          0   \n",
      "54a40e546529d92b2c004606          0          0          0   \n",
      "54a428b419925f464b37d5ce          1          0          0   \n",
      "54a453a519925f464b38fd16          0          1          0   \n",
      "54a41cb219925f464b376d82          0          0          0   \n",
      "54a431896529d92b2c014b27          0          1          0   \n",
      "54a423ab19925f464b3799f2          0          0          0   \n",
      "54a47c1419925f464b39bb28          0          0          0   \n",
      "593ee3ba12c27b182380821f          0          0          0   \n",
      "54a456366529d92b2c02235a          0          0          0   \n",
      "54a452d419925f464b38f1b5          0          0          0   \n",
      "54a4659b6529d92b2c026a53          0          0          0   \n",
      "54a46d5d19925f464b3982d3          1          0          0   \n",
      "54a4582119925f464b3927a1          0          0          0   \n",
      "54a4205319925f464b377c9f          0          0          0   \n",
      "54a470cc19925f464b39906b          0          0          0   \n",
      "54a44f4a6529d92b2c01de45          0          0          0   \n",
      "592ef494ae10ad089795ebfa          0          0          0   \n",
      "54a41f016529d92b2c00757d          0          0          1   \n",
      "54a436266529d92b2c01876e          1          1          0   \n",
      "54a428bd19925f464b37d63e          0          0          0   \n",
      "54a45a4f6529d92b2c0234d1          0          0          0   \n",
      "54a41ed76529d92b2c007440          1          1          0   \n",
      "54a42c906529d92b2c010b74          0          0          1   \n",
      "569519a6dc18ea6c22c9b9ab          0          0          0   \n",
      "54a438d56529d92b2c019648          0          0          0   \n",
      "\n",
      "                          tablespoon extra-virgin olive oil  \\\n",
      "id                                                            \n",
      "54a4270b19925f464b37c1dc                                  0   \n",
      "54a42cde19925f464b3809d2                                  0   \n",
      "54a433036529d92b2c015de3                                  0   \n",
      "54a451926529d92b2c01eda8                                  0   \n",
      "54a430876529d92b2c013e2b                                  0   \n",
      "54a453df6529d92b2c020687                                  0   \n",
      "55b0e7116284773353bf4580                                  0   \n",
      "54a42bab6529d92b2c00ffa7                                  0   \n",
      "54a4748f19925f464b399ef2                                  0   \n",
      "54a4356a19925f464b3875bb                                  0   \n",
      "54a4697e6529d92b2c0279d3                                  0   \n",
      "54a45e426529d92b2c02488f                                  0   \n",
      "54a452c96529d92b2c01f889                                  0   \n",
      "54a4323619925f464b384bcc                                  0   \n",
      "54a4259119925f464b37af9c                                  1   \n",
      "54a431da6529d92b2c014ee9                                  0   \n",
      "54a426fd19925f464b37c125                                  0   \n",
      "54a47bb019925f464b39b9b7                                  0   \n",
      "54a434d819925f464b386e62                                  0   \n",
      "54a428116529d92b2c00d1a7                                  0   \n",
      "54a436036529d92b2c01859e                                  0   \n",
      "54a47edf19925f464b39c58d                                  0   \n",
      "54a419706529d92b2c006650                                  0   \n",
      "54a4349619925f464b386b12                                  0   \n",
      "54a4340f6529d92b2c016be8                                  0   \n",
      "54a40e546529d92b2c004606                                  1   \n",
      "54a428b419925f464b37d5ce                                  0   \n",
      "54a453a519925f464b38fd16                                  0   \n",
      "54a41cb219925f464b376d82                                  0   \n",
      "54a431896529d92b2c014b27                                  0   \n",
      "54a423ab19925f464b3799f2                                  0   \n",
      "54a47c1419925f464b39bb28                                  1   \n",
      "593ee3ba12c27b182380821f                                  0   \n",
      "54a456366529d92b2c02235a                                  0   \n",
      "54a452d419925f464b38f1b5                                  0   \n",
      "54a4659b6529d92b2c026a53                                  0   \n",
      "54a46d5d19925f464b3982d3                                  0   \n",
      "54a4582119925f464b3927a1                                  0   \n",
      "54a4205319925f464b377c9f                                  0   \n",
      "54a470cc19925f464b39906b                                  0   \n",
      "54a44f4a6529d92b2c01de45                                  0   \n",
      "592ef494ae10ad089795ebfa                                  0   \n",
      "54a41f016529d92b2c00757d                                  0   \n",
      "54a436266529d92b2c01876e                                  0   \n",
      "54a428bd19925f464b37d63e                                  0   \n",
      "54a45a4f6529d92b2c0234d1                                  0   \n",
      "54a41ed76529d92b2c007440                                  0   \n",
      "54a42c906529d92b2c010b74                                  0   \n",
      "569519a6dc18ea6c22c9b9ab                                  0   \n",
      "54a438d56529d92b2c019648                                  0   \n",
      "\n",
      "                          tablespoon fresh lemon juice  tablespoon olive oil  \\\n",
      "id                                                                             \n",
      "54a4270b19925f464b37c1dc                             0                     0   \n",
      "54a42cde19925f464b3809d2                             0                     0   \n",
      "54a433036529d92b2c015de3                             1                     1   \n",
      "54a451926529d92b2c01eda8                             0                     0   \n",
      "54a430876529d92b2c013e2b                             0                     1   \n",
      "54a453df6529d92b2c020687                             0                     0   \n",
      "55b0e7116284773353bf4580                             0                     0   \n",
      "54a42bab6529d92b2c00ffa7                             0                     0   \n",
      "54a4748f19925f464b399ef2                             0                     0   \n",
      "54a4356a19925f464b3875bb                             0                     0   \n",
      "54a4697e6529d92b2c0279d3                             0                     0   \n",
      "54a45e426529d92b2c02488f                             0                     0   \n",
      "54a452c96529d92b2c01f889                             0                     0   \n",
      "54a4323619925f464b384bcc                             0                     0   \n",
      "54a4259119925f464b37af9c                             1                     0   \n",
      "54a431da6529d92b2c014ee9                             0                     0   \n",
      "54a426fd19925f464b37c125                             0                     0   \n",
      "54a47bb019925f464b39b9b7                             0                     0   \n",
      "54a434d819925f464b386e62                             0                     0   \n",
      "54a428116529d92b2c00d1a7                             0                     0   \n",
      "54a436036529d92b2c01859e                             0                     0   \n",
      "54a47edf19925f464b39c58d                             0                     0   \n",
      "54a419706529d92b2c006650                             0                     0   \n",
      "54a4349619925f464b386b12                             0                     0   \n",
      "54a4340f6529d92b2c016be8                             0                     0   \n",
      "54a40e546529d92b2c004606                             0                     0   \n",
      "54a428b419925f464b37d5ce                             0                     0   \n",
      "54a453a519925f464b38fd16                             0                     0   \n",
      "54a41cb219925f464b376d82                             0                     0   \n",
      "54a431896529d92b2c014b27                             0                     0   \n",
      "54a423ab19925f464b3799f2                             0                     0   \n",
      "54a47c1419925f464b39bb28                             1                     0   \n",
      "593ee3ba12c27b182380821f                             1                     0   \n",
      "54a456366529d92b2c02235a                             0                     1   \n",
      "54a452d419925f464b38f1b5                             0                     0   \n",
      "54a4659b6529d92b2c026a53                             1                     1   \n",
      "54a46d5d19925f464b3982d3                             0                     0   \n",
      "54a4582119925f464b3927a1                             0                     0   \n",
      "54a4205319925f464b377c9f                             0                     0   \n",
      "54a470cc19925f464b39906b                             0                     1   \n",
      "54a44f4a6529d92b2c01de45                             0                     0   \n",
      "592ef494ae10ad089795ebfa                             1                     0   \n",
      "54a41f016529d92b2c00757d                             0                     0   \n",
      "54a436266529d92b2c01876e                             0                     1   \n",
      "54a428bd19925f464b37d63e                             0                     0   \n",
      "54a45a4f6529d92b2c0234d1                             0                     0   \n",
      "54a41ed76529d92b2c007440                             0                     0   \n",
      "54a42c906529d92b2c010b74                             0                     0   \n",
      "569519a6dc18ea6c22c9b9ab                             0                     0   \n",
      "54a438d56529d92b2c019648                             0                     0   \n",
      "\n",
      "                          tablespoon sugar  tablespoon white wine vinegar  \\\n",
      "id                                                                          \n",
      "54a4270b19925f464b37c1dc                 0                              0   \n",
      "54a42cde19925f464b3809d2                 0                              0   \n",
      "54a433036529d92b2c015de3                 0                              0   \n",
      "54a451926529d92b2c01eda8                 0                              0   \n",
      "54a430876529d92b2c013e2b                 0                              0   \n",
      "54a453df6529d92b2c020687                 0                              0   \n",
      "55b0e7116284773353bf4580                 0                              0   \n",
      "54a42bab6529d92b2c00ffa7                 0                              0   \n",
      "54a4748f19925f464b399ef2                 0                              0   \n",
      "54a4356a19925f464b3875bb                 0                              1   \n",
      "54a4697e6529d92b2c0279d3                 0                              0   \n",
      "54a45e426529d92b2c02488f                 0                              0   \n",
      "54a452c96529d92b2c01f889                 0                              0   \n",
      "54a4323619925f464b384bcc                 1                              0   \n",
      "54a4259119925f464b37af9c                 0                              0   \n",
      "54a431da6529d92b2c014ee9                 0                              0   \n",
      "54a426fd19925f464b37c125                 0                              0   \n",
      "54a47bb019925f464b39b9b7                 0                              0   \n",
      "54a434d819925f464b386e62                 0                              0   \n",
      "54a428116529d92b2c00d1a7                 0                              0   \n",
      "54a436036529d92b2c01859e                 0                              0   \n",
      "54a47edf19925f464b39c58d                 0                              0   \n",
      "54a419706529d92b2c006650                 0                              0   \n",
      "54a4349619925f464b386b12                 1                              0   \n",
      "54a4340f6529d92b2c016be8                 0                              0   \n",
      "54a40e546529d92b2c004606                 0                              0   \n",
      "54a428b419925f464b37d5ce                 0                              0   \n",
      "54a453a519925f464b38fd16                 0                              0   \n",
      "54a41cb219925f464b376d82                 0                              0   \n",
      "54a431896529d92b2c014b27                 0                              0   \n",
      "54a423ab19925f464b3799f2                 0                              0   \n",
      "54a47c1419925f464b39bb28                 0                              0   \n",
      "593ee3ba12c27b182380821f                 0                              1   \n",
      "54a456366529d92b2c02235a                 0                              0   \n",
      "54a452d419925f464b38f1b5                 0                              0   \n",
      "54a4659b6529d92b2c026a53                 0                              0   \n",
      "54a46d5d19925f464b3982d3                 0                              0   \n",
      "54a4582119925f464b3927a1                 0                              0   \n",
      "54a4205319925f464b377c9f                 0                              0   \n",
      "54a470cc19925f464b39906b                 0                              0   \n",
      "54a44f4a6529d92b2c01de45                 0                              0   \n",
      "592ef494ae10ad089795ebfa                 0                              1   \n",
      "54a41f016529d92b2c00757d                 0                              0   \n",
      "54a436266529d92b2c01876e                 0                              0   \n",
      "54a428bd19925f464b37d63e                 0                              0   \n",
      "54a45a4f6529d92b2c0234d1                 1                              0   \n",
      "54a41ed76529d92b2c007440                 0                              0   \n",
      "54a42c906529d92b2c010b74                 0                              0   \n",
      "569519a6dc18ea6c22c9b9ab                 0                              0   \n",
      "54a438d56529d92b2c019648                 0                              0   \n",
      "\n",
      "                          teaspoon ground cumin  teaspoon salt  \\\n",
      "id                                                               \n",
      "54a4270b19925f464b37c1dc                      0              0   \n",
      "54a42cde19925f464b3809d2                      0              0   \n",
      "54a433036529d92b2c015de3                      0              0   \n",
      "54a451926529d92b2c01eda8                      0              1   \n",
      "54a430876529d92b2c013e2b                      0              0   \n",
      "54a453df6529d92b2c020687                      0              1   \n",
      "55b0e7116284773353bf4580                      0              0   \n",
      "54a42bab6529d92b2c00ffa7                      0              0   \n",
      "54a4748f19925f464b399ef2                      1              0   \n",
      "54a4356a19925f464b3875bb                      0              0   \n",
      "54a4697e6529d92b2c0279d3                      0              0   \n",
      "54a45e426529d92b2c02488f                      0              0   \n",
      "54a452c96529d92b2c01f889                      0              0   \n",
      "54a4323619925f464b384bcc                      0              0   \n",
      "54a4259119925f464b37af9c                      0              0   \n",
      "54a431da6529d92b2c014ee9                      0              0   \n",
      "54a426fd19925f464b37c125                      0              0   \n",
      "54a47bb019925f464b39b9b7                      0              1   \n",
      "54a434d819925f464b386e62                      0              0   \n",
      "54a428116529d92b2c00d1a7                      0              0   \n",
      "54a436036529d92b2c01859e                      0              0   \n",
      "54a47edf19925f464b39c58d                      0              0   \n",
      "54a419706529d92b2c006650                      0              0   \n",
      "54a4349619925f464b386b12                      0              0   \n",
      "54a4340f6529d92b2c016be8                      0              0   \n",
      "54a40e546529d92b2c004606                      0              0   \n",
      "54a428b419925f464b37d5ce                      0              0   \n",
      "54a453a519925f464b38fd16                      0              1   \n",
      "54a41cb219925f464b376d82                      0              0   \n",
      "54a431896529d92b2c014b27                      0              0   \n",
      "54a423ab19925f464b3799f2                      0              0   \n",
      "54a47c1419925f464b39bb28                      1              0   \n",
      "593ee3ba12c27b182380821f                      0              0   \n",
      "54a456366529d92b2c02235a                      1              1   \n",
      "54a452d419925f464b38f1b5                      0              0   \n",
      "54a4659b6529d92b2c026a53                      0              0   \n",
      "54a46d5d19925f464b3982d3                      0              0   \n",
      "54a4582119925f464b3927a1                      0              0   \n",
      "54a4205319925f464b377c9f                      0              0   \n",
      "54a470cc19925f464b39906b                      0              0   \n",
      "54a44f4a6529d92b2c01de45                      0              0   \n",
      "592ef494ae10ad089795ebfa                      0              0   \n",
      "54a41f016529d92b2c00757d                      0              0   \n",
      "54a436266529d92b2c01876e                      0              1   \n",
      "54a428bd19925f464b37d63e                      0              0   \n",
      "54a45a4f6529d92b2c0234d1                      0              0   \n",
      "54a41ed76529d92b2c007440                      0              0   \n",
      "54a42c906529d92b2c010b74                      0              0   \n",
      "569519a6dc18ea6c22c9b9ab                      0              0   \n",
      "54a438d56529d92b2c019648                      0              0   \n",
      "\n",
      "                          teaspoon vanilla extract  \n",
      "id                                                  \n",
      "54a4270b19925f464b37c1dc                         0  \n",
      "54a42cde19925f464b3809d2                         0  \n",
      "54a433036529d92b2c015de3                         0  \n",
      "54a451926529d92b2c01eda8                         0  \n",
      "54a430876529d92b2c013e2b                         0  \n",
      "54a453df6529d92b2c020687                         0  \n",
      "55b0e7116284773353bf4580                         0  \n",
      "54a42bab6529d92b2c00ffa7                         0  \n",
      "54a4748f19925f464b399ef2                         0  \n",
      "54a4356a19925f464b3875bb                         0  \n",
      "54a4697e6529d92b2c0279d3                         0  \n",
      "54a45e426529d92b2c02488f                         0  \n",
      "54a452c96529d92b2c01f889                         0  \n",
      "54a4323619925f464b384bcc                         0  \n",
      "54a4259119925f464b37af9c                         0  \n",
      "54a431da6529d92b2c014ee9                         0  \n",
      "54a426fd19925f464b37c125                         0  \n",
      "54a47bb019925f464b39b9b7                         0  \n",
      "54a434d819925f464b386e62                         0  \n",
      "54a428116529d92b2c00d1a7                         0  \n",
      "54a436036529d92b2c01859e                         0  \n",
      "54a47edf19925f464b39c58d                         0  \n",
      "54a419706529d92b2c006650                         0  \n",
      "54a4349619925f464b386b12                         1  \n",
      "54a4340f6529d92b2c016be8                         0  \n",
      "54a40e546529d92b2c004606                         0  \n",
      "54a428b419925f464b37d5ce                         1  \n",
      "54a453a519925f464b38fd16                         0  \n",
      "54a41cb219925f464b376d82                         0  \n",
      "54a431896529d92b2c014b27                         0  \n",
      "54a423ab19925f464b3799f2                         0  \n",
      "54a47c1419925f464b39bb28                         0  \n",
      "593ee3ba12c27b182380821f                         0  \n",
      "54a456366529d92b2c02235a                         0  \n",
      "54a452d419925f464b38f1b5                         0  \n",
      "54a4659b6529d92b2c026a53                         0  \n",
      "54a46d5d19925f464b3982d3                         0  \n",
      "54a4582119925f464b3927a1                         0  \n",
      "54a4205319925f464b377c9f                         0  \n",
      "54a470cc19925f464b39906b                         0  \n",
      "54a44f4a6529d92b2c01de45                         0  \n",
      "592ef494ae10ad089795ebfa                         0  \n",
      "54a41f016529d92b2c00757d                         0  \n",
      "54a436266529d92b2c01876e                         0  \n",
      "54a428bd19925f464b37d63e                         0  \n",
      "54a45a4f6529d92b2c0234d1                         0  \n",
      "54a41ed76529d92b2c007440                         0  \n",
      "54a42c906529d92b2c010b74                         1  \n",
      "569519a6dc18ea6c22c9b9ab                         0  \n",
      "54a438d56529d92b2c019648                         0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/31 21:33:16 WARNING mlflow.models.signature: Failed to infer the model signature from the input example. Reason: AttributeError(\"'CustomSKLearnWrapper' object has no attribute 'sklearn_transformer'\"). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`. To disable automatic signature inference, set `signature` to `False` in your `log_model` or `save_model` call.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5977046080c8466fbb28026c2d96f0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040e7dbb192c4a5e9fed712f479ace1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MlflowException",
     "evalue": "The following failures occurred while downloading one or more artifacts from ../models/sklearn_model:\n##### File sklearn_transformer.pkl #####\n[Errno 2] No such file or directory: '../models/sklearn_model/sklearn_transformer.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformed Data:\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(transformed_recipe)\n\u001b[0;32m---> 68\u001b[0m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msklearn_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../src/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpython_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCustomSKLearnWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_example\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_nlp_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mingredients\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(sklearn_transformer, sklearn_transformer_path)\n\u001b[1;32m     77\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(transformed_recipe, transformed_recipes_path)\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py:1934\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(path, loader_module, data_path, code_path, conda_env, mlflow_model, python_model, artifacts, signature, input_example, pip_requirements, extra_pip_requirements, metadata, model_config, **kwargs)\u001b[0m\n\u001b[1;32m   1922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _save_model_with_loader_module_and_data_path(\n\u001b[1;32m   1923\u001b[0m         path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   1924\u001b[0m         loader_module\u001b[38;5;241m=\u001b[39mloader_module,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1931\u001b[0m         model_config\u001b[38;5;241m=\u001b[39mmodel_config,\n\u001b[1;32m   1932\u001b[0m     )\n\u001b[1;32m   1933\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m second_argument_set_specified:\n\u001b[0;32m-> 1934\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_model_with_class_artifacts_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpython_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpython_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconda_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconda_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcode_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmlflow_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlflow_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpip_requirements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/mlflow/pyfunc/model.py:283\u001b[0m, in \u001b[0;36m_save_model_with_class_artifacts_params\u001b[0;34m(path, python_model, signature, hints, artifacts, conda_env, code_paths, mlflow_model, pip_requirements, extra_pip_requirements, model_config)\u001b[0m\n\u001b[1;32m    279\u001b[0m     saved_artifact_subpath \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    280\u001b[0m         Path(snapshot_location)\u001b[38;5;241m.\u001b[39mrelative_to(Path(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(path)))\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m     tmp_artifact_path \u001b[38;5;241m=\u001b[39m \u001b[43m_download_artifact_from_uri\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifact_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmp_artifacts_dir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     relative_path \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    288\u001b[0m         Path(tmp_artifact_path)\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;241m.\u001b[39mrelative_to(Path(tmp_artifacts_dir\u001b[38;5;241m.\u001b[39mpath()))\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    293\u001b[0m     saved_artifact_subpath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    294\u001b[0m         saved_artifacts_dir_subpath, relative_path\n\u001b[1;32m    295\u001b[0m     )\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/mlflow/tracking/artifact_utils.py:100\u001b[0m, in \u001b[0;36m_download_artifact_from_uri\u001b[0;34m(artifact_uri, output_path)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m:param artifact_uri: The *absolute* URI of the artifact to download.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m:param output_path: The local filesystem path to which to download the artifact. If unspecified,\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m                    a local output path will be created.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m root_uri, artifact_path \u001b[38;5;241m=\u001b[39m _get_root_uri_and_artifact_path(artifact_uri)\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_artifact_repository\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_uri\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/mlflow/store/artifact/local_artifact_repo.py:76\u001b[0m, in \u001b[0;36mLocalArtifactRepository.download_artifacts\u001b[0;34m(self, artifact_path, dst_path)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mArtifacts tracked by ``LocalArtifactRepository`` already exist on the local filesystem.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mIf ``dst_path`` is ``None``, the absolute filesystem path of the specified artifact is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m:return: Absolute path of the local filesystem location containing the desired artifacts.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dst_path:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# NOTE: The artifact_path is expected to be in posix format.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Posix paths work fine on windows but just in case we normalize it here.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m local_artifact_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39martifact_dir, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(artifact_path))\n",
      "File \u001b[0;32m~/Repos/Projects/MeaLeon/.venv/lib/python3.10/site-packages/mlflow/store/artifact/artifact_repo.py:221\u001b[0m, in \u001b[0;36mArtifactRepository.download_artifacts\u001b[0;34m(self, artifact_path, dst_path)\u001b[0m\n\u001b[1;32m    217\u001b[0m     template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m##### File \u001b[39m\u001b[38;5;132;01m{path}\u001b[39;00m\u001b[38;5;124m #####\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{error}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m     failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    219\u001b[0m         template\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mpath, error\u001b[38;5;241m=\u001b[39merror) \u001b[38;5;28;01mfor\u001b[39;00m path, error \u001b[38;5;129;01min\u001b[39;00m failed_downloads\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    220\u001b[0m     )\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    222\u001b[0m         message\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    223\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following failures occurred while downloading one or more\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m artifacts from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39martifact_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_truncate_error(failures)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m         )\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst_path, artifact_path)\n",
      "\u001b[0;31mMlflowException\u001b[0m: The following failures occurred while downloading one or more artifacts from ../models/sklearn_model:\n##### File sklearn_transformer.pkl #####\n[Errno 2] No such file or directory: '../models/sklearn_model/sklearn_transformer.pkl'"
     ]
    }
   ],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_transformer_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': CustomSKLearnAnalyzer().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':3,\n",
    "    'binary':True\n",
    "}\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "# bertopic_params = {\n",
    "#     'top_n_words':20,\n",
    "#     'min_topic_size':5,\n",
    "#     'nr_topics':'auto',\n",
    "#     'verbose':True,\n",
    "#     'low_memory':True,\n",
    "#     'calculate_probabilities':True\n",
    "# }\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'OneHotEncoder'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_transformer_params)\n",
    "# pipeline_params.update(bertopic_params)\n",
    "\n",
    "signature = infer_signature(to_nlp_df['ingredients'][0])\n",
    "\n",
    "with mlflow.start_run(experiment_id=mlflow_exp_id):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    # Will be useful in STAGING/Evaluation\n",
    "    \n",
    "    # LOG MODEL\n",
    "    # Instantiate sklearn OneHotEncoder\n",
    "    sklearn_transformer = CountVectorizer(**sklearn_transformer_params)\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('sklearn fit transform on ingredients:', end='\\n')\n",
    "\n",
    "    # Do fit transform on data\n",
    "    response = sklearn_transformer.fit_transform(tqdm(to_nlp_df['ingredients']))\n",
    "    transformed_recipe = pd.DataFrame(\n",
    "            response.toarray(),\n",
    "            columns=sklearn_transformer.get_feature_names_out(),\n",
    "            index=to_nlp_df.index\n",
    "    )\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('Transformed Data:', end='\\n')\n",
    "    print(transformed_recipe)\n",
    "\n",
    "    mlflow.pyfunc.save_model(\n",
    "        path=sklearn_path,\n",
    "        code_path=[\"../src/\"],\n",
    "        python_model=CustomSKLearnWrapper(),\n",
    "        input_example=to_nlp_df['ingredients'][0],    \n",
    "        artifacts=artifacts\n",
    "    )\n",
    "\n",
    "    joblib.dump(sklearn_transformer, sklearn_transformer_path)\n",
    "    joblib.dump(transformed_recipe, transformed_recipes_path)\n",
    "\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        code_path=[\"../src/\"]\n",
    "        python_model=CustomSKLearnWrapper(),\n",
    "        input_example=to_nlp_df['ingredients'][0],\n",
    "        signature=signature,        \n",
    "        artifact_path=\"sklearn_model\",\n",
    "        artifacts=artifacts\n",
    "        ) \n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# load dataframes from artifacts\n",
    "# mlflow.artifacts.download_artifacts(\n",
    "#     run_id=mlflow_run_id\n",
    "# )\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "cv_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':'auto',\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(cv_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_stanza_ingreds_full_set_v1\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Raw Dataframe:', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print('Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    # to_nlp_df = pre_proc_df[0:50]\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Subset Dataframe:', end='\\n')\n",
    "    # print(to_nlp_df.head())\n",
    "    # print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params,\n",
    "    )\n",
    "\n",
    "    def custom_analyzer(step_list, stanza_pipeline, minNgramLength, maxNgramLength):\n",
    "            lowered = \" brk \".join(map(str, [step for step in step_list if step is not None])).lower()\n",
    "\n",
    "            preproc = stanza_pipeline(lowered)\n",
    "            \n",
    "            lemmad = \" \".join(map(str,\n",
    "                                [word.text\n",
    "                                for sent in preproc.sentences \n",
    "                                for word in sent.words if (\n",
    "                                    word is not None\n",
    "                                )]\n",
    "                            )\n",
    "                        )\n",
    "            \n",
    "            # analyze each line of the input string seperately\n",
    "            for ln in lemmad.split(' brk '):\n",
    "                \n",
    "                # tokenize the input string (customize the regex as desired)\n",
    "                at_least_two_english_characters_whole_words = \"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "                terms = re.split(at_least_two_english_characters_whole_words, ln)\n",
    "\n",
    "                # loop ngram creation for every number between min and max ngram length\n",
    "                for ngramLength in range(minNgramLength, maxNgramLength+1):\n",
    "\n",
    "                    # find and return all ngrams\n",
    "                    # for ngram in zip(*[terms[i:] for i in range(3)]): \n",
    "                        # <-- solution without a generator (works the same but has higher memory usage)\n",
    "                    for ngram in zip(*[islice(seq, i, len(terms)) for i, seq in enumerate(tee(terms, ngramLength))]):   # <-- solution using a generator\n",
    "                        \n",
    "                        ngram = ' '.join(map(str, ngram))\n",
    "                        # yield ngram\n",
    "                        return str(ngram)\n",
    "\n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4}\n",
    "    \n",
    "    recipe_ingreds = pre_proc_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('Recipe ingredients:', end='\\n')\n",
    "    print(recipe_ingreds)\n",
    "\n",
    "    # train on the recipes' steps\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    # steps_vectorizer_model = CountVectorizer(**cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    steps_vectorizer_model = OnlineCountVectorizer(**cv_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=steps_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print('BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_full_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_full_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt run with lighter weight configuration\n",
    "#### This attempt will still use Stanza processing on the ingredients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_nlp_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# create sklearn pipeline as in BERTopic lightweight configuration\n",
    "# pipe = make_pipeline(\n",
    "#     TfidfVectorizer(**sklearn_nlp_params),\n",
    "#     TruncatedSVD(100)\n",
    "# )\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    # 'embedding_model': TfidfVectorizer(**sklearn_nlp_params),\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':50,\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True,\n",
    "    # 'min_cluster_size': 10 # Possibly only works if modifying individual HDBSCAN component of BERTopic\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_nlp_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_lightweight_stanza_ingreds_small_set_v1\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Raw Dataframe: ', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    to_nlp_df = pre_proc_df[0:100]\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Subset Dataframe:', end='\\n')\n",
    "    print(to_nlp_df.head())\n",
    "    print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params\n",
    "    )\n",
    "    \n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4}\n",
    "    \n",
    "    recipe_ingreds = to_nlp_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(**sklearn_nlp_params)\n",
    "    embeddings = vectorizer.fit_transform(recipe_ingreds)\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    print(recipe_ingreds)\n",
    "\n",
    "    # train on the recipes' ingredientss\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds, embeddings)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    sklearn_cv_params = {\n",
    "        'strip_accents':\"unicode\",\n",
    "        'lowercase':True,\n",
    "        'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "        # 'min_df':10,\n",
    "    }\n",
    "    steps_vectorizer_model = CountVectorizer(**sklearn_cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    # steps_vectorizer_model = OnlineCountVectorizer(**sklearn_nlp_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=steps_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representative Docs:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_full_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_full_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()['Representation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()['Representation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_nlp_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# create sklearn pipeline as in BERTopic lightweight configuration\n",
    "# pipe = make_pipeline(\n",
    "#     TfidfVectorizer(**sklearn_nlp_params),\n",
    "#     TruncatedSVD(100)\n",
    "# )\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    # 'embedding_model': TfidfVectorizer(**sklearn_nlp_params),\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':50,\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True,\n",
    "    # 'min_cluster_size': 10 # Possibly only works if modifying individual HDBSCAN component of BERTopic\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_nlp_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_lightweight_stanza_ingreds_small_set_v1\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Raw Dataframe: ', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    to_nlp_df = pre_proc_df[0:100]\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Subset Dataframe:', end='\\n')\n",
    "    print(to_nlp_df.head())\n",
    "    print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params\n",
    "    )\n",
    "    \n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4}\n",
    "    \n",
    "    recipe_ingreds = to_nlp_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(**sklearn_nlp_params)\n",
    "    embeddings = vectorizer.fit_transform(recipe_ingreds)\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    print(recipe_ingreds)\n",
    "\n",
    "    # train on the recipes' ingredientss\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds, embeddings)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    sklearn_cv_params = {\n",
    "        'strip_accents':\"unicode\",\n",
    "        'lowercase':True,\n",
    "        # 'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "        # 'min_df':10,\n",
    "    }\n",
    "    steps_vectorizer_model = CountVectorizer(**sklearn_cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    # steps_vectorizer_model = OnlineCountVectorizer(**sklearn_nlp_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=steps_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representative Docs:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_full_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_full_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_nlp_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# create sklearn pipeline as in BERTopic lightweight configuration\n",
    "# pipe = make_pipeline(\n",
    "#     TfidfVectorizer(**sklearn_nlp_params),\n",
    "#     TruncatedSVD(100)\n",
    "# )\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    # 'embedding_model': TfidfVectorizer(**sklearn_nlp_params),\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':50,\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True,\n",
    "    # 'min_cluster_size': 10 # Possibly only works if modifying individual HDBSCAN component of BERTopic\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_nlp_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_lightweight_stanza_ingreds_small_set_v1.01\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Raw Dataframe: ', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    to_nlp_df = pre_proc_df[0:100]\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Subset Dataframe:', end='\\n')\n",
    "    print(to_nlp_df.head())\n",
    "    print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params\n",
    "    )\n",
    "    \n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4\n",
    "                       , 'lemmatize': True}\n",
    "    \n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    recipe_ingreds = to_nlp_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    print([ingred for ingred in recipe_ingreds])\n",
    "\n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(**sklearn_nlp_params)\n",
    "    embeddings = vectorizer.fit_transform(tqdm(recipe_ingreds))\n",
    "\n",
    "    # train on the recipes' ingredientss\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds, embeddings)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    sklearn_cv_params = {\n",
    "        # 'strip_accents':\"unicode\",\n",
    "        # 'lowercase':True,\n",
    "        # 'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "        # 'min_df':10,\n",
    "        'token_pattern': r\"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "    }\n",
    "    ingreds_vectorizer_model = CountVectorizer(**sklearn_cv_params)\n",
    "\n",
    "    # May need to use BERTopic's OnlineCountVectorizer\n",
    "    # steps_vectorizer_model = OnlineCountVectorizer(**sklearn_nlp_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=ingreds_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representative Docs:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_small_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_small_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_nlp_df['ingredients'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipe_ingreds = to_nlp_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i think i should start leaving out units/including stopwords again since i'm not using Stanza's deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from MLflow\n",
    "mlflow_client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow')\n",
    "\n",
    "# cv_params are parameters for the sklearn CountVectorizer or TFIDFVectorizer\n",
    "sklearn_nlp_params = {\n",
    "    'strip_accents':\"unicode\",\n",
    "    'lowercase':True,\n",
    "    'analyzer': StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4),\n",
    "    'min_df':10,\n",
    "}\n",
    "\n",
    "# bertopic_params are a superset of cv_params\n",
    "bertopic_params = {\n",
    "    'top_n_words':20,\n",
    "    'min_topic_size':10,\n",
    "    'nr_topics':50,\n",
    "    'verbose':True,\n",
    "    'low_memory':True,\n",
    "    'calculate_probabilities':True,\n",
    "    # 'min_cluster_size': 10 # Possibly only works if modifying individual HDBSCAN component of BERTopic\n",
    "}\n",
    "\n",
    "# update bertopic_params to include cv_params\n",
    "# bertopic_params.update(cv_params)\n",
    "\n",
    "# pipeline_params are parameters that will be logged in MLFlow and are a superset of library parameters\n",
    "pipeline_params = {\n",
    "    'stanza_model': 'en',\n",
    "    'sklearn-transformer': 'TfidfVectorizer'\n",
    "}\n",
    "\n",
    "# update the pipeline parameters with the library-specific ones so that they show up in MLflow Tracking\n",
    "pipeline_params.update(sklearn_nlp_params)\n",
    "pipeline_params.update(bertopic_params)\n",
    "\n",
    "with mlflow.start_run(experiment_id=get_experiment_id(f\"{DAGSHUB_EMAIL}/bertopic_lightweight_stanza_ingreds_full_set_v1.00\")):    \n",
    "    # LOG PARAMETERS\n",
    "    mlflow.log_params(pipeline_params)\n",
    "\n",
    "    # LOG INPUTS (QUERIES) AND OUTPUTS\n",
    "    # MLflow example uses a list of strings or a list of str->str dicts\n",
    "    \n",
    "    # load raw data and preprocess/clean\n",
    "    data = dvc.api.read(\n",
    "           path='../data/recipes-en-201706/epicurious-recipes_m2.json'\n",
    "           , mode='r')\n",
    "    raw_df = pd.read_json(data)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Raw Dataframe: ', end='\\n')\n",
    "    print(raw_df.head())\n",
    "    print(raw_df.shape)\n",
    "\n",
    "    # pre_proc_df is cleaned dataframe\n",
    "    pre_proc_df = dfpp.preprocess_dataframe(raw_df)\n",
    "    print('\\n')\n",
    "    print('--------------')\n",
    "    print(f'{datetime.now()}, Preprocessed Dataframe:', end='\\n')\n",
    "    print(pre_proc_df.head())\n",
    "    print(pre_proc_df.shape)\n",
    "\n",
    "\n",
    "    # pre_proc_df = pd.read_json(\n",
    "    #     mlflow.artifacts.download_artifacts(\n",
    "    #         run_id=mlflow_run_id,\n",
    "    #         artifact_path='artifacts/preprocessed_dataframes/preprocessed_dataframe.json',\n",
    "    #         # tracking_uri=f'https://dagshub.com/{DAGSHUB_USER_NAME}/MeaLeon.mlflow'\n",
    "    #     )\n",
    "    # )\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print('Preprocessed Dataframe:', end='\\n')\n",
    "    # print(pre_proc_df.head())\n",
    "    # print(pre_proc_df.shape)\n",
    "\n",
    "    # create subset for dev purposes\n",
    "    # to_nlp_df = pre_proc_df[0:100]\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print(f'{datetime.now()}, Subset Dataframe:', end='\\n')\n",
    "    # print(to_nlp_df.head())\n",
    "    # print(to_nlp_df.shape)\n",
    "\n",
    "    # LOG MODEL\n",
    "    # Instantiate BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        **bertopic_params\n",
    "    )\n",
    "    \n",
    "    analyzer_kwargs = {'stanza_pipeline': nlp\n",
    "                       , 'minNgramLength': 1\n",
    "                       , 'maxNgramLength': 4\n",
    "                       , 'lemmatize': True}\n",
    "    \n",
    "    recipe_ingreds = pre_proc_df[\"ingredients\"].apply(custom_analyzer, **analyzer_kwargs)\n",
    "    \n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    print(recipe_ingreds)\n",
    "\n",
    "    # Create TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(**sklearn_nlp_params)\n",
    "    embeddings = vectorizer.fit_transform(tqdm(recipe_ingreds))\n",
    "\n",
    "    # recipe_steps = \"\".join(str(to_nlp_df[\"prepSteps\"].apply(StanzaWrapper().stanza_analyzer(stanza_pipeline=nlp, minNgramLength=1, maxNgramLength=4))))\n",
    "    # print('\\n')\n",
    "    # print('-' * 80)\n",
    "    # print(f'{datetime.now()}, Recipe ingredients:', end='\\n')\n",
    "    # print(recipe_ingreds)\n",
    "\n",
    "    # train on the recipes' ingredientss\n",
    "    topics, probs = topic_model.fit_transform(recipe_ingreds, embeddings)\n",
    "\n",
    "    # since this uses a custom Stanza analyzer, we have to use a custom mlflow.Pyfunc.PythonModel\n",
    "    # Instantiate sklearn CountVectorizer\n",
    "    sklearn_cv_params = {\n",
    "        # 'strip_accents':\"unicode\",\n",
    "        # 'lowercase':True,\n",
    "        'token_pattern': r\"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "    }\n",
    "    ingreds_vectorizer_model = CountVectorizer(**sklearn_cv_params)\n",
    "\n",
    "    # Do fit transform on data\n",
    "    # steps_test_tfidf_transform = steps_tfidf_vectorizer_model.fit_transform(tqdm(to_nlp_df[\"steps\"]))\n",
    "    topic_model.update_topics(\n",
    "        recipe_ingreds\n",
    "        , vectorizer_model=ingreds_vectorizer_model\n",
    "    )\n",
    "\n",
    "    # Display topic model results\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Dataframe:', end='\\n')\n",
    "    print(topic_model.get_topic_info())\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representations:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representation'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 80)\n",
    "    print(f'{datetime.now()}, BERTopic Model Representative Docs:', end='\\n')\n",
    "    print(topic_model.get_topic_info()['Representative_Docs'])\n",
    "\n",
    "    # Save and log the topic model dataframe\n",
    "    topic_model.get_topic_info().to_json('../data/processed/bertopic_model_ingreds_full_set_df.json')\n",
    "    mlflow.log_artifact('../data/processed/bertopic_model_ingreds_full_set_df.json',\n",
    "                        artifact_path='bertopic_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try splitting among CPU and GPU. Try Stanza on CPU due to its memory usage\n",
    "nlp2 = stanza.Pipeline('en', use_gpu=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
