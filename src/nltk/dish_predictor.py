""" This Python file calls the Edamam API with user-desired dish name and user-
specified cuisine style, generates a JSON from Edamam with recipes to analyze, 
reduces the 10 provided recipes to one list of ingredients to vectorize, loads 
the database generated by prepare_database.py, finds the top five similar 
recipes based on cosine similarity distances for each list of ingredients.
"""

import requests
import json
import csv
from datetime import datetime
import joblib
import re
import pandas as pd
import numpy as np
from pandas.io.json import json_normalize
import nltk
nltk.download('omw-1.4')
nltk.download('wordnet')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk import word_tokenize, FreqDist
from nltk.stem.wordnet import WordNetLemmatizer
import string
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

# Define all functions

def import_stored_files():
  # Load in the stored Epicurious database, TFIDF Vectorizer object to transform,
  # the input, and the TFIDF word matrix from joblib and created by 
  # prepare_database.py
  with open("joblib/tfidf_recipe_dataframe.joblib", "rb") as fo:
    prepped = joblib.load("joblib/tfidf_recipe_dataframe.joblib")

  with open("joblib/recipe_tfidf.joblib", "rb") as fo:
    ingred_tfidf = joblib.load("joblib/recipe_tfidf.joblib")

  with open("joblib/recipe_word_matrix_tfidf.joblib", "rb") as fo:
    ingred_word_matrix = joblib.load("joblib/recipe_word_matrix_tfidf.joblib")

  return prepped, ingred_tfidf, ingred_word_matrix


def transform_tfidf(ingred_tfidf, recipe):
  # This function takes in a TFIDF Vectorizer object and a recipe, then 
  # creates/transforms the given recipe into a TFIDF form

  ingreds = recipe['ingredients'].apply(" ".join).str.lower()
  response = ingred_tfidf.transform(ingreds)
  transformed_recipe = pd.DataFrame(response.toarray(),
                                    columns=ingred_tfidf.get_feature_names(),
                                    index=recipe.index)
  return transformed_recipe


def filter_out_cuisine(ingred_word_matrix, 
                        X_df, 
                        cuisine_name, 
                        tfidf):
  # This function takes in the ingredient word matrix (from joblib), a 
  # dataframe made from the database (from joblib), the user inputted cuisine 
  # name, and the ingredient TFIDF Vectorizer object (from joblib) and returns
  # a word sub matrix that removes all recipes with the same cuisine as the 
  # inputted recipe.

  east_asian = ['Asian', 'Chinese', 'Japanese']

  southeast_asian = ['Thai', 'Vietnamese']

  euro_islands = ['English', 'Irish']

  euro_continental = ['French', 'German', 'Eastern European']

  mediterranean = ['Italian', 'Mediterranean', 'Kosher', 'Middle Eastern']

  all_cuisines = ['African', 'American', 'Asian', 'Cajun/Creole', 'Chinese', 'Eastern European', 'English',
                  'French', 'German', 'Indian', 'Irish', 'Italian', 'Japanese', 'Kosher', 'Latin American', 
                  'Mediterranean', 'Mexican', 'Middle Eastern', 'Moroccan', 'Scandinavian', 'Southwestern', 
                  'Thai', 'Vietnamese']

  if cuisine_name in east_asian:
    choices = [cuis for cuis in all_cuisines if cuis not in east_asian]
  elif cuisine_name in southeast_asian:
    choices = [cuis for cuis in all_cuisines if cuis not in southeast_asian]
  elif cuisine_name in euro_islands:
    choices = [cuis for cuis in all_cuisines if cuis not in euro_islands]
  elif cuisine_name in euro_continental:
    choices = [cuis for cuis in all_cuisines if cuis not in euro_continental]
  elif cuisine_name in mediterranean:
    choices = [cuis for cuis in all_cuisines if cuis not in mediterranean]
  else:
    choices = [cuis for cuis in all_cuisines if cuis != cuisine_name]

  combo = pd.concat([ingred_word_matrix, X_df['imputed_label']], axis=1)
  filtered_ingred_word_matrix = combo[combo['imputed_label'].isin(choices)].drop('imputed_label', 
                                                                    axis=1)
  return filtered_ingred_word_matrix


def picture_placer(filename):
  # This function takes in a filename and returns the relative location inside
  # an HTML tag
  location = f'photos/{filename}'
  return location


def link_maker(recipe_link):
  # This function takes in the incomplete recipe link from the dataframe and 
  # returns the complete one.
  full_link = f'https://www.epicurious.com{recipe_link}'
  return full_link


def find_closest_recipes(filtered_ingred_word_matrix, 
                          recipe_tfidf, 
                          X_df):
  # This function takes in the filtered ingredient word matrix from function
  # filter_out_cuisine, the TFIDF recipe from function transform_tfidf, and 
  # a dataframe made from the database (from joblib) and returns a Pandas 
  # DataFrame with the top five most similar recipes and a Pandas Series 
  # containing the similarity amount

  m2 = (recipe_tfidf != 0).any()
  recipe_weights = recipe_tfidf.iloc[0][recipe_tfidf.iloc[0] != 0].to_dict()

  ingreds_used = m2.index[m2].tolist()
  search_vec = np.array(recipe_tfidf).reshape(1,-1)
  res_cos_sim = cosine_similarity(filtered_ingred_word_matrix, search_vec)
  top_five = np.argsort(res_cos_sim.flatten())[-5:][::-1]
  top_five_list = top_five.tolist()
  
  recipe_ids = [filtered_ingred_word_matrix.iloc[idx].name for idx in top_five]

  suggest_df = X_df.loc[recipe_ids]
  proximity = pd.DataFrame(data=res_cos_sim[top_five], 
                            columns=['cosine_similarity'], 
                            index=suggest_df.index)
  
  full_df = pd.concat([suggest_df, proximity], axis=1)
  expand_photo_df = pd.concat([full_df.drop(["photoData"], axis=1), 
                                full_df["photoData"].apply(pd.Series)], axis=1)
  reduced = expand_photo_df[['hed', 'recipe_url', 'filename', 'imputed_label', 'ingredients', 'cosine_similarity']].dropna(axis=1)
  reduced['photo'] = reduced['filename'].apply(picture_placer)
  reduced['fixed_url'] = reduced["recipe_url"].apply(link_maker)
  reduced['rounded'] = reduced['cosine_similarity'].round(3)

  reduced = reduced.drop('recipe_url', axis=1)

  ingr_weights = [filtered_ingred_word_matrix.iloc[num][filtered_ingred_word_matrix.iloc[num] != 0].to_dict() for num in top_five_list]
  reduced['ingred_weights'] = ingr_weights

  return reduced, ingreds_used, recipe_weights


def find_similar_dishes(dish_name, cuisine_name):
  prepped, ingred_tfidf, ingred_word_matrix = import_stored_files()
  # This function calls the Edamam API, stores the results as a JSON, and 
  # stores the timestamp, dish name, and cuisine name/classification in a 
  # separate csv.
  now = datetime.now()
  dt_string = now.strftime("%d_%m_%Y_%H_%M_%S")

  api_base = "https://api.edamam.com/search?"

  # Level up:
  # Implement lemmatization using trained dataset on input in order to make 
  # future database be less likely to have redundant entries 
  # (e.g., taco vs tacos)

  q = f"q={dish_name}"

  # Level up:
  # Check a database of dishes to see if this query has been asked for already
  # If not, do an API call

  # Currently, just does an API call, may hit API limit if continuing with this
  # version
  f = open("secrets/edamam.json","r")
  cred = json.load(f)
  f.close()

  app_id = cred["id"]
  app_id_s = f"&app_id={app_id}"

  app_key = cred["key"]
  app_key_s = f"&app_key={app_key}"
  
  # Level up: 
  # Explicitly ask for a few recipes using limiter and make an "average version"
  # of the input in order to get better results from the API call
  # limiter = "&from=0&to=4"
  # API currently defaults to returning 10

  api_call = api_base + q+ app_id_s + app_key_s #+ limiter

  resp = requests.get(api_call)

  if resp.status_code == 200:
    response_dict = resp.json()
    resp_dict_hits = response_dict['hits']
    
    # Store the API result into a JSON and the cuisine type and dish name into a 
    # csv
    # Heroku does not save files to directory
    # Can work with EC2
    # with open(f"../write_data/{dt_string}_{dish_name}_edamam_api_return.json", "w") as f:
    #   json.dump(resp_dict_hits, f)

    # fields = [dt_string, dish_name, cuisine_name]
    # with open("../write_data/user_requests.csv", "a", newline='') as f:
    #   writer = csv.writer(f)
    #   writer.writerow(fields)

    urls = []
    labels = []
    sources = []
    ingreds = []

    for recipe in resp_dict_hits:
        recipe_path = recipe['recipe']
        urls.append(recipe_path['url'])
        labels.append(recipe_path['label'])
        sources.append(recipe_path['source'])
        ingreds.append([item['text'] for item in recipe_path['ingredients']])
        
    all_recipes = {'url': urls,
                  'label': labels, 
                  'source': sources, 
                  'ingredients': ingreds
                  }

    recipe_df = pd.DataFrame(all_recipes)

    one_recipe = []

    for listing in recipe_df['ingredients']:
        for ingred in listing:
            one_recipe.append(ingred.lower())
        
    one_recipe = list(set(one_recipe))

    query_df = pd.DataFrame(data={'name': dish_name, 'ingredients': [one_recipe], 'cuisine': cuisine_name})

    query_tfidf = transform_tfidf(ingred_tfidf=ingred_tfidf, recipe=query_df)
    query_matrix = filter_out_cuisine(ingred_word_matrix=ingred_word_matrix, 
                                      X_df=prepped, 
                                      cuisine_name=cuisine_name, 
                                      tfidf=ingred_tfidf)
                                      
    query_similar, ingreds_used, recipe_weights = find_closest_recipes(filtered_ingred_word_matrix=query_matrix, 
                                          recipe_tfidf=query_tfidf, 
                                          X_df=prepped)
    
    return query_similar.to_dict(orient='records'), ingreds_used, recipe_weights
    
    
  else:
    return("Error, unable to retrieve. Server response code is: ", 
          resp.status_code)
